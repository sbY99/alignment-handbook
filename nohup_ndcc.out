/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
INFO:root:Using nproc_per_node=2.
[2024-02-11 04:34:12,053] torch.distributed.run: [WARNING] 
[2024-02-11 04:34:12,053] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 04:34:12,053] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 04:34:12,053] torch.distributed.run: [WARNING] *****************************************
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
[2024-02-11 04:34:15,142] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-11 04:34:15,269] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-11 04:34:15,397] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-11 04:34:15,397] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
2024-02-11 04:34:15 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False
2024-02-11 04:34:15 - INFO - __main__ - Model parameters ModelArguments(base_model_revision=None, model_name_or_path='LDCC/LDCC-SOLAR-10.7B', model_revision='main', model_code_revision=None, torch_dtype='float16', trust_remote_code=False, use_flash_attention_2=False, use_peft=True, lora_r=16, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)
2024-02-11 04:34:15 - INFO - __main__ - Training/evaluation parameters SFTConfig(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=LDCC-SOLAR-10.7B,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=model/LDCC-SOLAR-10.7B-sft-qlora-v3/runs/Feb11_04-34-15_gpusvr1222,
logging_first_step=True,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_seq_length=512,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=adamw_torch,
optim_args=None,
output_dir=model/LDCC-SOLAR-10.7B-sft-qlora-v3,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=16,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=model/LDCC-SOLAR-10.7B-sft-qlora-v3,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.0,
)
[2024-02-11 04:34:15,534] [INFO] [comm.py:637:init_distributed] cdb=None
2024-02-11 04:34:15 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1 distributed training: True, 16-bits training: False
[INFO|tokenization_utils_base.py:2026] 2024-02-11 04:34:15,777 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2026] 2024-02-11 04:34:15,777 >> loading file tokenizer.json from cache at /home/seungbinyang/.cache/huggingface/hub/models--LDCC--LDCC-SOLAR-10.7B/snapshots/f0162665973def1ad517005d74c9a3df3e4a9b56/tokenizer.json
[INFO|tokenization_utils_base.py:2026] 2024-02-11 04:34:15,778 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2026] 2024-02-11 04:34:15,778 >> loading file special_tokens_map.json from cache at /home/seungbinyang/.cache/huggingface/hub/models--LDCC--LDCC-SOLAR-10.7B/snapshots/f0162665973def1ad517005d74c9a3df3e4a9b56/special_tokens_map.json
[INFO|tokenization_utils_base.py:2026] 2024-02-11 04:34:15,778 >> loading file tokenizer_config.json from cache at /home/seungbinyang/.cache/huggingface/hub/models--LDCC--LDCC-SOLAR-10.7B/snapshots/f0162665973def1ad517005d74c9a3df3e4a9b56/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-11 04:35:34,193 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:329] 2024-02-11 04:35:34,200 >> 
No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.

[WARNING|logging.py:314] 2024-02-11 04:35:34,232 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:329] 2024-02-11 04:35:34,239 >> 
No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.

2024-02-11 04:35:55 - INFO - __main__ - *** Load pretrained model ***
2024-02-11 04:35:55 - INFO - __main__ - *** Model loaded! ***
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:158: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.
  warnings.warn(
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:158: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.
  warnings.warn(
[INFO|configuration_utils.py:739] 2024-02-11 04:35:56,093 >> loading configuration file config.json from cache at /home/seungbinyang/.cache/huggingface/hub/models--LDCC--LDCC-SOLAR-10.7B/snapshots/f0162665973def1ad517005d74c9a3df3e4a9b56/config.json
[INFO|configuration_utils.py:802] 2024-02-11 04:35:56,094 >> Model config LlamaConfig {
  "_name_or_path": "LDCC/LDCC-SOLAR-10.7B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 32000,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.36.2",
  "use_cache": false,
  "vocab_size": 48000
}

[INFO|modeling_utils.py:3344] 2024-02-11 04:35:56,097 >> loading weights file model.safetensors from cache at /home/seungbinyang/.cache/huggingface/hub/models--LDCC--LDCC-SOLAR-10.7B/snapshots/f0162665973def1ad517005d74c9a3df3e4a9b56/model.safetensors.index.json
[INFO|modeling_utils.py:1341] 2024-02-11 04:35:56,098 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:826] 2024-02-11 04:35:56,099 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 32000,
  "pad_token_id": 2,
  "use_cache": false
}


Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:03<00:15,  3.84s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:03<00:15,  3.83s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:07<00:10,  3.65s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:07<00:10,  3.65s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:11<00:07,  3.74s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:11<00:07,  3.74s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:15<00:03,  3.79s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:15<00:03,  3.79s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:16<00:00,  3.00s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:16<00:00,  3.34s/it]

Loading checkpoint shards: 100%|██████████| 5/5 [00:16<00:00,  3.01s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:16<00:00,  3.34s/it]
[INFO|modeling_utils.py:4185] 2024-02-11 04:36:13,067 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4193] 2024-02-11 04:36:13,067 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at LDCC/LDCC-SOLAR-10.7B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:781] 2024-02-11 04:36:13,285 >> loading configuration file generation_config.json from cache at /home/seungbinyang/.cache/huggingface/hub/models--LDCC--LDCC-SOLAR-10.7B/snapshots/f0162665973def1ad517005d74c9a3df3e4a9b56/generation_config.json
[INFO|configuration_utils.py:826] 2024-02-11 04:36:13,286 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 32000,
  "pad_token_id": 2
}

/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
Using custom data configuration default-7ef73599a769a14a
2024-02-11 04:36:14 - INFO - datasets.builder - Using custom data configuration default-7ef73599a769a14a
Loading Dataset Infos from /home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/datasets/packaged_modules/generator
2024-02-11 04:36:14 - INFO - datasets.info - Loading Dataset Infos from /home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/datasets/packaged_modules/generator
Overwrite dataset info from restored data version if exists.
2024-02-11 04:36:14 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/seungbinyang/.cache/huggingface/datasets/generator/default-7ef73599a769a14a/0.0.0
2024-02-11 04:36:14 - INFO - datasets.info - Loading Dataset info from /home/seungbinyang/.cache/huggingface/datasets/generator/default-7ef73599a769a14a/0.0.0
Found cached dataset generator (/home/seungbinyang/.cache/huggingface/datasets/generator/default-7ef73599a769a14a/0.0.0)
2024-02-11 04:36:14 - INFO - datasets.builder - Found cached dataset generator (/home/seungbinyang/.cache/huggingface/datasets/generator/default-7ef73599a769a14a/0.0.0)
Loading Dataset info from /home/seungbinyang/.cache/huggingface/datasets/generator/default-7ef73599a769a14a/0.0.0
2024-02-11 04:36:14 - INFO - datasets.info - Loading Dataset info from /home/seungbinyang/.cache/huggingface/datasets/generator/default-7ef73599a769a14a/0.0.0
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
Using custom data configuration default-6652d3f511a76d3e
2024-02-11 04:36:14 - INFO - datasets.builder - Using custom data configuration default-6652d3f511a76d3e
Loading Dataset Infos from /home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/datasets/packaged_modules/generator
2024-02-11 04:36:14 - INFO - datasets.info - Loading Dataset Infos from /home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/datasets/packaged_modules/generator
Overwrite dataset info from restored data version if exists.
2024-02-11 04:36:14 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/seungbinyang/.cache/huggingface/datasets/generator/default-6652d3f511a76d3e/0.0.0
2024-02-11 04:36:14 - INFO - datasets.info - Loading Dataset info from /home/seungbinyang/.cache/huggingface/datasets/generator/default-6652d3f511a76d3e/0.0.0
Found cached dataset generator (/home/seungbinyang/.cache/huggingface/datasets/generator/default-6652d3f511a76d3e/0.0.0)
2024-02-11 04:36:14 - INFO - datasets.builder - Found cached dataset generator (/home/seungbinyang/.cache/huggingface/datasets/generator/default-6652d3f511a76d3e/0.0.0)
Loading Dataset info from /home/seungbinyang/.cache/huggingface/datasets/generator/default-6652d3f511a76d3e/0.0.0
2024-02-11 04:36:14 - INFO - datasets.info - Loading Dataset info from /home/seungbinyang/.cache/huggingface/datasets/generator/default-6652d3f511a76d3e/0.0.0
[INFO|trainer.py:568] 2024-02-11 04:36:14,842 >> Using auto half precision backend
2024-02-11 04:36:14 - INFO - __main__ - *** Train ***
[2024-02-11 04:36:15,093] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.2, git-hash=unknown, git-branch=unknown
[2024-02-11 04:36:54,324] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-02-11 04:36:54,330] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-02-11 04:36:54,330] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-02-11 04:36:54,397] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-02-11 04:36:54,397] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-02-11 04:36:54,397] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-02-11 04:36:54,397] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-02-11 04:36:54,568] [INFO] [utils.py:802:see_memory_usage] Stage 3 initialize beginning
[2024-02-11 04:36:54,569] [INFO] [utils.py:803:see_memory_usage] MA 20.45 GB         Max_MA 20.45 GB         CA 20.63 GB         Max_CA 21 GB 
[2024-02-11 04:36:54,569] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.07 GB, percent = 5.6%
[2024-02-11 04:36:54,581] [INFO] [stage3.py:126:__init__] Reduce bucket size 500,000,000
[2024-02-11 04:36:54,581] [INFO] [stage3.py:127:__init__] Prefetch bucket size 50,000,000
[2024-02-11 04:36:54,719] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-02-11 04:36:54,719] [INFO] [utils.py:803:see_memory_usage] MA 20.45 GB         Max_MA 20.45 GB         CA 20.63 GB         Max_CA 21 GB 
[2024-02-11 04:36:54,719] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.07 GB, percent = 5.6%
Parameter Offload: Total persistent parameters: 30281728 in 625 params
[2024-02-11 04:37:06,561] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-02-11 04:37:06,562] [INFO] [utils.py:803:see_memory_usage] MA 10.27 GB         Max_MA 20.63 GB         CA 20.82 GB         Max_CA 21 GB 
[2024-02-11 04:37:06,562] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.21 GB, percent = 5.6%
[2024-02-11 04:37:06,768] [INFO] [utils.py:802:see_memory_usage] Before creating fp16 partitions
[2024-02-11 04:37:06,769] [INFO] [utils.py:803:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 20.82 GB         Max_CA 21 GB 
[2024-02-11 04:37:06,769] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.21 GB, percent = 5.6%
[2024-02-11 04:37:07,453] [INFO] [utils.py:802:see_memory_usage] After creating fp16 partitions: 1
[2024-02-11 04:37:07,454] [INFO] [utils.py:803:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.48 GB         Max_CA 21 GB 
[2024-02-11 04:37:07,454] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.22 GB, percent = 5.6%
[2024-02-11 04:37:07,659] [INFO] [utils.py:802:see_memory_usage] Before creating fp32 partitions
[2024-02-11 04:37:07,660] [INFO] [utils.py:803:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.48 GB         Max_CA 10 GB 
[2024-02-11 04:37:07,660] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.22 GB, percent = 5.6%
[2024-02-11 04:37:07,870] [INFO] [utils.py:802:see_memory_usage] After creating fp32 partitions
[2024-02-11 04:37:07,871] [INFO] [utils.py:803:see_memory_usage] MA 10.39 GB         Max_MA 10.45 GB         CA 10.65 GB         Max_CA 11 GB 
[2024-02-11 04:37:07,871] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.22 GB, percent = 5.6%
[2024-02-11 04:37:08,075] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-02-11 04:37:08,076] [INFO] [utils.py:803:see_memory_usage] MA 10.39 GB         Max_MA 10.39 GB         CA 10.65 GB         Max_CA 11 GB 
[2024-02-11 04:37:08,076] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.22 GB, percent = 5.6%
[2024-02-11 04:37:08,287] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-02-11 04:37:08,288] [INFO] [utils.py:803:see_memory_usage] MA 10.62 GB         Max_MA 10.86 GB         CA 11.12 GB         Max_CA 11 GB 
[2024-02-11 04:37:08,288] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.22 GB, percent = 5.6%
[2024-02-11 04:37:08,288] [INFO] [stage3.py:460:_setup_for_real_optimizer] optimizer state initialized
[2024-02-11 04:37:08,796] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-02-11 04:37:08,797] [INFO] [utils.py:803:see_memory_usage] MA 11.61 GB         Max_MA 11.61 GB         CA 12.05 GB         Max_CA 12 GB 
[2024-02-11 04:37:08,797] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.22 GB, percent = 5.6%
[2024-02-11 04:37:08,798] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-02-11 04:37:08,798] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-02-11 04:37:08,798] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-02-11 04:37:08,798] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2024-02-11 04:37:08,806] [INFO] [config.py:972:print] DeepSpeedEngine configuration:
[2024-02-11 04:37:08,806] [INFO] [config.py:976:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-02-11 04:37:08,806] [INFO] [config.py:976:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-02-11 04:37:08,806] [INFO] [config.py:976:print]   amp_enabled .................. False
[2024-02-11 04:37:08,806] [INFO] [config.py:976:print]   amp_params ................... False
[2024-02-11 04:37:08,806] [INFO] [config.py:976:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-02-11 04:37:08,806] [INFO] [config.py:976:print]   bfloat16_enabled ............. True
[2024-02-11 04:37:08,806] [INFO] [config.py:976:print]   checkpoint_parallel_write_pipeline  False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   checkpoint_tag_validation_enabled  True
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   checkpoint_tag_validation_fail  False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb701facb80>
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   communication_data_type ...... None
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   curriculum_enabled_legacy .... False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   curriculum_params_legacy ..... False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   data_efficiency_enabled ...... False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   dataloader_drop_last ......... False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   disable_allgather ............ False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   dump_state ................... False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   dynamic_loss_scale_args ...... None
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   eigenvalue_enabled ........... False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   eigenvalue_gas_boundary_resolution  1
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   eigenvalue_layer_num ......... 0
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   eigenvalue_max_iter .......... 100
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   eigenvalue_stability ......... 1e-06
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   eigenvalue_tol ............... 0.01
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   eigenvalue_verbose ........... False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   elasticity_enabled ........... False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   fp16_auto_cast ............... None
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   fp16_enabled ................. False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   fp16_master_weights_and_gradients  False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   global_rank .................. 0
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   grad_accum_dtype ............. None
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   gradient_accumulation_steps .. 2
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   gradient_clipping ............ 0.0
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   gradient_predivide_factor .... 1.0
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   initial_dynamic_scale ........ 1
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   load_universal_checkpoint .... False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   loss_scale ................... 1.0
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   memory_breakdown ............. False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   mics_hierarchial_params_gather  False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   mics_shard_size .............. -1
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-02-11 04:37:08,807] [INFO] [config.py:976:print]   optimizer_legacy_fusion ...... False
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   optimizer_name ............... None
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   optimizer_params ............. None
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   pld_enabled .................. False
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   pld_params ................... False
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   prescale_gradients ........... False
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   scheduler_name ............... None
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   scheduler_params ............. None
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   seq_parallel_communication_data_type  torch.float32
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   sparse_attention ............. None
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   sparse_gradients_enabled ..... False
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   steps_per_print .............. inf
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   train_batch_size ............. 128
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   train_micro_batch_size_per_gpu  32
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   use_node_local_storage ....... False
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   wall_clock_breakdown ......... False
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   weight_quantization_config ... None
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   world_size ................... 2
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   zero_allow_untested_optimizer  True
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   zero_enabled ................. True
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   zero_force_ds_cpu_optimizer .. True
[2024-02-11 04:37:08,808] [INFO] [config.py:976:print]   zero_optimization_stage ...... 3
[2024-02-11 04:37:08,808] [INFO] [config.py:962:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "gradient_accumulation_steps": 2, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
[INFO|trainer.py:1706] 2024-02-11 04:37:08,808 >> ***** Running training *****
[INFO|trainer.py:1707] 2024-02-11 04:37:08,808 >>   Num examples = 252,975
[INFO|trainer.py:1708] 2024-02-11 04:37:08,808 >>   Num Epochs = 1
[INFO|trainer.py:1709] 2024-02-11 04:37:08,808 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1712] 2024-02-11 04:37:08,808 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:1713] 2024-02-11 04:37:08,808 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1714] 2024-02-11 04:37:08,808 >>   Total optimization steps = 1,976
[INFO|trainer.py:1715] 2024-02-11 04:37:08,816 >>   Number of trainable parameters = 62,914,560

  0%|          | 0/1976 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-02-11 04:37:08,848 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:314] 2024-02-11 04:37:08,851 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

  0%|          | 1/1976 [01:05<35:41:02, 65.04s/it]
                                                   
{'loss': 1.8194, 'learning_rate': 1.0101010101010103e-06, 'epoch': 0.0}

  0%|          | 1/1976 [01:05<35:41:02, 65.04s/it]
  0%|          | 2/1976 [01:59<32:18:29, 58.92s/it]
  0%|          | 3/1976 [02:53<31:07:17, 56.79s/it]
  0%|          | 4/1976 [03:48<30:33:24, 55.78s/it]
  0%|          | 5/1976 [04:42<30:17:56, 55.34s/it]
                                                   
{'loss': 1.862, 'learning_rate': 5.050505050505051e-06, 'epoch': 0.0}

  0%|          | 5/1976 [04:42<30:17:56, 55.34s/it]
  0%|          | 6/1976 [05:36<30:04:44, 54.97s/it]
  0%|          | 7/1976 [06:31<30:03:06, 54.94s/it]
  0%|          | 8/1976 [07:26<29:55:57, 54.75s/it]
  0%|          | 9/1976 [08:21<29:58:16, 54.85s/it]
  1%|          | 10/1976 [09:15<29:54:42, 54.77s/it]
                                                    
{'loss': 1.8358, 'learning_rate': 1.0101010101010101e-05, 'epoch': 0.01}

  1%|          | 10/1976 [09:15<29:54:42, 54.77s/it]
  1%|          | 11/1976 [10:10<29:48:04, 54.60s/it]
  1%|          | 12/1976 [11:04<29:45:47, 54.56s/it]
  1%|          | 13/1976 [11:58<29:37:37, 54.33s/it]
  1%|          | 14/1976 [12:52<29:37:52, 54.37s/it]
  1%|          | 15/1976 [13:46<29:31:40, 54.21s/it]
                                                    
{'loss': 1.7292, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.01}

  1%|          | 15/1976 [13:46<29:31:40, 54.21s/it]
  1%|          | 16/1976 [14:41<29:36:05, 54.37s/it]
  1%|          | 17/1976 [15:35<29:36:16, 54.40s/it]
  1%|          | 18/1976 [16:30<29:36:40, 54.44s/it]
  1%|          | 19/1976 [17:25<29:38:25, 54.52s/it]
  1%|          | 20/1976 [18:19<29:34:45, 54.44s/it]
                                                    
{'loss': 1.6368, 'learning_rate': 2.0202020202020203e-05, 'epoch': 0.01}

  1%|          | 20/1976 [18:19<29:34:45, 54.44s/it]
  1%|          | 21/1976 [19:13<29:32:36, 54.40s/it]
  1%|          | 22/1976 [20:07<29:26:58, 54.26s/it]
  1%|          | 23/1976 [21:02<29:28:41, 54.34s/it]
  1%|          | 24/1976 [21:56<29:25:14, 54.26s/it]
  1%|▏         | 25/1976 [22:50<29:25:58, 54.31s/it]
                                                    
{'loss': 1.5623, 'learning_rate': 2.5252525252525256e-05, 'epoch': 0.01}

  1%|▏         | 25/1976 [22:50<29:25:58, 54.31s/it]
  1%|▏         | 26/1976 [23:45<29:30:28, 54.48s/it]
  1%|▏         | 27/1976 [24:39<29:27:57, 54.43s/it]
  1%|▏         | 28/1976 [25:34<29:33:48, 54.63s/it]
  1%|▏         | 29/1976 [26:29<29:30:07, 54.55s/it]
  2%|▏         | 30/1976 [27:24<29:31:31, 54.62s/it]
                                                    
{'loss': 1.4817, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.02}

  2%|▏         | 30/1976 [27:24<29:31:31, 54.62s/it]
  2%|▏         | 31/1976 [28:18<29:25:52, 54.47s/it]
  2%|▏         | 32/1976 [29:12<29:22:04, 54.39s/it]
  2%|▏         | 33/1976 [30:06<29:19:20, 54.33s/it]
  2%|▏         | 34/1976 [31:00<29:13:46, 54.18s/it]
  2%|▏         | 35/1976 [31:54<29:14:24, 54.23s/it]
                                                    
{'loss': 1.3996, 'learning_rate': 3.535353535353535e-05, 'epoch': 0.02}

  2%|▏         | 35/1976 [31:54<29:14:24, 54.23s/it]
  2%|▏         | 36/1976 [32:48<29:13:14, 54.22s/it]
  2%|▏         | 37/1976 [33:43<29:15:49, 54.33s/it]
  2%|▏         | 38/1976 [34:37<29:15:59, 54.36s/it]
  2%|▏         | 39/1976 [35:32<29:16:12, 54.40s/it]
  2%|▏         | 40/1976 [36:26<29:16:23, 54.43s/it]
                                                    
{'loss': 1.3516, 'learning_rate': 4.0404040404040405e-05, 'epoch': 0.02}

  2%|▏         | 40/1976 [36:26<29:16:23, 54.43s/it]
  2%|▏         | 41/1976 [37:20<29:10:29, 54.28s/it]
  2%|▏         | 42/1976 [38:15<29:09:45, 54.28s/it]
  2%|▏         | 43/1976 [39:08<29:04:06, 54.14s/it]
  2%|▏         | 44/1976 [40:03<29:04:43, 54.18s/it]
  2%|▏         | 45/1976 [40:57<29:03:01, 54.16s/it]
                                                    
{'loss': 1.2983, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.02}

  2%|▏         | 45/1976 [40:57<29:03:01, 54.16s/it]
  2%|▏         | 46/1976 [41:51<29:05:34, 54.27s/it]
  2%|▏         | 47/1976 [42:46<29:09:04, 54.40s/it]
  2%|▏         | 48/1976 [43:40<29:06:10, 54.34s/it]
  2%|▏         | 49/1976 [44:35<29:08:17, 54.44s/it]
  3%|▎         | 50/1976 [45:29<29:05:34, 54.38s/it]
                                                    
{'loss': 1.2424, 'learning_rate': 5.050505050505051e-05, 'epoch': 0.03}

  3%|▎         | 50/1976 [45:29<29:05:34, 54.38s/it]
  3%|▎         | 51/1976 [46:24<29:08:12, 54.49s/it]
  3%|▎         | 52/1976 [47:18<29:03:47, 54.38s/it]
  3%|▎         | 53/1976 [48:12<28:59:57, 54.29s/it]
  3%|▎         | 54/1976 [49:07<29:04:33, 54.46s/it]
  3%|▎         | 55/1976 [50:01<28:58:35, 54.30s/it]
                                                    
{'loss': 1.1909, 'learning_rate': 5.555555555555556e-05, 'epoch': 0.03}

  3%|▎         | 55/1976 [50:01<28:58:35, 54.30s/it]
  3%|▎         | 56/1976 [50:56<29:02:06, 54.44s/it]
  3%|▎         | 57/1976 [51:50<28:59:18, 54.38s/it]
  3%|▎         | 58/1976 [52:45<29:05:41, 54.61s/it]
  3%|▎         | 59/1976 [53:40<29:03:24, 54.57s/it]
  3%|▎         | 60/1976 [54:34<29:01:17, 54.53s/it]
                                                    
{'loss': 1.143, 'learning_rate': 6.060606060606061e-05, 'epoch': 0.03}

  3%|▎         | 60/1976 [54:34<29:01:17, 54.53s/it]
  3%|▎         | 61/1976 [55:29<29:01:32, 54.57s/it]
  3%|▎         | 62/1976 [56:23<28:54:08, 54.36s/it]
  3%|▎         | 63/1976 [57:17<28:54:44, 54.41s/it]
  3%|▎         | 64/1976 [58:11<28:49:05, 54.26s/it]
  3%|▎         | 65/1976 [59:06<28:51:17, 54.36s/it]
                                                    
{'loss': 1.0926, 'learning_rate': 6.565656565656566e-05, 'epoch': 0.03}

  3%|▎         | 65/1976 [59:06<28:51:17, 54.36s/it]
  3%|▎         | 66/1976 [1:00:00<28:51:36, 54.40s/it]
  3%|▎         | 67/1976 [1:00:55<28:53:12, 54.47s/it]
  3%|▎         | 68/1976 [1:01:50<28:56:44, 54.61s/it]
  3%|▎         | 69/1976 [1:02:44<28:52:18, 54.50s/it]
  4%|▎         | 70/1976 [1:03:39<28:54:30, 54.60s/it]
                                                      
{'loss': 1.0371, 'learning_rate': 7.07070707070707e-05, 'epoch': 0.04}

  4%|▎         | 70/1976 [1:03:39<28:54:30, 54.60s/it]
  4%|▎         | 71/1976 [1:04:33<28:47:03, 54.40s/it]
  4%|▎         | 72/1976 [1:05:27<28:46:51, 54.42s/it]
  4%|▎         | 73/1976 [1:06:21<28:42:56, 54.32s/it]
  4%|▎         | 74/1976 [1:07:16<28:41:38, 54.31s/it]
  4%|▍         | 75/1976 [1:08:10<28:41:34, 54.34s/it]
                                                      
{'loss': 0.9971, 'learning_rate': 7.575757575757576e-05, 'epoch': 0.04}

  4%|▍         | 75/1976 [1:08:10<28:41:34, 54.34s/it]
  4%|▍         | 76/1976 [1:09:04<28:39:52, 54.31s/it]
  4%|▍         | 77/1976 [1:09:59<28:44:32, 54.49s/it]
  4%|▍         | 78/1976 [1:10:53<28:41:30, 54.42s/it]
  4%|▍         | 79/1976 [1:11:48<28:46:19, 54.60s/it]
  4%|▍         | 80/1976 [1:12:43<28:43:39, 54.55s/it]
                                                      
{'loss': 0.949, 'learning_rate': 8.080808080808081e-05, 'epoch': 0.04}

  4%|▍         | 80/1976 [1:12:43<28:43:39, 54.55s/it]
  4%|▍         | 81/1976 [1:13:37<28:39:12, 54.43s/it]
  4%|▍         | 82/1976 [1:14:31<28:39:11, 54.46s/it]
  4%|▍         | 83/1976 [1:15:25<28:33:18, 54.30s/it]
  4%|▍         | 84/1976 [1:16:20<28:33:12, 54.33s/it]
  4%|▍         | 85/1976 [1:17:14<28:28:15, 54.20s/it]
                                                      
{'loss': 0.8996, 'learning_rate': 8.585858585858586e-05, 'epoch': 0.04}

  4%|▍         | 85/1976 [1:17:14<28:28:15, 54.20s/it]
  4%|▍         | 86/1976 [1:18:09<28:34:10, 54.42s/it]
  4%|▍         | 87/1976 [1:19:03<28:33:50, 54.44s/it]
  4%|▍         | 88/1976 [1:19:58<28:33:37, 54.46s/it]
  5%|▍         | 89/1976 [1:20:52<28:35:27, 54.55s/it]
  5%|▍         | 90/1976 [1:21:47<28:31:44, 54.46s/it]
                                                      
{'loss': 0.8484, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.05}

  5%|▍         | 90/1976 [1:21:47<28:31:44, 54.46s/it]
  5%|▍         | 91/1976 [1:22:41<28:30:06, 54.43s/it]
  5%|▍         | 92/1976 [1:23:35<28:25:05, 54.30s/it]
  5%|▍         | 93/1976 [1:24:30<28:27:54, 54.42s/it]
  5%|▍         | 94/1976 [1:25:24<28:24:54, 54.35s/it]
  5%|▍         | 95/1976 [1:26:18<28:22:52, 54.32s/it]
                                                      
{'loss': 0.805, 'learning_rate': 9.595959595959596e-05, 'epoch': 0.05}

  5%|▍         | 95/1976 [1:26:18<28:22:52, 54.32s/it]
  5%|▍         | 96/1976 [1:27:13<28:26:04, 54.45s/it]
  5%|▍         | 97/1976 [1:28:07<28:22:50, 54.38s/it]
  5%|▍         | 98/1976 [1:29:02<28:25:50, 54.50s/it]
  5%|▌         | 99/1976 [1:29:56<28:22:07, 54.41s/it]
  5%|▌         | 100/1976 [1:30:51<28:23:46, 54.49s/it]
                                                       
{'loss': 0.7567, 'learning_rate': 0.00010101010101010102, 'epoch': 0.05}

  5%|▌         | 100/1976 [1:30:51<28:23:46, 54.49s/it][INFO|trainer.py:2889] 2024-02-11 06:08:26,116 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-100
[INFO|tokenization_utils_base.py:2432] 2024-02-11 06:08:26,439 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 06:08:26,439 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-100/special_tokens_map.json
[2024-02-11 06:08:26,533] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 06:08:32,917] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 06:08:32,917] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 06:08:45,525] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 06:08:46,337] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 06:08:46,922] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 06:08:46,922] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 06:08:46,986] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

  5%|▌         | 101/1976 [1:32:34<35:57:31, 69.04s/it]
  5%|▌         | 102/1976 [1:33:28<33:36:19, 64.56s/it]
  5%|▌         | 103/1976 [1:34:22<32:00:27, 61.52s/it]
  5%|▌         | 104/1976 [1:35:16<30:48:20, 59.24s/it]
  5%|▌         | 105/1976 [1:36:11<30:02:15, 57.80s/it]
                                                       
{'loss': 0.7114, 'learning_rate': 0.00010606060606060606, 'epoch': 0.05}

  5%|▌         | 105/1976 [1:36:11<30:02:15, 57.80s/it]
  5%|▌         | 106/1976 [1:37:05<29:28:37, 56.75s/it]
  5%|▌         | 107/1976 [1:38:00<29:13:57, 56.31s/it]
  5%|▌         | 108/1976 [1:38:56<29:04:54, 56.05s/it]
  6%|▌         | 109/1976 [1:39:50<28:47:52, 55.53s/it]
  6%|▌         | 110/1976 [1:40:45<28:41:30, 55.35s/it]
                                                       
{'loss': 0.6676, 'learning_rate': 0.00011111111111111112, 'epoch': 0.06}

  6%|▌         | 110/1976 [1:40:45<28:41:30, 55.35s/it]
  6%|▌         | 111/1976 [1:41:39<28:27:21, 54.93s/it]
  6%|▌         | 112/1976 [1:42:33<28:22:37, 54.81s/it]
  6%|▌         | 113/1976 [1:43:27<28:13:50, 54.55s/it]
  6%|▌         | 114/1976 [1:44:22<28:12:59, 54.55s/it]
  6%|▌         | 115/1976 [1:45:16<28:11:58, 54.55s/it]
                                                       
{'loss': 0.6279, 'learning_rate': 0.00011616161616161616, 'epoch': 0.06}

  6%|▌         | 115/1976 [1:45:16<28:11:58, 54.55s/it]
  6%|▌         | 116/1976 [1:46:11<28:08:25, 54.47s/it]
  6%|▌         | 117/1976 [1:47:06<28:11:03, 54.58s/it]
  6%|▌         | 118/1976 [1:48:00<28:06:40, 54.47s/it]
  6%|▌         | 119/1976 [1:48:55<28:09:27, 54.59s/it]
  6%|▌         | 120/1976 [1:49:49<28:05:22, 54.48s/it]
                                                       
{'loss': 0.5904, 'learning_rate': 0.00012121212121212122, 'epoch': 0.06}

  6%|▌         | 120/1976 [1:49:49<28:05:22, 54.48s/it]
  6%|▌         | 121/1976 [1:50:43<28:03:55, 54.47s/it]
  6%|▌         | 122/1976 [1:51:38<28:02:26, 54.45s/it]
  6%|▌         | 123/1976 [1:52:32<27:56:26, 54.28s/it]
  6%|▋         | 124/1976 [1:53:26<27:56:21, 54.31s/it]
  6%|▋         | 125/1976 [1:54:20<27:51:11, 54.17s/it]
                                                       
{'loss': 0.5679, 'learning_rate': 0.00012626262626262626, 'epoch': 0.06}

  6%|▋         | 125/1976 [1:54:20<27:51:11, 54.17s/it]
  6%|▋         | 126/1976 [1:55:15<27:55:33, 54.34s/it]
  6%|▋         | 127/1976 [1:56:09<27:55:42, 54.38s/it]
  6%|▋         | 128/1976 [1:57:04<27:56:12, 54.42s/it]
  7%|▋         | 129/1976 [1:57:58<27:57:27, 54.49s/it]
  7%|▋         | 130/1976 [1:58:52<27:53:57, 54.41s/it]
                                                       
{'loss': 0.534, 'learning_rate': 0.00013131313131313133, 'epoch': 0.07}

  7%|▋         | 130/1976 [1:58:52<27:53:57, 54.41s/it]
  7%|▋         | 131/1976 [1:59:47<27:51:40, 54.36s/it]
  7%|▋         | 132/1976 [2:00:40<27:45:43, 54.20s/it]
  7%|▋         | 133/1976 [2:01:35<27:45:02, 54.21s/it]
  7%|▋         | 134/1976 [2:02:29<27:42:32, 54.15s/it]
  7%|▋         | 135/1976 [2:03:23<27:40:57, 54.13s/it]
                                                       
{'loss': 0.5039, 'learning_rate': 0.00013636363636363637, 'epoch': 0.07}

  7%|▋         | 135/1976 [2:03:23<27:40:57, 54.13s/it]
  7%|▋         | 136/1976 [2:04:17<27:45:21, 54.31s/it]
  7%|▋         | 137/1976 [2:05:12<27:43:38, 54.28s/it]
  7%|▋         | 138/1976 [2:06:06<27:46:47, 54.41s/it]
  7%|▋         | 139/1976 [2:07:01<27:44:01, 54.35s/it]
  7%|▋         | 140/1976 [2:07:55<27:45:47, 54.44s/it]
                                                       
{'loss': 0.4712, 'learning_rate': 0.0001414141414141414, 'epoch': 0.07}

  7%|▋         | 140/1976 [2:07:55<27:45:47, 54.44s/it]
  7%|▋         | 141/1976 [2:08:49<27:41:08, 54.32s/it]
  7%|▋         | 142/1976 [2:09:43<27:37:20, 54.22s/it]
  7%|▋         | 143/1976 [2:10:38<27:37:14, 54.25s/it]
  7%|▋         | 144/1976 [2:11:31<27:32:37, 54.13s/it]
  7%|▋         | 145/1976 [2:12:26<27:33:22, 54.18s/it]
                                                       
{'loss': 0.4437, 'learning_rate': 0.00014646464646464648, 'epoch': 0.07}

  7%|▋         | 145/1976 [2:12:26<27:33:22, 54.18s/it]
  7%|▋         | 146/1976 [2:13:20<27:32:31, 54.18s/it]
  7%|▋         | 147/1976 [2:14:15<27:36:04, 54.33s/it]
  7%|▋         | 148/1976 [2:15:09<27:36:20, 54.37s/it]
  8%|▊         | 149/1976 [2:16:03<27:35:38, 54.37s/it]
  8%|▊         | 150/1976 [2:16:58<27:37:44, 54.47s/it]
                                                       
{'loss': 0.4137, 'learning_rate': 0.00015151515151515152, 'epoch': 0.08}

  8%|▊         | 150/1976 [2:16:58<27:37:44, 54.47s/it]
  8%|▊         | 151/1976 [2:17:52<27:31:22, 54.29s/it]
  8%|▊         | 152/1976 [2:18:46<27:31:50, 54.34s/it]
  8%|▊         | 153/1976 [2:19:40<27:26:27, 54.19s/it]
  8%|▊         | 154/1976 [2:20:35<27:26:44, 54.23s/it]
  8%|▊         | 155/1976 [2:21:29<27:24:52, 54.20s/it]
                                                       
{'loss': 0.3947, 'learning_rate': 0.00015656565656565658, 'epoch': 0.08}

  8%|▊         | 155/1976 [2:21:29<27:24:52, 54.20s/it]
  8%|▊         | 156/1976 [2:22:23<27:26:29, 54.28s/it]
  8%|▊         | 157/1976 [2:23:18<27:29:16, 54.40s/it]
  8%|▊         | 158/1976 [2:24:12<27:26:38, 54.34s/it]
  8%|▊         | 159/1976 [2:25:07<27:29:12, 54.46s/it]
  8%|▊         | 160/1976 [2:26:01<27:26:04, 54.39s/it]
                                                       
{'loss': 0.3762, 'learning_rate': 0.00016161616161616162, 'epoch': 0.08}

  8%|▊         | 160/1976 [2:26:01<27:26:04, 54.39s/it]
  8%|▊         | 161/1976 [2:26:55<27:24:03, 54.35s/it]
  8%|▊         | 162/1976 [2:27:49<27:20:35, 54.26s/it]
  8%|▊         | 163/1976 [2:28:43<27:17:20, 54.19s/it]
  8%|▊         | 164/1976 [2:29:38<27:16:35, 54.19s/it]
  8%|▊         | 165/1976 [2:30:31<27:12:40, 54.09s/it]
                                                       
{'loss': 0.3549, 'learning_rate': 0.0001666666666666667, 'epoch': 0.08}

  8%|▊         | 165/1976 [2:30:31<27:12:40, 54.09s/it]
  8%|▊         | 166/1976 [2:31:26<27:17:30, 54.28s/it]
  8%|▊         | 167/1976 [2:32:20<27:16:04, 54.26s/it]
  9%|▊         | 168/1976 [2:33:15<27:18:17, 54.37s/it]
  9%|▊         | 169/1976 [2:34:09<27:17:36, 54.38s/it]
  9%|▊         | 170/1976 [2:35:04<27:17:09, 54.39s/it]
                                                       
{'loss': 0.3246, 'learning_rate': 0.00017171717171717173, 'epoch': 0.09}

  9%|▊         | 170/1976 [2:35:04<27:17:09, 54.39s/it]
  9%|▊         | 171/1976 [2:35:58<27:17:02, 54.42s/it]
  9%|▊         | 172/1976 [2:36:52<27:10:56, 54.24s/it]
  9%|▉         | 173/1976 [2:37:46<27:10:02, 54.24s/it]
  9%|▉         | 174/1976 [2:38:40<27:05:23, 54.12s/it]
  9%|▉         | 175/1976 [2:39:35<27:07:13, 54.21s/it]
                                                       
{'loss': 0.3132, 'learning_rate': 0.0001767676767676768, 'epoch': 0.09}

  9%|▉         | 175/1976 [2:39:35<27:07:13, 54.21s/it]
  9%|▉         | 176/1976 [2:40:29<27:08:02, 54.27s/it]
  9%|▉         | 177/1976 [2:41:24<27:09:38, 54.35s/it]
  9%|▉         | 178/1976 [2:42:18<27:11:28, 54.44s/it]
  9%|▉         | 179/1976 [2:43:13<27:08:34, 54.38s/it]
  9%|▉         | 180/1976 [2:44:07<27:09:58, 54.45s/it]
                                                       
{'loss': 0.2989, 'learning_rate': 0.00018181818181818183, 'epoch': 0.09}

  9%|▉         | 180/1976 [2:44:07<27:09:58, 54.45s/it]
  9%|▉         | 181/1976 [2:45:01<27:03:38, 54.27s/it]
  9%|▉         | 182/1976 [2:45:55<27:02:48, 54.27s/it]
  9%|▉         | 183/1976 [2:46:49<27:00:26, 54.23s/it]
  9%|▉         | 184/1976 [2:47:43<26:58:11, 54.18s/it]
  9%|▉         | 185/1976 [2:48:38<26:58:21, 54.22s/it]
                                                       
{'loss': 0.2804, 'learning_rate': 0.00018686868686868687, 'epoch': 0.09}

  9%|▉         | 185/1976 [2:48:38<26:58:21, 54.22s/it]
  9%|▉         | 186/1976 [2:49:32<26:57:26, 54.22s/it]
  9%|▉         | 187/1976 [2:50:27<27:01:14, 54.37s/it]
 10%|▉         | 188/1976 [2:51:21<26:59:27, 54.34s/it]
 10%|▉         | 189/1976 [2:52:16<27:01:32, 54.44s/it]
 10%|▉         | 190/1976 [2:53:10<27:02:06, 54.49s/it]
                                                       
{'loss': 0.2693, 'learning_rate': 0.00019191919191919191, 'epoch': 0.1}

 10%|▉         | 190/1976 [2:53:10<27:02:06, 54.49s/it]
 10%|▉         | 191/1976 [2:54:04<26:57:31, 54.37s/it]
 10%|▉         | 192/1976 [2:54:59<26:56:55, 54.38s/it]
 10%|▉         | 193/1976 [2:55:53<26:51:14, 54.22s/it]
 10%|▉         | 194/1976 [2:56:47<26:50:34, 54.23s/it]
 10%|▉         | 195/1976 [2:57:41<26:46:13, 54.11s/it]
                                                       
{'loss': 0.253, 'learning_rate': 0.00019696969696969698, 'epoch': 0.1}

 10%|▉         | 195/1976 [2:57:41<26:46:13, 54.11s/it]
 10%|▉         | 196/1976 [2:58:35<26:51:29, 54.32s/it]
 10%|▉         | 197/1976 [2:59:30<26:51:54, 54.36s/it]
 10%|█         | 198/1976 [3:00:25<26:52:39, 54.42s/it]
 10%|█         | 199/1976 [3:01:19<26:55:11, 54.54s/it]
 10%|█         | 200/1976 [3:02:14<26:51:32, 54.44s/it]
                                                       
{'loss': 0.2415, 'learning_rate': 0.00019999937559585725, 'epoch': 0.1}

 10%|█         | 200/1976 [3:02:14<26:51:32, 54.44s/it][INFO|trainer.py:2889] 2024-02-11 07:39:48,277 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-200
[INFO|tokenization_utils_base.py:2432] 2024-02-11 07:39:48,556 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 07:39:48,556 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-200/special_tokens_map.json
[2024-02-11 07:39:48,649] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 07:39:54,129] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 07:39:54,129] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 07:40:06,440] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 07:40:07,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 07:40:07,638] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 07:40:07,638] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 07:40:07,704] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[INFO|trainer.py:2979] 2024-02-11 07:40:07,707 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-100] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 10%|█         | 201/1976 [3:03:56<33:55:05, 68.79s/it]
 10%|█         | 202/1976 [3:04:50<31:42:56, 64.36s/it]
 10%|█         | 203/1976 [3:05:44<30:11:21, 61.30s/it]
 10%|█         | 204/1976 [3:06:38<29:08:43, 59.21s/it]
 10%|█         | 205/1976 [3:07:32<28:19:58, 57.59s/it]
                                                       
{'loss': 0.2288, 'learning_rate': 0.0001999923511388017, 'epoch': 0.1}

 10%|█         | 205/1976 [3:07:32<28:19:58, 57.59s/it]
 10%|█         | 206/1976 [3:08:27<27:53:19, 56.72s/it]
 10%|█         | 207/1976 [3:09:21<27:30:16, 55.97s/it]
 11%|█         | 208/1976 [3:10:16<27:18:47, 55.62s/it]
 11%|█         | 209/1976 [3:11:10<27:07:46, 55.27s/it]
 11%|█         | 210/1976 [3:12:05<26:59:22, 55.02s/it]
                                                       
{'loss': 0.2232, 'learning_rate': 0.00019997752226959923, 'epoch': 0.11}

 11%|█         | 210/1976 [3:12:05<26:59:22, 55.02s/it]
 11%|█         | 211/1976 [3:12:59<26:52:06, 54.80s/it]
 11%|█         | 212/1976 [3:13:53<26:43:18, 54.53s/it]
 11%|█         | 213/1976 [3:14:47<26:41:11, 54.49s/it]
 11%|█         | 214/1976 [3:15:41<26:34:44, 54.30s/it]
 11%|█         | 215/1976 [3:16:36<26:34:17, 54.32s/it]
                                                       
{'loss': 0.211, 'learning_rate': 0.0001999548901456444, 'epoch': 0.11}

 11%|█         | 215/1976 [3:16:36<26:34:17, 54.32s/it]
 11%|█         | 216/1976 [3:17:30<26:35:03, 54.38s/it]
 11%|█         | 217/1976 [3:18:24<26:34:17, 54.38s/it]
 11%|█         | 218/1976 [3:19:19<26:36:30, 54.49s/it]
 11%|█         | 219/1976 [3:20:13<26:33:23, 54.41s/it]
 11%|█         | 220/1976 [3:21:08<26:34:32, 54.48s/it]
                                                       
{'loss': 0.2069, 'learning_rate': 0.00019992445653337665, 'epoch': 0.11}

 11%|█         | 220/1976 [3:21:08<26:34:32, 54.48s/it]
 11%|█         | 221/1976 [3:22:02<26:28:18, 54.30s/it]
 11%|█         | 222/1976 [3:22:56<26:28:18, 54.33s/it]
 11%|█▏        | 223/1976 [3:23:51<26:26:26, 54.30s/it]
 11%|█▏        | 224/1976 [3:24:45<26:23:51, 54.24s/it]
 11%|█▏        | 225/1976 [3:25:39<26:24:08, 54.28s/it]
                                                       
{'loss': 0.1947, 'learning_rate': 0.00019988622380814213, 'epoch': 0.11}

 11%|█▏        | 225/1976 [3:25:39<26:24:08, 54.28s/it]
 11%|█▏        | 226/1976 [3:26:33<26:22:20, 54.25s/it]
 11%|█▏        | 227/1976 [3:27:28<26:25:02, 54.38s/it]
 12%|█▏        | 228/1976 [3:28:22<26:22:50, 54.33s/it]
 12%|█▏        | 229/1976 [3:29:17<26:25:12, 54.44s/it]
 12%|█▏        | 230/1976 [3:30:11<26:24:18, 54.44s/it]
                                                       
{'loss': 0.1877, 'learning_rate': 0.00019984019495400857, 'epoch': 0.12}

 12%|█▏        | 230/1976 [3:30:11<26:24:18, 54.44s/it]
 12%|█▏        | 231/1976 [3:31:05<26:19:38, 54.31s/it]
 12%|█▏        | 232/1976 [3:32:00<26:18:27, 54.30s/it]
 12%|█▏        | 233/1976 [3:32:53<26:13:31, 54.17s/it]
 12%|█▏        | 234/1976 [3:33:48<26:13:52, 54.21s/it]
 12%|█▏        | 235/1976 [3:34:42<26:09:52, 54.10s/it]
                                                       
{'loss': 0.1845, 'learning_rate': 0.0001997863735635322, 'epoch': 0.12}

 12%|█▏        | 235/1976 [3:34:42<26:09:52, 54.10s/it]
 12%|█▏        | 236/1976 [3:35:36<26:13:20, 54.25s/it]
 12%|█▏        | 237/1976 [3:36:31<26:14:19, 54.32s/it]
 12%|█▏        | 238/1976 [3:37:25<26:13:50, 54.33s/it]
 12%|█▏        | 239/1976 [3:38:20<26:14:50, 54.40s/it]
 12%|█▏        | 240/1976 [3:39:14<26:12:18, 54.34s/it]
                                                       
{'loss': 0.1803, 'learning_rate': 0.00019972476383747748, 'epoch': 0.12}

 12%|█▏        | 240/1976 [3:39:14<26:12:18, 54.34s/it]
 12%|█▏        | 241/1976 [3:40:08<26:10:59, 54.33s/it]
 12%|█▏        | 242/1976 [3:41:02<26:06:23, 54.20s/it]
 12%|█▏        | 243/1976 [3:41:56<26:05:51, 54.21s/it]
 12%|█▏        | 244/1976 [3:42:50<26:03:51, 54.18s/it]
 12%|█▏        | 245/1976 [3:43:44<26:01:39, 54.13s/it]
                                                       
{'loss': 0.1795, 'learning_rate': 0.0001996553705844892, 'epoch': 0.12}

 12%|█▏        | 245/1976 [3:43:44<26:01:39, 54.13s/it]
 12%|█▏        | 246/1976 [3:44:39<26:05:55, 54.31s/it]
 12%|█▎        | 247/1976 [3:45:33<26:04:16, 54.28s/it]
 13%|█▎        | 248/1976 [3:46:28<26:06:04, 54.38s/it]
 13%|█▎        | 249/1976 [3:47:22<26:03:56, 54.34s/it]
 13%|█▎        | 250/1976 [3:48:17<26:06:36, 54.46s/it]
                                                       
{'loss': 0.1741, 'learning_rate': 0.00019957819922071702, 'epoch': 0.13}

 13%|█▎        | 250/1976 [3:48:17<26:06:36, 54.46s/it]
 13%|█▎        | 251/1976 [3:49:11<26:02:13, 54.34s/it]
 13%|█▎        | 252/1976 [3:50:05<25:59:16, 54.27s/it]
 13%|█▎        | 253/1976 [3:50:59<25:58:00, 54.25s/it]
 13%|█▎        | 254/1976 [3:51:53<25:53:37, 54.13s/it]
 13%|█▎        | 255/1976 [3:52:47<25:53:38, 54.17s/it]
                                                       
{'loss': 0.1669, 'learning_rate': 0.00019949325576939307, 'epoch': 0.13}

 13%|█▎        | 255/1976 [3:52:47<25:53:38, 54.17s/it]
 13%|█▎        | 256/1976 [3:53:42<25:53:19, 54.19s/it]
 13%|█▎        | 257/1976 [3:54:36<25:57:30, 54.36s/it]
 13%|█▎        | 258/1976 [3:55:31<25:56:51, 54.37s/it]
 13%|█▎        | 259/1976 [3:56:25<25:55:56, 54.37s/it]
 13%|█▎        | 260/1976 [3:57:20<25:57:49, 54.47s/it]
                                                       
{'loss': 0.1635, 'learning_rate': 0.00019940054686036135, 'epoch': 0.13}

 13%|█▎        | 260/1976 [3:57:20<25:57:49, 54.47s/it]
 13%|█▎        | 261/1976 [3:58:14<25:51:39, 54.29s/it]
 13%|█▎        | 262/1976 [3:59:08<25:50:27, 54.28s/it]
 13%|█▎        | 263/1976 [4:00:02<25:45:45, 54.14s/it]
 13%|█▎        | 264/1976 [4:00:56<25:45:25, 54.16s/it]
 13%|█▎        | 265/1976 [4:01:50<25:43:09, 54.11s/it]
                                                       
{'loss': 0.1627, 'learning_rate': 0.00019930007972956078, 'epoch': 0.13}

 13%|█▎        | 265/1976 [4:01:50<25:43:09, 54.11s/it]
 13%|█▎        | 266/1976 [4:02:44<25:45:08, 54.22s/it]
 14%|█▎        | 267/1976 [4:03:39<25:47:57, 54.35s/it]
 14%|█▎        | 268/1976 [4:04:33<25:45:58, 54.31s/it]
 14%|█▎        | 269/1976 [4:05:28<25:47:21, 54.39s/it]
 14%|█▎        | 270/1976 [4:06:22<25:44:50, 54.33s/it]
                                                       
{'loss': 0.1611, 'learning_rate': 0.00019919186221846005, 'epoch': 0.14}

 14%|█▎        | 270/1976 [4:06:22<25:44:50, 54.33s/it]
 14%|█▎        | 271/1976 [4:07:16<25:42:55, 54.30s/it]
 14%|█▍        | 272/1976 [4:08:10<25:39:44, 54.22s/it]
 14%|█▍        | 273/1976 [4:09:04<25:37:11, 54.16s/it]
 14%|█▍        | 274/1976 [4:09:59<25:37:47, 54.21s/it]
 14%|█▍        | 275/1976 [4:10:53<25:33:46, 54.10s/it]
                                                       
{'loss': 0.1558, 'learning_rate': 0.00019907590277344582, 'epoch': 0.14}

 14%|█▍        | 275/1976 [4:10:53<25:33:46, 54.10s/it]
 14%|█▍        | 276/1976 [4:11:47<25:37:01, 54.25s/it]
 14%|█▍        | 277/1976 [4:12:41<25:35:22, 54.22s/it]
 14%|█▍        | 278/1976 [4:13:36<25:37:18, 54.32s/it]
 14%|█▍        | 279/1976 [4:14:30<25:37:29, 54.36s/it]
 14%|█▍        | 280/1976 [4:15:25<25:37:00, 54.38s/it]
                                                       
{'loss': 0.1568, 'learning_rate': 0.00019895221044516335, 'epoch': 0.14}

 14%|█▍        | 280/1976 [4:15:25<25:37:00, 54.38s/it]
 14%|█▍        | 281/1976 [4:16:19<25:35:14, 54.35s/it]
 14%|█▍        | 282/1976 [4:17:13<25:30:00, 54.19s/it]
 14%|█▍        | 283/1976 [4:18:07<25:30:52, 54.25s/it]
 14%|█▍        | 284/1976 [4:19:01<25:26:49, 54.14s/it]
 14%|█▍        | 285/1976 [4:19:55<25:26:36, 54.17s/it]
                                                       
{'loss': 0.1517, 'learning_rate': 0.0001988207948878102, 'epoch': 0.14}

 14%|█▍        | 285/1976 [4:19:55<25:26:36, 54.17s/it]
 14%|█▍        | 286/1976 [4:20:50<25:28:29, 54.27s/it]
 15%|█▍        | 287/1976 [4:21:44<25:28:55, 54.31s/it]
 15%|█▍        | 288/1976 [4:22:39<25:31:07, 54.42s/it]
 15%|█▍        | 289/1976 [4:23:33<25:28:20, 54.36s/it]
 15%|█▍        | 290/1976 [4:24:28<25:29:47, 54.44s/it]
                                                       
{'loss': 0.1495, 'learning_rate': 0.0001986816663583826, 'epoch': 0.15}

 15%|█▍        | 290/1976 [4:24:28<25:29:47, 54.44s/it]
 15%|█▍        | 291/1976 [4:25:22<25:23:50, 54.26s/it]
 15%|█▍        | 292/1976 [4:26:16<25:23:00, 54.26s/it]
 15%|█▍        | 293/1976 [4:27:10<25:20:33, 54.21s/it]
 15%|█▍        | 294/1976 [4:28:04<25:18:02, 54.15s/it]
 15%|█▍        | 295/1976 [4:28:58<25:19:01, 54.22s/it]
                                                       
{'loss': 0.1487, 'learning_rate': 0.00019853483571587502, 'epoch': 0.15}

 15%|█▍        | 295/1976 [4:28:58<25:19:01, 54.22s/it]
 15%|█▍        | 296/1976 [4:29:53<25:17:46, 54.21s/it]
 15%|█▌        | 297/1976 [4:30:47<25:19:31, 54.30s/it]
 15%|█▌        | 298/1976 [4:31:41<25:17:52, 54.27s/it]
 15%|█▌        | 299/1976 [4:32:36<25:20:10, 54.39s/it]
 15%|█▌        | 300/1976 [4:33:30<25:20:13, 54.42s/it]
                                                       
{'loss': 0.1468, 'learning_rate': 0.00019838031442043252, 'epoch': 0.15}

 15%|█▌        | 300/1976 [4:33:30<25:20:13, 54.42s/it][INFO|trainer.py:2889] 2024-02-11 09:11:05,374 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-300
[INFO|tokenization_utils_base.py:2432] 2024-02-11 09:11:05,649 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 09:11:05,649 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-300/special_tokens_map.json
[2024-02-11 09:11:06,697] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 09:11:10,951] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 09:11:10,951] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 09:11:22,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 09:11:23,086] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 09:11:23,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 09:11:23,695] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 09:11:23,761] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
[INFO|trainer.py:2979] 2024-02-11 09:11:23,764 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-200] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 15%|█▌        | 301/1976 [4:35:11<31:42:01, 68.13s/it]
 15%|█▌        | 302/1976 [4:36:05<29:45:48, 64.01s/it]
 15%|█▌        | 303/1976 [4:36:59<28:20:48, 61.00s/it]
 15%|█▌        | 304/1976 [4:37:53<27:25:28, 59.05s/it]
 15%|█▌        | 305/1976 [4:38:48<26:45:01, 57.63s/it]
                                                       
{'loss': 0.1451, 'learning_rate': 0.00019821811453245633, 'epoch': 0.15}

 15%|█▌        | 305/1976 [4:38:48<26:45:01, 57.63s/it]
 15%|█▌        | 306/1976 [4:39:42<26:19:09, 56.74s/it]
 16%|█▌        | 307/1976 [4:40:37<26:02:41, 56.18s/it]
 16%|█▌        | 308/1976 [4:41:32<25:46:20, 55.62s/it]
 16%|█▌        | 309/1976 [4:42:26<25:39:09, 55.40s/it]
 16%|█▌        | 310/1976 [4:43:21<25:29:23, 55.08s/it]
                                                       
{'loss': 0.1452, 'learning_rate': 0.00019804824871166255, 'epoch': 0.16}

 16%|█▌        | 310/1976 [4:43:21<25:29:23, 55.08s/it]
 16%|█▌        | 311/1976 [4:44:15<25:23:44, 54.91s/it]
 16%|█▌        | 312/1976 [4:45:09<25:16:38, 54.69s/it]
 16%|█▌        | 313/1976 [4:46:04<25:12:01, 54.55s/it]
 16%|█▌        | 314/1976 [4:46:58<25:09:59, 54.51s/it]
 16%|█▌        | 315/1976 [4:47:52<25:04:34, 54.35s/it]
                                                       
{'loss': 0.1432, 'learning_rate': 0.000197870730216094, 'epoch': 0.16}

 16%|█▌        | 315/1976 [4:47:52<25:04:34, 54.35s/it]
 16%|█▌        | 316/1976 [4:48:47<25:07:38, 54.49s/it]
 16%|█▌        | 317/1976 [4:49:41<25:05:57, 54.47s/it]
 16%|█▌        | 318/1976 [4:50:36<25:05:41, 54.49s/it]
 16%|█▌        | 319/1976 [4:51:30<25:05:30, 54.51s/it]
 16%|█▌        | 320/1976 [4:52:25<25:02:31, 54.44s/it]
                                                       
{'loss': 0.1468, 'learning_rate': 0.0001976855729010855, 'epoch': 0.16}

 16%|█▌        | 320/1976 [4:52:25<25:02:31, 54.44s/it]
 16%|█▌        | 321/1976 [4:53:19<24:59:20, 54.36s/it]
 16%|█▋        | 322/1976 [4:54:13<24:54:50, 54.23s/it]
 16%|█▋        | 323/1976 [4:55:07<24:54:16, 54.24s/it]
 16%|█▋        | 324/1976 [4:56:01<24:50:41, 54.14s/it]
 16%|█▋        | 325/1976 [4:56:55<24:50:18, 54.16s/it]
                                                       
{'loss': 0.1413, 'learning_rate': 0.00019749279121818235, 'epoch': 0.16}

 16%|█▋        | 325/1976 [4:56:55<24:50:18, 54.16s/it]
 16%|█▋        | 326/1976 [4:57:50<24:52:50, 54.28s/it]
 17%|█▋        | 327/1976 [4:58:44<24:51:59, 54.29s/it]
 17%|█▋        | 328/1976 [4:59:39<24:52:50, 54.35s/it]
 17%|█▋        | 329/1976 [5:00:33<24:51:15, 54.33s/it]
 17%|█▋        | 330/1976 [5:01:27<24:52:03, 54.39s/it]
                                                       
{'loss': 0.1394, 'learning_rate': 0.00019729240021401263, 'epoch': 0.17}

 17%|█▋        | 330/1976 [5:01:27<24:52:03, 54.39s/it]
 17%|█▋        | 331/1976 [5:02:21<24:47:17, 54.25s/it]
 17%|█▋        | 332/1976 [5:03:15<24:45:31, 54.22s/it]
 17%|█▋        | 333/1976 [5:04:10<24:44:26, 54.21s/it]
 17%|█▋        | 334/1976 [5:05:04<24:41:03, 54.12s/it]
 17%|█▋        | 335/1976 [5:05:58<24:40:23, 54.13s/it]
                                                       
{'loss': 0.1385, 'learning_rate': 0.0001970844155291125, 'epoch': 0.17}

 17%|█▋        | 335/1976 [5:05:58<24:40:23, 54.13s/it]
 17%|█▋        | 336/1976 [5:06:52<24:40:46, 54.17s/it]
 17%|█▋        | 337/1976 [5:07:46<24:42:39, 54.28s/it]
 17%|█▋        | 338/1976 [5:08:41<24:41:40, 54.27s/it]
 17%|█▋        | 339/1976 [5:09:35<24:42:26, 54.33s/it]
 17%|█▋        | 340/1976 [5:10:30<24:43:31, 54.41s/it]
                                                       
{'loss': 0.1376, 'learning_rate': 0.00019686885339670557, 'epoch': 0.17}

 17%|█▋        | 340/1976 [5:10:30<24:43:31, 54.41s/it]
 17%|█▋        | 341/1976 [5:11:24<24:38:25, 54.25s/it]
 17%|█▋        | 342/1976 [5:12:18<24:36:47, 54.23s/it]
 17%|█▋        | 343/1976 [5:13:12<24:33:07, 54.13s/it]
 17%|█▋        | 344/1976 [5:14:06<24:32:46, 54.15s/it]
 17%|█▋        | 345/1976 [5:15:00<24:30:15, 54.09s/it]
                                                       
{'loss': 0.1377, 'learning_rate': 0.00019664573064143604, 'epoch': 0.17}

 17%|█▋        | 345/1976 [5:15:00<24:30:15, 54.09s/it]
 18%|█▊        | 346/1976 [5:15:54<24:33:25, 54.24s/it]
 18%|█▊        | 347/1976 [5:16:49<24:36:04, 54.37s/it]
 18%|█▊        | 348/1976 [5:17:43<24:34:28, 54.34s/it]
 18%|█▊        | 349/1976 [5:18:38<24:34:57, 54.39s/it]
 18%|█▊        | 350/1976 [5:19:32<24:33:18, 54.37s/it]
                                                       
{'loss': 0.1357, 'learning_rate': 0.00019641506467805533, 'epoch': 0.18}

 18%|█▊        | 350/1976 [5:19:32<24:33:18, 54.37s/it]
 18%|█▊        | 351/1976 [5:20:26<24:31:00, 54.31s/it]
 18%|█▊        | 352/1976 [5:21:20<24:26:59, 54.20s/it]
 18%|█▊        | 353/1976 [5:22:15<24:25:56, 54.19s/it]
 18%|█▊        | 354/1976 [5:23:09<24:24:53, 54.19s/it]
 18%|█▊        | 355/1976 [5:24:03<24:21:40, 54.10s/it]
                                                       
{'loss': 0.1338, 'learning_rate': 0.000196176873510063, 'epoch': 0.18}

 18%|█▊        | 355/1976 [5:24:03<24:21:40, 54.10s/it]
 18%|█▊        | 356/1976 [5:24:57<24:23:44, 54.21s/it]
 18%|█▊        | 357/1976 [5:25:51<24:23:32, 54.24s/it]
 18%|█▊        | 358/1976 [5:26:46<24:25:27, 54.34s/it]
 18%|█▊        | 359/1976 [5:27:40<24:23:58, 54.32s/it]
 18%|█▊        | 360/1976 [5:28:35<24:24:28, 54.37s/it]
                                                       
{'loss': 0.1346, 'learning_rate': 0.00019593117572830142, 'epoch': 0.18}

 18%|█▊        | 360/1976 [5:28:35<24:24:28, 54.37s/it]
 18%|█▊        | 361/1976 [5:29:29<24:22:07, 54.32s/it]
 18%|█▊        | 362/1976 [5:30:23<24:17:50, 54.19s/it]
 18%|█▊        | 363/1976 [5:31:17<24:16:19, 54.17s/it]
 18%|█▊        | 364/1976 [5:32:11<24:13:27, 54.10s/it]
 18%|█▊        | 365/1976 [5:33:05<24:12:58, 54.11s/it]
                                                       
{'loss': 0.1326, 'learning_rate': 0.00019567799050950496, 'epoch': 0.18}

 18%|█▊        | 365/1976 [5:33:05<24:12:58, 54.11s/it]
 19%|█▊        | 366/1976 [5:33:59<24:13:15, 54.16s/it]
 19%|█▊        | 367/1976 [5:34:54<24:15:27, 54.27s/it]
 19%|█▊        | 368/1976 [5:35:48<24:16:20, 54.34s/it]
 19%|█▊        | 369/1976 [5:36:43<24:14:44, 54.32s/it]
 19%|█▊        | 370/1976 [5:37:37<24:14:43, 54.35s/it]
                                                       
{'loss': 0.133, 'learning_rate': 0.00019541733761480311, 'epoch': 0.19}

 19%|█▊        | 370/1976 [5:37:37<24:14:43, 54.35s/it]
 19%|█▉        | 371/1976 [5:38:31<24:10:24, 54.22s/it]
 19%|█▉        | 372/1976 [5:39:25<24:09:10, 54.21s/it]
 19%|█▉        | 373/1976 [5:40:19<24:05:42, 54.11s/it]
 19%|█▉        | 374/1976 [5:41:13<24:05:10, 54.13s/it]
 19%|█▉        | 375/1976 [5:42:07<24:04:55, 54.15s/it]
                                                       
{'loss': 0.1333, 'learning_rate': 0.0001951492373881781, 'epoch': 0.19}

 19%|█▉        | 375/1976 [5:42:07<24:04:55, 54.15s/it]
 19%|█▉        | 376/1976 [5:43:02<24:05:12, 54.20s/it]
 19%|█▉        | 377/1976 [5:43:56<24:06:48, 54.29s/it]
 19%|█▉        | 378/1976 [5:44:50<24:06:00, 54.29s/it]
 19%|█▉        | 379/1976 [5:45:45<24:07:00, 54.36s/it]
 19%|█▉        | 380/1976 [5:46:39<24:05:14, 54.33s/it]
                                                       
{'loss': 0.1314, 'learning_rate': 0.00019487371075487713, 'epoch': 0.19}

 19%|█▉        | 380/1976 [5:46:39<24:05:14, 54.33s/it]
 19%|█▉        | 381/1976 [5:47:34<24:03:46, 54.31s/it]
 19%|█▉        | 382/1976 [5:48:28<24:02:43, 54.31s/it]
 19%|█▉        | 383/1976 [5:49:22<23:58:58, 54.20s/it]
 19%|█▉        | 384/1976 [5:50:16<23:57:57, 54.19s/it]
 19%|█▉        | 385/1976 [5:51:10<23:54:54, 54.11s/it]
                                                       
{'loss': 0.1319, 'learning_rate': 0.0001945907792197791, 'epoch': 0.19}

 19%|█▉        | 385/1976 [5:51:10<23:54:54, 54.11s/it]
 20%|█▉        | 386/1976 [5:52:04<23:57:06, 54.23s/it]
 20%|█▉        | 387/1976 [5:52:59<23:56:30, 54.24s/it]
 20%|█▉        | 388/1976 [5:53:53<23:57:54, 54.33s/it]
 20%|█▉        | 389/1976 [5:54:48<23:58:04, 54.37s/it]
 20%|█▉        | 390/1976 [5:55:42<23:56:19, 54.34s/it]
                                                       
{'loss': 0.1304, 'learning_rate': 0.0001943004648657162, 'epoch': 0.2}

 20%|█▉        | 390/1976 [5:55:42<23:56:19, 54.34s/it]
 20%|█▉        | 391/1976 [5:56:36<23:54:19, 54.30s/it]
 20%|█▉        | 392/1976 [5:57:30<23:50:08, 54.17s/it]
 20%|█▉        | 393/1976 [5:58:24<23:49:10, 54.17s/it]
 20%|█▉        | 394/1976 [5:59:18<23:46:07, 54.09s/it]
 20%|█▉        | 395/1976 [6:00:12<23:45:58, 54.12s/it]
                                                       
{'loss': 0.129, 'learning_rate': 0.00019400279035175033, 'epoch': 0.2}

 20%|█▉        | 395/1976 [6:00:12<23:45:58, 54.12s/it]
 20%|██        | 396/1976 [6:01:07<23:48:34, 54.25s/it]
 20%|██        | 397/1976 [6:02:01<23:48:19, 54.27s/it]
 20%|██        | 398/1976 [6:02:56<23:49:48, 54.37s/it]
 20%|██        | 399/1976 [6:03:50<23:48:35, 54.35s/it]
 20%|██        | 400/1976 [6:04:45<23:49:40, 54.43s/it]
                                                       
{'loss': 0.1297, 'learning_rate': 0.0001936977789114045, 'epoch': 0.2}

 20%|██        | 400/1976 [6:04:45<23:49:40, 54.43s/it][INFO|trainer.py:2889] 2024-02-11 10:42:20,039 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-400
[INFO|tokenization_utils_base.py:2432] 2024-02-11 10:42:20,323 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 10:42:20,324 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-400/special_tokens_map.json
[2024-02-11 10:42:20,420] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 10:42:24,462] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 10:42:24,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 10:42:35,865] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 10:42:36,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 10:42:37,568] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 10:42:37,568] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 10:42:37,638] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|trainer.py:2979] 2024-02-11 10:42:37,642 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-300] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 20%|██        | 401/1976 [6:06:26<29:56:07, 68.42s/it]
 20%|██        | 402/1976 [6:07:20<28:03:05, 64.16s/it]
 20%|██        | 403/1976 [6:08:14<26:45:56, 61.26s/it]
 20%|██        | 404/1976 [6:09:08<25:47:40, 59.07s/it]
 20%|██        | 405/1976 [6:10:03<25:11:26, 57.73s/it]
                                                       
{'loss': 0.1303, 'learning_rate': 0.0001933854543508495, 'epoch': 0.2}

 20%|██        | 405/1976 [6:10:03<25:11:26, 57.73s/it]
 21%|██        | 406/1976 [6:10:57<24:44:06, 56.72s/it]
 21%|██        | 407/1976 [6:11:52<24:29:05, 56.18s/it]
 21%|██        | 408/1976 [6:12:47<24:15:19, 55.69s/it]
 21%|██        | 409/1976 [6:13:41<24:05:50, 55.36s/it]
 21%|██        | 410/1976 [6:14:36<24:01:21, 55.22s/it]
                                                       
{'loss': 0.1271, 'learning_rate': 0.0001930658410470458, 'epoch': 0.21}

 21%|██        | 410/1976 [6:14:36<24:01:21, 55.22s/it]
 21%|██        | 411/1976 [6:15:30<23:50:34, 54.85s/it]
 21%|██        | 412/1976 [6:16:25<23:46:47, 54.74s/it]
 21%|██        | 413/1976 [6:17:19<23:39:45, 54.50s/it]
 21%|██        | 414/1976 [6:18:13<23:39:01, 54.51s/it]
 21%|██        | 415/1976 [6:19:08<23:37:09, 54.47s/it]
                                                       
{'loss': 0.1254, 'learning_rate': 0.00019273896394584103, 'epoch': 0.21}

 21%|██        | 415/1976 [6:19:08<23:37:09, 54.47s/it]
 21%|██        | 416/1976 [6:20:02<23:35:06, 54.43s/it]
 21%|██        | 417/1976 [6:20:57<23:36:55, 54.53s/it]
 21%|██        | 418/1976 [6:21:51<23:34:13, 54.46s/it]
 21%|██        | 419/1976 [6:22:46<23:35:40, 54.55s/it]
 21%|██▏       | 420/1976 [6:23:40<23:32:55, 54.48s/it]
                                                       
{'loss': 0.1264, 'learning_rate': 0.0001924048485600228, 'epoch': 0.21}

 21%|██▏       | 420/1976 [6:23:40<23:32:55, 54.48s/it]
 21%|██▏       | 421/1976 [6:24:35<23:31:34, 54.47s/it]
 21%|██▏       | 422/1976 [6:25:29<23:30:55, 54.48s/it]
 21%|██▏       | 423/1976 [6:26:23<23:26:02, 54.32s/it]
 21%|██▏       | 424/1976 [6:27:18<23:26:54, 54.39s/it]
 22%|██▏       | 425/1976 [6:28:11<23:22:32, 54.26s/it]
                                                       
{'loss': 0.1264, 'learning_rate': 0.00019206352096732757, 'epoch': 0.22}

 22%|██▏       | 425/1976 [6:28:11<23:22:32, 54.26s/it]
 22%|██▏       | 426/1976 [6:29:06<23:26:25, 54.44s/it]
 22%|██▏       | 427/1976 [6:30:01<23:24:43, 54.41s/it]
 22%|██▏       | 428/1976 [6:30:56<23:27:06, 54.54s/it]
 22%|██▏       | 429/1976 [6:31:50<23:27:57, 54.61s/it]
 22%|██▏       | 430/1976 [6:32:45<23:24:47, 54.52s/it]
                                                       
{'loss': 0.1259, 'learning_rate': 0.00019171500780840504, 'epoch': 0.22}

 22%|██▏       | 430/1976 [6:32:45<23:24:47, 54.52s/it]
 22%|██▏       | 431/1976 [6:33:39<23:23:15, 54.50s/it]
 22%|██▏       | 432/1976 [6:34:33<23:17:57, 54.32s/it]
 22%|██▏       | 433/1976 [6:35:27<23:17:59, 54.36s/it]
 22%|██▏       | 434/1976 [6:36:21<23:13:47, 54.23s/it]
 22%|██▏       | 435/1976 [6:37:16<23:13:41, 54.26s/it]
                                                       
{'loss': 0.1244, 'learning_rate': 0.0001913593362847392, 'epoch': 0.22}

 22%|██▏       | 435/1976 [6:37:16<23:13:41, 54.26s/it]
 22%|██▏       | 436/1976 [6:38:10<23:16:36, 54.41s/it]
 22%|██▏       | 437/1976 [6:39:05<23:14:50, 54.38s/it]
 22%|██▏       | 438/1976 [6:40:00<23:16:47, 54.49s/it]
 22%|██▏       | 439/1976 [6:40:54<23:14:25, 54.43s/it]
 22%|██▏       | 440/1976 [6:41:49<23:16:30, 54.55s/it]
                                                       
{'loss': 0.1229, 'learning_rate': 0.00019099653415652497, 'epoch': 0.22}

 22%|██▏       | 440/1976 [6:41:49<23:16:30, 54.55s/it]
 22%|██▏       | 441/1976 [6:42:43<23:10:33, 54.35s/it]
 22%|██▏       | 442/1976 [6:43:37<23:09:52, 54.36s/it]
 22%|██▏       | 443/1976 [6:44:31<23:10:39, 54.43s/it]
 22%|██▏       | 444/1976 [6:45:25<23:06:12, 54.29s/it]
 23%|██▎       | 445/1976 [6:46:20<23:06:17, 54.33s/it]
                                                       
{'loss': 0.1246, 'learning_rate': 0.00019062662974050164, 'epoch': 0.23}

 23%|██▎       | 445/1976 [6:46:20<23:06:17, 54.33s/it]
 23%|██▎       | 446/1976 [6:47:14<23:05:24, 54.33s/it]
 23%|██▎       | 447/1976 [6:48:09<23:08:52, 54.50s/it]
 23%|██▎       | 448/1976 [6:49:03<23:06:45, 54.45s/it]
 23%|██▎       | 449/1976 [6:49:58<23:08:30, 54.56s/it]
 23%|██▎       | 450/1976 [6:50:53<23:09:51, 54.65s/it]
                                                       
{'loss': 0.1227, 'learning_rate': 0.00019024965190774263, 'epoch': 0.23}

 23%|██▎       | 450/1976 [6:50:53<23:09:51, 54.65s/it]
 23%|██▎       | 451/1976 [6:51:47<23:03:40, 54.44s/it]
 23%|██▎       | 452/1976 [6:52:42<23:03:04, 54.45s/it]
 23%|██▎       | 453/1976 [6:53:35<22:58:17, 54.30s/it]
 23%|██▎       | 454/1976 [6:54:30<22:58:26, 54.34s/it]
 23%|██▎       | 455/1976 [6:55:24<22:54:11, 54.21s/it]
                                                       
{'loss': 0.1261, 'learning_rate': 0.00018986563008140234, 'epoch': 0.23}

 23%|██▎       | 455/1976 [6:55:24<22:54:11, 54.21s/it]
 23%|██▎       | 456/1976 [6:56:19<22:57:07, 54.36s/it]
 23%|██▎       | 457/1976 [6:57:13<23:00:22, 54.52s/it]
 23%|██▎       | 458/1976 [6:58:08<22:57:36, 54.45s/it]
 23%|██▎       | 459/1976 [6:59:02<22:58:27, 54.52s/it]
 23%|██▎       | 460/1976 [6:59:57<22:55:53, 54.45s/it]
                                                       
{'loss': 0.1222, 'learning_rate': 0.00018947459423441934, 'epoch': 0.23}

 23%|██▎       | 460/1976 [6:59:57<22:55:53, 54.45s/it]
 23%|██▎       | 461/1976 [7:00:51<22:54:52, 54.45s/it]
 23%|██▎       | 462/1976 [7:01:45<22:50:03, 54.30s/it]
 23%|██▎       | 463/1976 [7:02:40<22:51:03, 54.37s/it]
 23%|██▎       | 464/1976 [7:03:34<22:50:36, 54.39s/it]
 24%|██▎       | 465/1976 [7:04:28<22:46:16, 54.25s/it]
                                                       
{'loss': 0.1219, 'learning_rate': 0.00018907657488717727, 'epoch': 0.24}

 24%|██▎       | 465/1976 [7:04:28<22:46:16, 54.25s/it]
 24%|██▎       | 466/1976 [7:05:23<22:49:14, 54.41s/it]
 24%|██▎       | 467/1976 [7:06:17<22:47:35, 54.38s/it]
 24%|██▎       | 468/1976 [7:07:12<22:48:59, 54.47s/it]
 24%|██▎       | 469/1976 [7:08:06<22:46:46, 54.42s/it]
 24%|██▍       | 470/1976 [7:09:01<22:48:28, 54.52s/it]
                                                       
{'loss': 0.1242, 'learning_rate': 0.00018867160310512255, 'epoch': 0.24}

 24%|██▍       | 470/1976 [7:09:01<22:48:28, 54.52s/it]
 24%|██▍       | 471/1976 [7:09:55<22:47:23, 54.51s/it]
 24%|██▍       | 472/1976 [7:10:49<22:42:28, 54.35s/it]
 24%|██▍       | 473/1976 [7:11:44<22:43:12, 54.42s/it]
 24%|██▍       | 474/1976 [7:12:38<22:38:39, 54.27s/it]
 24%|██▍       | 475/1976 [7:13:32<22:39:20, 54.34s/it]
                                                       
{'loss': 0.1231, 'learning_rate': 0.00018825971049633982, 'epoch': 0.24}

 24%|██▍       | 475/1976 [7:13:32<22:39:20, 54.34s/it]
 24%|██▍       | 476/1976 [7:14:27<22:38:19, 54.33s/it]
 24%|██▍       | 477/1976 [7:15:21<22:41:18, 54.49s/it]
 24%|██▍       | 478/1976 [7:16:16<22:42:35, 54.58s/it]
 24%|██▍       | 479/1976 [7:17:11<22:39:44, 54.50s/it]
 24%|██▍       | 480/1976 [7:18:05<22:41:09, 54.59s/it]
                                                       
{'loss': 0.1216, 'learning_rate': 0.0001878409292090848, 'epoch': 0.24}

 24%|██▍       | 480/1976 [7:18:05<22:41:09, 54.59s/it]
 24%|██▍       | 481/1976 [7:18:59<22:35:57, 54.42s/it]
 24%|██▍       | 482/1976 [7:19:54<22:35:49, 54.45s/it]
 24%|██▍       | 483/1976 [7:20:48<22:31:23, 54.31s/it]
 24%|██▍       | 484/1976 [7:21:42<22:31:50, 54.36s/it]
 25%|██▍       | 485/1976 [7:22:37<22:31:48, 54.40s/it]
                                                       
{'loss': 0.121, 'learning_rate': 0.00018741529192927526, 'epoch': 0.25}

 25%|██▍       | 485/1976 [7:22:37<22:31:48, 54.40s/it]
 25%|██▍       | 486/1976 [7:23:31<22:30:10, 54.37s/it]
 25%|██▍       | 487/1976 [7:24:26<22:32:29, 54.50s/it]
 25%|██▍       | 488/1976 [7:25:20<22:29:45, 54.43s/it]
 25%|██▍       | 489/1976 [7:26:15<22:30:48, 54.50s/it]
 25%|██▍       | 490/1976 [7:27:09<22:28:01, 54.43s/it]
                                                       
{'loss': 0.1183, 'learning_rate': 0.00018698283187793986, 'epoch': 0.25}

 25%|██▍       | 490/1976 [7:27:09<22:28:01, 54.43s/it]
 25%|██▍       | 491/1976 [7:28:04<22:26:30, 54.40s/it]
 25%|██▍       | 492/1976 [7:28:58<22:25:26, 54.40s/it]
 25%|██▍       | 493/1976 [7:29:52<22:21:02, 54.26s/it]
 25%|██▌       | 494/1976 [7:30:46<22:21:10, 54.30s/it]
 25%|██▌       | 495/1976 [7:31:40<22:17:28, 54.19s/it]
                                                       
{'loss': 0.1217, 'learning_rate': 0.00018654358280862504, 'epoch': 0.25}

 25%|██▌       | 495/1976 [7:31:40<22:17:28, 54.19s/it]
 25%|██▌       | 496/1976 [7:32:35<22:20:21, 54.34s/it]
 25%|██▌       | 497/1976 [7:33:29<22:19:18, 54.33s/it]
 25%|██▌       | 498/1976 [7:34:24<22:21:44, 54.47s/it]
 25%|██▌       | 499/1976 [7:35:19<22:22:41, 54.54s/it]
 25%|██▌       | 500/1976 [7:36:13<22:20:02, 54.47s/it]
                                                       
{'loss': 0.1196, 'learning_rate': 0.00018609757900476086, 'epoch': 0.25}

 25%|██▌       | 500/1976 [7:36:13<22:20:02, 54.47s/it][INFO|trainer.py:2889] 2024-02-11 12:13:48,365 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-500
[INFO|tokenization_utils_base.py:2432] 2024-02-11 12:13:48,649 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 12:13:48,649 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-500/special_tokens_map.json
[2024-02-11 12:13:49,785] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 12:13:54,288] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 12:13:54,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 12:14:05,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 12:14:06,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 12:14:06,762] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 12:14:06,762] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 12:14:06,922] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
[INFO|trainer.py:2979] 2024-02-11 12:14:06,926 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-400] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 25%|██▌       | 501/1976 [7:37:54<28:05:29, 68.56s/it]
 25%|██▌       | 502/1976 [7:38:48<26:16:09, 64.16s/it]
 25%|██▌       | 503/1976 [7:39:43<25:02:29, 61.20s/it]
 26%|██▌       | 504/1976 [7:40:37<24:09:59, 59.10s/it]
 26%|██▌       | 505/1976 [7:41:31<23:33:14, 57.64s/it]
                                                       
{'loss': 0.1197, 'learning_rate': 0.00018564485527698498, 'epoch': 0.26}

 26%|██▌       | 505/1976 [7:41:31<23:33:14, 57.64s/it]
 26%|██▌       | 506/1976 [7:42:26<23:11:45, 56.81s/it]
 26%|██▌       | 507/1976 [7:43:20<22:52:16, 56.05s/it]
 26%|██▌       | 508/1976 [7:44:15<22:42:18, 55.68s/it]
 26%|██▌       | 509/1976 [7:45:09<22:31:53, 55.29s/it]
 26%|██▌       | 510/1976 [7:46:04<22:28:25, 55.19s/it]
                                                       
{'loss': 0.1182, 'learning_rate': 0.00018518544696042577, 'epoch': 0.26}

 26%|██▌       | 510/1976 [7:46:04<22:28:25, 55.19s/it]
 26%|██▌       | 511/1976 [7:46:59<22:20:44, 54.91s/it]
 26%|██▌       | 512/1976 [7:47:53<22:14:57, 54.71s/it]
 26%|██▌       | 513/1976 [7:48:47<22:12:58, 54.67s/it]
 26%|██▌       | 514/1976 [7:49:41<22:07:04, 54.46s/it]
 26%|██▌       | 515/1976 [7:50:36<22:06:58, 54.50s/it]
                                                       
{'loss': 0.1177, 'learning_rate': 0.00018471938991194428, 'epoch': 0.26}

 26%|██▌       | 515/1976 [7:50:36<22:06:58, 54.50s/it]
 26%|██▌       | 516/1976 [7:51:30<22:04:53, 54.45s/it]
 26%|██▌       | 517/1976 [7:52:25<22:07:07, 54.58s/it]
 26%|██▌       | 518/1976 [7:53:20<22:08:36, 54.68s/it]
 26%|██▋       | 519/1976 [7:54:14<22:05:02, 54.57s/it]
 26%|██▋       | 520/1976 [7:55:09<22:05:57, 54.64s/it]
                                                       
{'loss': 0.1185, 'learning_rate': 0.00018424672050733576, 'epoch': 0.26}

 26%|██▋       | 520/1976 [7:55:09<22:05:57, 54.64s/it]
 26%|██▋       | 521/1976 [7:56:03<22:00:06, 54.44s/it]
 26%|██▋       | 522/1976 [7:56:58<21:59:23, 54.45s/it]
 26%|██▋       | 523/1976 [7:57:52<21:55:00, 54.30s/it]
 27%|██▋       | 524/1976 [7:58:46<21:55:57, 54.38s/it]
 27%|██▋       | 525/1976 [7:59:41<21:55:20, 54.39s/it]
                                                       
{'loss': 0.1185, 'learning_rate': 0.00018376747563849044, 'epoch': 0.27}

 27%|██▋       | 525/1976 [7:59:41<21:55:20, 54.39s/it]
 27%|██▋       | 526/1976 [8:00:35<21:53:56, 54.37s/it]
 27%|██▋       | 527/1976 [8:01:30<21:56:41, 54.52s/it]
 27%|██▋       | 528/1976 [8:02:24<21:54:20, 54.46s/it]
 27%|██▋       | 529/1976 [8:03:19<21:56:57, 54.61s/it]
 27%|██▋       | 530/1976 [8:04:13<21:54:11, 54.53s/it]
                                                       
{'loss': 0.1174, 'learning_rate': 0.00018328169271051418, 'epoch': 0.27}

 27%|██▋       | 530/1976 [8:04:13<21:54:11, 54.53s/it]
 27%|██▋       | 531/1976 [8:05:08<21:53:21, 54.53s/it]
 27%|██▋       | 532/1976 [8:06:02<21:52:24, 54.53s/it]
 27%|██▋       | 533/1976 [8:06:56<21:47:28, 54.36s/it]
 27%|██▋       | 534/1976 [8:07:51<21:46:54, 54.38s/it]
 27%|██▋       | 535/1976 [8:08:45<21:42:46, 54.24s/it]
                                                       
{'loss': 0.1162, 'learning_rate': 0.0001827894096388089, 'epoch': 0.27}

 27%|██▋       | 535/1976 [8:08:45<21:42:46, 54.24s/it]
 27%|██▋       | 536/1976 [8:09:40<21:45:43, 54.41s/it]
 27%|██▋       | 537/1976 [8:10:34<21:44:02, 54.37s/it]
 27%|██▋       | 538/1976 [8:11:29<21:46:18, 54.51s/it]
 27%|██▋       | 539/1976 [8:12:23<21:47:16, 54.58s/it]
 27%|██▋       | 540/1976 [8:13:18<21:44:19, 54.50s/it]
                                                       
{'loss': 0.1176, 'learning_rate': 0.0001822906648461133, 'epoch': 0.27}

 27%|██▋       | 540/1976 [8:13:18<21:44:19, 54.50s/it]
 27%|██▋       | 541/1976 [8:14:12<21:42:31, 54.46s/it]
 27%|██▋       | 542/1976 [8:15:06<21:37:37, 54.29s/it]
 27%|██▋       | 543/1976 [8:16:00<21:37:15, 54.32s/it]
 28%|██▊       | 544/1976 [8:16:54<21:33:42, 54.21s/it]
 28%|██▊       | 545/1976 [8:17:49<21:34:20, 54.27s/it]
                                                       
{'loss': 0.117, 'learning_rate': 0.00018178549725950412, 'epoch': 0.28}

 28%|██▊       | 545/1976 [8:17:49<21:34:20, 54.27s/it]
 28%|██▊       | 546/1976 [8:18:44<21:37:11, 54.43s/it]
 28%|██▊       | 547/1976 [8:19:38<21:35:16, 54.39s/it]
 28%|██▊       | 548/1976 [8:20:32<21:36:12, 54.46s/it]
 28%|██▊       | 549/1976 [8:21:27<21:34:10, 54.42s/it]
 28%|██▊       | 550/1976 [8:22:22<21:35:53, 54.53s/it]
                                                       
{'loss': 0.1172, 'learning_rate': 0.0001812739463073576, 'epoch': 0.28}

 28%|██▊       | 550/1976 [8:22:22<21:35:53, 54.53s/it]
 28%|██▊       | 551/1976 [8:23:16<21:30:53, 54.35s/it]
 28%|██▊       | 552/1976 [8:24:10<21:30:22, 54.37s/it]
 28%|██▊       | 553/1976 [8:25:04<21:30:46, 54.42s/it]
 28%|██▊       | 554/1976 [8:25:58<21:26:33, 54.29s/it]
 28%|██▊       | 555/1976 [8:26:53<21:26:30, 54.32s/it]
                                                       
{'loss': 0.1169, 'learning_rate': 0.0001807560519162724, 'epoch': 0.28}

 28%|██▊       | 555/1976 [8:26:53<21:26:30, 54.32s/it]
 28%|██▊       | 556/1976 [8:27:47<21:24:59, 54.30s/it]
 28%|██▊       | 557/1976 [8:28:42<21:27:19, 54.43s/it]
 28%|██▊       | 558/1976 [8:29:36<21:25:13, 54.38s/it]
 28%|██▊       | 559/1976 [8:30:31<21:26:31, 54.48s/it]
 28%|██▊       | 560/1976 [8:31:26<21:27:50, 54.57s/it]
                                                       
{'loss': 0.1172, 'learning_rate': 0.00018023185450795302, 'epoch': 0.28}

 28%|██▊       | 560/1976 [8:31:26<21:27:50, 54.57s/it]
 28%|██▊       | 561/1976 [8:32:19<21:22:10, 54.37s/it]
 28%|██▊       | 562/1976 [8:33:14<21:21:15, 54.37s/it]
 28%|██▊       | 563/1976 [8:34:08<21:17:11, 54.23s/it]
 29%|██▊       | 564/1976 [8:35:02<21:17:32, 54.29s/it]
 29%|██▊       | 565/1976 [8:35:56<21:14:18, 54.19s/it]
                                                       
{'loss': 0.1154, 'learning_rate': 0.0001797013949960551, 'epoch': 0.29}

 29%|██▊       | 565/1976 [8:35:56<21:14:18, 54.19s/it]
 29%|██▊       | 566/1976 [8:36:51<21:17:56, 54.38s/it]
 29%|██▊       | 567/1976 [8:37:46<21:19:53, 54.50s/it]
 29%|██▊       | 568/1976 [8:38:40<21:17:46, 54.45s/it]
 29%|██▉       | 569/1976 [8:39:35<21:18:41, 54.53s/it]
 29%|██▉       | 570/1976 [8:40:29<21:16:01, 54.45s/it]
                                                       
{'loss': 0.1161, 'learning_rate': 0.00017916471478299204, 'epoch': 0.29}

 29%|██▉       | 570/1976 [8:40:29<21:16:01, 54.45s/it]
 29%|██▉       | 571/1976 [8:41:23<21:15:01, 54.45s/it]
 29%|██▉       | 572/1976 [8:42:17<21:10:15, 54.28s/it]
 29%|██▉       | 573/1976 [8:43:12<21:09:40, 54.30s/it]
 29%|██▉       | 574/1976 [8:44:06<21:09:40, 54.34s/it]
 29%|██▉       | 575/1976 [8:45:00<21:05:57, 54.22s/it]
                                                       
{'loss': 0.1142, 'learning_rate': 0.00017862185575670357, 'epoch': 0.29}

 29%|██▉       | 575/1976 [8:45:00<21:05:57, 54.22s/it]
 29%|██▉       | 576/1976 [8:45:55<21:08:36, 54.37s/it]
 29%|██▉       | 577/1976 [8:46:49<21:06:54, 54.33s/it]
 29%|██▉       | 578/1976 [8:47:44<21:09:12, 54.47s/it]
 29%|██▉       | 579/1976 [8:48:38<21:07:04, 54.42s/it]
 29%|██▉       | 580/1976 [8:49:33<21:08:17, 54.51s/it]
                                                       
{'loss': 0.1157, 'learning_rate': 0.00017807286028738624, 'epoch': 0.29}

 29%|██▉       | 580/1976 [8:49:33<21:08:17, 54.51s/it]
 29%|██▉       | 581/1976 [8:50:27<21:06:56, 54.49s/it]
 29%|██▉       | 582/1976 [8:51:21<21:02:33, 54.34s/it]
 30%|██▉       | 583/1976 [8:52:16<21:02:56, 54.40s/it]
 30%|██▉       | 584/1976 [8:53:10<20:59:08, 54.27s/it]
 30%|██▉       | 585/1976 [8:54:04<20:59:10, 54.31s/it]
                                                       
{'loss': 0.114, 'learning_rate': 0.00017751777122418667, 'epoch': 0.3}

 30%|██▉       | 585/1976 [8:54:04<20:59:10, 54.31s/it]
 30%|██▉       | 586/1976 [8:54:59<20:58:28, 54.32s/it]
 30%|██▉       | 587/1976 [8:55:53<21:00:25, 54.45s/it]
 30%|██▉       | 588/1976 [8:56:48<21:01:52, 54.55s/it]
 30%|██▉       | 589/1976 [8:57:42<20:59:08, 54.47s/it]
 30%|██▉       | 590/1976 [8:58:37<21:00:07, 54.55s/it]
                                                       
{'loss': 0.1141, 'learning_rate': 0.000176956631891857, 'epoch': 0.3}

 30%|██▉       | 590/1976 [8:58:37<21:00:07, 54.55s/it]
 30%|██▉       | 591/1976 [8:59:31<20:54:43, 54.36s/it]
 30%|██▉       | 592/1976 [9:00:25<20:53:26, 54.34s/it]
 30%|███       | 593/1976 [9:01:19<20:49:46, 54.22s/it]
 30%|███       | 594/1976 [9:02:14<20:49:29, 54.25s/it]
 30%|███       | 595/1976 [9:03:08<20:49:52, 54.30s/it]
                                                       
{'loss': 0.1146, 'learning_rate': 0.0001763894860873734, 'epoch': 0.3}

 30%|███       | 595/1976 [9:03:08<20:49:52, 54.30s/it]
 30%|███       | 596/1976 [9:04:02<20:48:36, 54.29s/it]
 30%|███       | 597/1976 [9:04:57<20:50:19, 54.40s/it]
 30%|███       | 598/1976 [9:05:51<20:48:24, 54.36s/it]
 30%|███       | 599/1976 [9:06:46<20:49:20, 54.44s/it]
 30%|███       | 600/1976 [9:07:40<20:47:28, 54.40s/it]
                                                       
{'loss': 0.1129, 'learning_rate': 0.00017581637807651768, 'epoch': 0.3}

 30%|███       | 600/1976 [9:07:40<20:47:28, 54.40s/it][INFO|trainer.py:2889] 2024-02-11 13:45:14,731 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-600
[INFO|tokenization_utils_base.py:2432] 2024-02-11 13:45:15,009 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 13:45:15,009 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-600/special_tokens_map.json
[2024-02-11 13:45:15,942] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 13:45:20,122] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 13:45:20,122] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 13:45:31,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 13:45:32,064] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 13:45:32,478] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 13:45:32,478] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 13:45:32,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[INFO|trainer.py:2979] 2024-02-11 13:45:32,645 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-500] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 30%|███       | 601/1976 [9:09:20<25:56:42, 67.93s/it]
 30%|███       | 602/1976 [9:10:14<24:21:51, 63.84s/it]
 31%|███       | 603/1976 [9:11:08<23:12:37, 60.86s/it]
 31%|███       | 604/1976 [9:12:02<22:26:37, 58.89s/it]
 31%|███       | 605/1976 [9:12:56<21:53:39, 57.49s/it]
                                                       
{'loss': 0.1147, 'learning_rate': 0.00017523735259042256, 'epoch': 0.31}

 31%|███       | 605/1976 [9:12:56<21:53:39, 57.49s/it]
 31%|███       | 606/1976 [9:13:51<21:32:33, 56.61s/it]
 31%|███       | 607/1976 [9:14:46<21:18:18, 56.03s/it]
 31%|███       | 608/1976 [9:15:40<21:05:14, 55.49s/it]
 31%|███       | 609/1976 [9:16:35<20:59:23, 55.28s/it]
 31%|███       | 610/1976 [9:17:29<20:51:57, 54.99s/it]
                                                       
{'loss': 0.1125, 'learning_rate': 0.00017465245482208017, 'epoch': 0.31}

 31%|███       | 610/1976 [9:17:29<20:51:57, 54.99s/it]
 31%|███       | 611/1976 [9:18:23<20:47:31, 54.84s/it]
 31%|███       | 612/1976 [9:19:18<20:42:18, 54.65s/it]
 31%|███       | 613/1976 [9:20:12<20:38:15, 54.51s/it]
 31%|███       | 614/1976 [9:21:06<20:36:35, 54.48s/it]
 31%|███       | 615/1976 [9:22:00<20:31:37, 54.30s/it]
                                                       
{'loss': 0.113, 'learning_rate': 0.00017406173042281472, 'epoch': 0.31}

 31%|███       | 615/1976 [9:22:00<20:31:37, 54.30s/it]
 31%|███       | 616/1976 [9:22:55<20:34:10, 54.45s/it]
 31%|███       | 617/1976 [9:23:49<20:32:02, 54.39s/it]
 31%|███▏      | 618/1976 [9:24:44<20:33:08, 54.48s/it]
 31%|███▏      | 619/1976 [9:25:38<20:32:13, 54.48s/it]
 31%|███▏      | 620/1976 [9:26:33<20:31:08, 54.48s/it]
                                                       
{'loss': 0.1134, 'learning_rate': 0.00017346522549871964, 'epoch': 0.31}

 31%|███▏      | 620/1976 [9:26:33<20:31:08, 54.48s/it]
 31%|███▏      | 621/1976 [9:27:27<20:30:21, 54.48s/it]
 31%|███▏      | 622/1976 [9:28:21<20:25:43, 54.32s/it]
 32%|███▏      | 623/1976 [9:29:16<20:25:22, 54.34s/it]
 32%|███▏      | 624/1976 [9:30:09<20:21:33, 54.21s/it]
 32%|███▏      | 625/1976 [9:31:04<20:22:23, 54.29s/it]
                                                       
{'loss': 0.1122, 'learning_rate': 0.00017286298660705875, 'epoch': 0.32}

 32%|███▏      | 625/1976 [9:31:04<20:22:23, 54.29s/it]
 32%|███▏      | 626/1976 [9:31:58<20:22:52, 54.35s/it]
 32%|███▏      | 627/1976 [9:32:53<20:22:37, 54.38s/it]
 32%|███▏      | 628/1976 [9:33:48<20:24:14, 54.49s/it]
 32%|███▏      | 629/1976 [9:34:42<20:21:44, 54.42s/it]
 32%|███▏      | 630/1976 [9:35:37<20:23:00, 54.52s/it]
                                                       
{'loss': 0.1124, 'learning_rate': 0.0001722550607526326, 'epoch': 0.32}

 32%|███▏      | 630/1976 [9:35:37<20:23:00, 54.52s/it]
 32%|███▏      | 631/1976 [9:36:31<20:18:05, 54.34s/it]
 32%|███▏      | 632/1976 [9:37:25<20:17:49, 54.37s/it]
 32%|███▏      | 633/1976 [9:38:19<20:15:47, 54.32s/it]
 32%|███▏      | 634/1976 [9:39:13<20:13:40, 54.26s/it]
 32%|███▏      | 635/1976 [9:40:08<20:13:52, 54.31s/it]
                                                       
{'loss': 0.1104, 'learning_rate': 0.00017164149538410973, 'epoch': 0.32}

 32%|███▏      | 635/1976 [9:40:08<20:13:52, 54.31s/it]
 32%|███▏      | 636/1976 [9:41:02<20:12:53, 54.31s/it]
 32%|███▏      | 637/1976 [9:41:57<20:15:06, 54.45s/it]
 32%|███▏      | 638/1976 [9:42:51<20:13:19, 54.41s/it]
 32%|███▏      | 639/1976 [9:43:46<20:15:04, 54.53s/it]
 32%|███▏      | 640/1976 [9:44:41<20:14:35, 54.55s/it]
                                                       
{'loss': 0.1123, 'learning_rate': 0.00017102233839032317, 'epoch': 0.32}

 32%|███▏      | 640/1976 [9:44:41<20:14:35, 54.55s/it]
 32%|███▏      | 641/1976 [9:45:35<20:11:39, 54.46s/it]
 32%|███▏      | 642/1976 [9:46:29<20:11:02, 54.47s/it]
 33%|███▎      | 643/1976 [9:47:23<20:06:24, 54.30s/it]
 33%|███▎      | 644/1976 [9:48:18<20:05:27, 54.30s/it]
 33%|███▎      | 645/1976 [9:49:11<20:02:16, 54.20s/it]
                                                       
{'loss': 0.1126, 'learning_rate': 0.00017039763809653294, 'epoch': 0.33}

 33%|███▎      | 645/1976 [9:49:11<20:02:16, 54.20s/it]
 33%|███▎      | 646/1976 [9:50:06<20:05:26, 54.38s/it]
 33%|███▎      | 647/1976 [9:51:01<20:05:47, 54.44s/it]
 33%|███▎      | 648/1976 [9:51:55<20:05:20, 54.46s/it]
 33%|███▎      | 649/1976 [9:52:50<20:06:46, 54.56s/it]
 33%|███▎      | 650/1976 [9:53:44<20:04:17, 54.49s/it]
                                                       
{'loss': 0.1124, 'learning_rate': 0.0001697674432606541, 'epoch': 0.33}

 33%|███▎      | 650/1976 [9:53:44<20:04:17, 54.49s/it]
 33%|███▎      | 651/1976 [9:54:39<20:02:43, 54.46s/it]
 33%|███▎      | 652/1976 [9:55:33<19:58:27, 54.31s/it]
 33%|███▎      | 653/1976 [9:56:27<19:58:54, 54.37s/it]
 33%|███▎      | 654/1976 [9:57:21<19:56:33, 54.31s/it]
 33%|███▎      | 655/1976 [9:58:16<19:54:43, 54.26s/it]
                                                       
{'loss': 0.111, 'learning_rate': 0.00016913180306945113, 'epoch': 0.33}

 33%|███▎      | 655/1976 [9:58:16<19:54:43, 54.26s/it]
 33%|███▎      | 656/1976 [9:59:10<19:57:07, 54.42s/it]
 33%|███▎      | 657/1976 [10:00:05<19:55:18, 54.37s/it]
 33%|███▎      | 658/1976 [10:01:00<19:57:43, 54.52s/it]
 33%|███▎      | 659/1976 [10:01:54<19:55:09, 54.45s/it]
 33%|███▎      | 660/1976 [10:02:49<19:55:33, 54.51s/it]
                                                        
{'loss': 0.11, 'learning_rate': 0.00016849076713469914, 'epoch': 0.33}

 33%|███▎      | 660/1976 [10:02:49<19:55:33, 54.51s/it]
 33%|███▎      | 661/1976 [10:03:43<19:52:09, 54.40s/it]
 34%|███▎      | 662/1976 [10:04:37<19:49:41, 54.32s/it]
 34%|███▎      | 663/1976 [10:05:31<19:49:19, 54.35s/it]
 34%|███▎      | 664/1976 [10:06:25<19:45:39, 54.22s/it]
 34%|███▎      | 665/1976 [10:07:20<19:46:22, 54.30s/it]
                                                        
{'loss': 0.1126, 'learning_rate': 0.00016784438548931134, 'epoch': 0.34}

 34%|███▎      | 665/1976 [10:07:20<19:46:22, 54.30s/it]
 34%|███▎      | 666/1976 [10:08:14<19:45:32, 54.30s/it]
 34%|███▍      | 667/1976 [10:09:09<19:47:49, 54.45s/it]
 34%|███▍      | 668/1976 [10:10:03<19:47:06, 54.45s/it]
 34%|███▍      | 669/1976 [10:10:58<19:46:48, 54.48s/it]
 34%|███▍      | 670/1976 [10:11:52<19:47:21, 54.55s/it]
                                                        
{'loss': 0.1097, 'learning_rate': 0.00016719270858343429, 'epoch': 0.34}

 34%|███▍      | 670/1976 [10:11:52<19:47:21, 54.55s/it]
 34%|███▍      | 671/1976 [10:12:46<19:42:17, 54.36s/it]
 34%|███▍      | 672/1976 [10:13:41<19:41:45, 54.38s/it]
 34%|███▍      | 673/1976 [10:14:35<19:38:02, 54.25s/it]
 34%|███▍      | 674/1976 [10:15:29<19:37:58, 54.28s/it]
 34%|███▍      | 675/1976 [10:16:23<19:35:56, 54.23s/it]
                                                        
{'loss': 0.1098, 'learning_rate': 0.00016653578728051003, 'epoch': 0.34}

 34%|███▍      | 675/1976 [10:16:23<19:35:56, 54.23s/it]
 34%|███▍      | 676/1976 [10:17:18<19:36:53, 54.32s/it]
 34%|███▍      | 677/1976 [10:18:12<19:38:46, 54.45s/it]
 34%|███▍      | 678/1976 [10:19:07<19:36:30, 54.38s/it]
 34%|███▍      | 679/1976 [10:20:01<19:37:15, 54.46s/it]
 34%|███▍      | 680/1976 [10:20:56<19:35:15, 54.41s/it]
                                                        
{'loss': 0.1104, 'learning_rate': 0.0001658736728533064, 'epoch': 0.34}

 34%|███▍      | 680/1976 [10:20:56<19:35:15, 54.41s/it]
 34%|███▍      | 681/1976 [10:21:50<19:33:59, 54.39s/it]
 35%|███▍      | 682/1976 [10:22:44<19:31:39, 54.33s/it]
 35%|███▍      | 683/1976 [10:23:38<19:29:43, 54.28s/it]
 35%|███▍      | 684/1976 [10:24:33<19:29:50, 54.33s/it]
 35%|███▍      | 685/1976 [10:25:27<19:26:25, 54.21s/it]
                                                        
{'loss': 0.1081, 'learning_rate': 0.0001652064169799149, 'epoch': 0.35}

 35%|███▍      | 685/1976 [10:25:27<19:26:25, 54.21s/it]
 35%|███▍      | 686/1976 [10:26:21<19:29:16, 54.39s/it]
 35%|███▍      | 687/1976 [10:27:16<19:27:42, 54.35s/it]
 35%|███▍      | 688/1976 [10:28:11<19:29:23, 54.48s/it]
 35%|███▍      | 689/1976 [10:29:05<19:28:41, 54.48s/it]
 35%|███▍      | 690/1976 [10:30:00<19:28:20, 54.51s/it]
                                                        
{'loss': 0.1115, 'learning_rate': 0.00016453407173971742, 'epoch': 0.35}

 35%|███▍      | 690/1976 [10:30:00<19:28:20, 54.51s/it]
 35%|███▍      | 691/1976 [10:30:54<19:27:17, 54.50s/it]
 35%|███▌      | 692/1976 [10:31:48<19:22:48, 54.34s/it]
 35%|███▌      | 693/1976 [10:32:42<19:22:33, 54.37s/it]
 35%|███▌      | 694/1976 [10:33:36<19:18:59, 54.24s/it]
 35%|███▌      | 695/1976 [10:34:31<19:20:06, 54.34s/it]
                                                        
{'loss': 0.1103, 'learning_rate': 0.00016385668960932143, 'epoch': 0.35}

 35%|███▌      | 695/1976 [10:34:31<19:20:06, 54.34s/it]
 35%|███▌      | 696/1976 [10:35:26<19:21:04, 54.43s/it]
 35%|███▌      | 697/1976 [10:36:20<19:21:16, 54.48s/it]
 35%|███▌      | 698/1976 [10:37:15<19:23:05, 54.61s/it]
 35%|███▌      | 699/1976 [10:38:09<19:20:20, 54.52s/it]
 35%|███▌      | 700/1976 [10:39:04<19:21:33, 54.62s/it]
                                                        
{'loss': 0.1088, 'learning_rate': 0.0001631743234584642, 'epoch': 0.35}

 35%|███▌      | 700/1976 [10:39:04<19:21:33, 54.62s/it][INFO|trainer.py:2889] 2024-02-11 15:16:39,266 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-700
[INFO|tokenization_utils_base.py:2432] 2024-02-11 15:16:39,547 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 15:16:39,548 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-700/special_tokens_map.json
[2024-02-11 15:16:40,665] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step700 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 15:16:44,905] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-700/global_step700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 15:16:44,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-700/global_step700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 15:16:56,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-700/global_step700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 15:16:57,063] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-700/global_step700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 15:16:57,478] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-700/global_step700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 15:16:57,478] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-700/global_step700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 15:16:57,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step700 is ready now!
[INFO|trainer.py:2979] 2024-02-11 15:16:57,648 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-600] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 35%|███▌      | 701/1976 [10:40:45<24:14:38, 68.45s/it]
 36%|███▌      | 702/1976 [10:41:39<22:41:49, 64.14s/it]
 36%|███▌      | 703/1976 [10:42:33<21:36:49, 61.12s/it]
 36%|███▌      | 704/1976 [10:43:27<20:51:15, 59.02s/it]
 36%|███▌      | 705/1976 [10:44:22<20:20:35, 57.62s/it]
                                                        
{'loss': 0.1095, 'learning_rate': 0.00016248702654588607, 'epoch': 0.36}

 36%|███▌      | 705/1976 [10:44:22<20:20:35, 57.62s/it]
 36%|███▌      | 706/1976 [10:45:16<19:58:28, 56.62s/it]
 36%|███▌      | 707/1976 [10:46:11<19:45:49, 56.07s/it]
 36%|███▌      | 708/1976 [10:47:05<19:34:51, 55.59s/it]
 36%|███▌      | 709/1976 [10:48:00<19:26:51, 55.26s/it]
 36%|███▌      | 710/1976 [10:48:54<19:22:10, 55.08s/it]
                                                        
{'loss': 0.1089, 'learning_rate': 0.00016179485251517398, 'epoch': 0.36}

 36%|███▌      | 710/1976 [10:48:54<19:22:10, 55.08s/it]
 36%|███▌      | 711/1976 [10:49:48<19:13:59, 54.73s/it]
 36%|███▌      | 712/1976 [10:50:43<19:11:06, 54.64s/it]
 36%|███▌      | 713/1976 [10:51:37<19:05:53, 54.44s/it]
 36%|███▌      | 714/1976 [10:52:31<19:05:00, 54.44s/it]
 36%|███▌      | 715/1976 [10:53:25<19:02:06, 54.34s/it]
                                                        
{'loss': 0.1084, 'learning_rate': 0.00016109785539057428, 'epoch': 0.36}

 36%|███▌      | 715/1976 [10:53:25<19:02:06, 54.34s/it]
 36%|███▌      | 716/1976 [10:54:20<19:01:53, 54.38s/it]
 36%|███▋      | 717/1976 [10:55:14<19:03:11, 54.48s/it]
 36%|███▋      | 718/1976 [10:56:09<19:00:49, 54.41s/it]
 36%|███▋      | 719/1976 [10:57:03<19:01:24, 54.48s/it]
 36%|███▋      | 720/1976 [10:57:58<18:59:20, 54.43s/it]
                                                        
{'loss': 0.107, 'learning_rate': 0.0001603960895727762, 'epoch': 0.36}

 36%|███▋      | 720/1976 [10:57:58<18:59:20, 54.43s/it]
 36%|███▋      | 721/1976 [10:58:52<18:58:18, 54.42s/it]
 37%|███▋      | 722/1976 [10:59:46<18:55:23, 54.33s/it]
 37%|███▋      | 723/1976 [11:00:40<18:52:48, 54.24s/it]
 37%|███▋      | 724/1976 [11:01:35<18:53:10, 54.31s/it]
 37%|███▋      | 725/1976 [11:02:28<18:49:31, 54.17s/it]
                                                        
{'loss': 0.1075, 'learning_rate': 0.00015968960983466608, 'epoch': 0.37}

 37%|███▋      | 725/1976 [11:02:28<18:49:31, 54.17s/it]
 37%|███▋      | 726/1976 [11:03:23<18:51:50, 54.33s/it]
 37%|███▋      | 727/1976 [11:04:17<18:50:45, 54.32s/it]
 37%|███▋      | 728/1976 [11:05:12<18:52:11, 54.43s/it]
 37%|███▋      | 729/1976 [11:06:07<18:53:39, 54.55s/it]
 37%|███▋      | 730/1976 [11:07:01<18:50:55, 54.46s/it]
                                                        
{'loss': 0.1071, 'learning_rate': 0.00015897847131705195, 'epoch': 0.37}

 37%|███▋      | 730/1976 [11:07:01<18:50:55, 54.46s/it]
 37%|███▋      | 731/1976 [11:07:56<18:49:19, 54.43s/it]
 37%|███▋      | 732/1976 [11:08:49<18:45:16, 54.27s/it]
 37%|███▋      | 733/1976 [11:09:44<18:45:54, 54.35s/it]
 37%|███▋      | 734/1976 [11:10:38<18:42:21, 54.22s/it]
 37%|███▋      | 735/1976 [11:11:32<18:42:06, 54.25s/it]
                                                        
{'loss': 0.1087, 'learning_rate': 0.0001582627295243602, 'epoch': 0.37}

 37%|███▋      | 735/1976 [11:11:32<18:42:06, 54.25s/it]
 37%|███▋      | 736/1976 [11:12:27<18:44:53, 54.43s/it]
 37%|███▋      | 737/1976 [11:13:21<18:43:15, 54.40s/it]
 37%|███▋      | 738/1976 [11:14:16<18:44:29, 54.50s/it]
 37%|███▋      | 739/1976 [11:15:10<18:42:12, 54.43s/it]
 37%|███▋      | 740/1976 [11:16:05<18:43:04, 54.52s/it]
                                                        
{'loss': 0.1063, 'learning_rate': 0.00015754244032030313, 'epoch': 0.37}

 37%|███▋      | 740/1976 [11:16:05<18:43:04, 54.52s/it]
 38%|███▊      | 741/1976 [11:16:59<18:38:19, 54.33s/it]
 38%|███▊      | 742/1976 [11:17:53<18:37:20, 54.33s/it]
 38%|███▊      | 743/1976 [11:18:48<18:37:01, 54.36s/it]
 38%|███▊      | 744/1976 [11:19:42<18:33:09, 54.21s/it]
 38%|███▊      | 745/1976 [11:20:36<18:32:51, 54.24s/it]
                                                        
{'loss': 0.1077, 'learning_rate': 0.00015681765992351895, 'epoch': 0.38}

 38%|███▊      | 745/1976 [11:20:36<18:32:51, 54.24s/it]
 38%|███▊      | 746/1976 [11:21:30<18:32:06, 54.25s/it]
 38%|███▊      | 747/1976 [11:22:25<18:34:31, 54.41s/it]
 38%|███▊      | 748/1976 [11:23:19<18:32:46, 54.37s/it]
 38%|███▊      | 749/1976 [11:24:14<18:34:03, 54.48s/it]
 38%|███▊      | 750/1976 [11:25:09<18:34:44, 54.55s/it]
                                                        
{'loss': 0.1069, 'learning_rate': 0.00015608844490318393, 'epoch': 0.38}

 38%|███▊      | 750/1976 [11:25:09<18:34:44, 54.55s/it]
 38%|███▊      | 751/1976 [11:26:03<18:29:51, 54.36s/it]
 38%|███▊      | 752/1976 [11:26:57<18:29:33, 54.39s/it]
 38%|███▊      | 753/1976 [11:27:51<18:25:13, 54.22s/it]
 38%|███▊      | 754/1976 [11:28:45<18:24:15, 54.22s/it]
 38%|███▊      | 755/1976 [11:29:39<18:21:18, 54.12s/it]
                                                        
{'loss': 0.1058, 'learning_rate': 0.00015535485217459705, 'epoch': 0.38}

 38%|███▊      | 755/1976 [11:29:39<18:21:18, 54.12s/it]
 38%|███▊      | 756/1976 [11:30:34<18:23:56, 54.29s/it]
 38%|███▊      | 757/1976 [11:31:29<18:25:59, 54.44s/it]
 38%|███▊      | 758/1976 [11:32:23<18:23:55, 54.38s/it]
 38%|███▊      | 759/1976 [11:33:18<18:25:18, 54.49s/it]
 38%|███▊      | 760/1976 [11:34:12<18:23:00, 54.42s/it]
                                                        
{'loss': 0.1064, 'learning_rate': 0.00015461693899473778, 'epoch': 0.38}

 38%|███▊      | 760/1976 [11:34:12<18:23:00, 54.42s/it]
 39%|███▊      | 761/1976 [11:35:06<18:22:55, 54.47s/it]
 39%|███▊      | 762/1976 [11:36:00<18:18:33, 54.29s/it]
 39%|███▊      | 763/1976 [11:36:55<18:18:45, 54.35s/it]
 39%|███▊      | 764/1976 [11:37:49<18:19:13, 54.42s/it]
 39%|███▊      | 765/1976 [11:38:43<18:15:18, 54.27s/it]
                                                        
{'loss': 0.1075, 'learning_rate': 0.00015387476295779736, 'epoch': 0.39}

 39%|███▊      | 765/1976 [11:38:43<18:15:18, 54.27s/it]
 39%|███▉      | 766/1976 [11:39:38<18:18:26, 54.47s/it]
 39%|███▉      | 767/1976 [11:40:32<18:16:44, 54.43s/it]
 39%|███▉      | 768/1976 [11:41:27<18:18:36, 54.57s/it]
 39%|███▉      | 769/1976 [11:42:22<18:15:54, 54.48s/it]
 39%|███▉      | 770/1976 [11:43:16<18:17:13, 54.59s/it]
                                                        
{'loss': 0.1075, 'learning_rate': 0.00015312838199068327, 'epoch': 0.39}

 39%|███▉      | 770/1976 [11:43:17<18:17:13, 54.59s/it]
 39%|███▉      | 771/1976 [11:44:11<18:15:17, 54.54s/it]
 39%|███▉      | 772/1976 [11:45:05<18:10:07, 54.33s/it]
 39%|███▉      | 773/1976 [11:45:59<18:10:00, 54.36s/it]
 39%|███▉      | 774/1976 [11:46:53<18:05:58, 54.21s/it]
 39%|███▉      | 775/1976 [11:47:47<18:05:49, 54.25s/it]
                                                        
{'loss': 0.1059, 'learning_rate': 0.0001523778543484982, 'epoch': 0.39}

 39%|███▉      | 775/1976 [11:47:47<18:05:49, 54.25s/it]
 39%|███▉      | 776/1976 [11:48:42<18:04:52, 54.24s/it]
 39%|███▉      | 777/1976 [11:49:36<18:06:54, 54.39s/it]
 39%|███▉      | 778/1976 [11:50:31<18:08:12, 54.50s/it]
 39%|███▉      | 779/1976 [11:51:25<18:05:48, 54.43s/it]
 39%|███▉      | 780/1976 [11:52:20<18:06:59, 54.53s/it]
                                                        
{'loss': 0.1067, 'learning_rate': 0.00015162323860999326, 'epoch': 0.39}

 39%|███▉      | 780/1976 [11:52:20<18:06:59, 54.53s/it]
 40%|███▉      | 781/1976 [11:53:14<18:02:08, 54.33s/it]
 40%|███▉      | 782/1976 [11:54:08<18:01:41, 54.36s/it]
 40%|███▉      | 783/1976 [11:55:02<17:57:54, 54.21s/it]
 40%|███▉      | 784/1976 [11:55:57<17:58:12, 54.27s/it]
 40%|███▉      | 785/1976 [11:56:51<17:58:02, 54.31s/it]
                                                        
{'loss': 0.1064, 'learning_rate': 0.00015086459367299578, 'epoch': 0.4}

 40%|███▉      | 785/1976 [11:56:51<17:58:02, 54.31s/it]
 40%|███▉      | 786/1976 [11:57:45<17:56:49, 54.29s/it]
 40%|███▉      | 787/1976 [11:58:40<17:59:25, 54.47s/it]
 40%|███▉      | 788/1976 [11:59:34<17:57:09, 54.40s/it]
 40%|███▉      | 789/1976 [12:00:29<17:58:24, 54.51s/it]
 40%|███▉      | 790/1976 [12:01:24<17:58:02, 54.54s/it]
                                                        
{'loss': 0.1045, 'learning_rate': 0.00015010197874981248, 'epoch': 0.4}

 40%|███▉      | 790/1976 [12:01:24<17:58:02, 54.54s/it]
 40%|████      | 791/1976 [12:02:18<17:54:26, 54.40s/it]
 40%|████      | 792/1976 [12:03:12<17:53:07, 54.38s/it]
 40%|████      | 793/1976 [12:04:06<17:48:56, 54.22s/it]
 40%|████      | 794/1976 [12:05:00<17:48:34, 54.24s/it]
 40%|████      | 795/1976 [12:05:54<17:45:09, 54.11s/it]
                                                        
{'loss': 0.1019, 'learning_rate': 0.00014933545336260785, 'epoch': 0.4}

 40%|████      | 795/1976 [12:05:54<17:45:09, 54.11s/it]
 40%|████      | 796/1976 [12:06:49<17:48:08, 54.31s/it]
 40%|████      | 797/1976 [12:07:43<17:47:50, 54.34s/it]
 40%|████      | 798/1976 [12:08:38<17:47:24, 54.37s/it]
 40%|████      | 799/1976 [12:09:33<17:48:29, 54.47s/it]
 40%|████      | 800/1976 [12:10:27<17:45:51, 54.38s/it]
                                                        
{'loss': 0.106, 'learning_rate': 0.00014856507733875836, 'epoch': 0.4}

 40%|████      | 800/1976 [12:10:27<17:45:51, 54.38s/it][INFO|trainer.py:2889] 2024-02-11 16:48:01,655 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-800
[INFO|tokenization_utils_base.py:2432] 2024-02-11 16:48:01,936 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 16:48:01,936 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-800/special_tokens_map.json
[2024-02-11 16:48:02,030] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 16:48:05,955] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 16:48:05,956] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 16:48:17,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 16:48:17,896] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 16:48:18,462] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 16:48:18,462] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 16:48:18,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
[INFO|trainer.py:2979] 2024-02-11 16:48:18,552 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-700] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 41%|████      | 801/1976 [12:12:07<22:12:27, 68.04s/it]
 41%|████      | 802/1976 [12:13:01<20:48:08, 63.79s/it]
 41%|████      | 803/1976 [12:13:55<19:51:16, 60.93s/it]
 41%|████      | 804/1976 [12:14:49<19:11:40, 58.96s/it]
 41%|████      | 805/1976 [12:15:43<18:41:00, 57.44s/it]
                                                        
{'loss': 0.106, 'learning_rate': 0.00014779091080618318, 'epoch': 0.41}

 41%|████      | 805/1976 [12:15:43<18:41:00, 57.44s/it]
 41%|████      | 806/1976 [12:16:38<18:24:21, 56.63s/it]
 41%|████      | 807/1976 [12:17:32<18:09:25, 55.92s/it]
 41%|████      | 808/1976 [12:18:27<18:01:28, 55.56s/it]
 41%|████      | 809/1976 [12:19:21<17:54:32, 55.25s/it]
 41%|████      | 810/1976 [12:20:16<17:49:16, 55.02s/it]
                                                        
{'loss': 0.1036, 'learning_rate': 0.00014701301418865086, 'epoch': 0.41}

 41%|████      | 810/1976 [12:20:16<17:49:16, 55.02s/it]
 41%|████      | 811/1976 [12:21:10<17:44:38, 54.83s/it]
 41%|████      | 812/1976 [12:22:04<17:38:26, 54.56s/it]
 41%|████      | 813/1976 [12:22:58<17:36:25, 54.50s/it]
 41%|████      | 814/1976 [12:23:52<17:32:10, 54.33s/it]
 41%|████      | 815/1976 [12:24:47<17:32:13, 54.38s/it]
                                                        
{'loss': 0.1061, 'learning_rate': 0.0001462314482010636, 'epoch': 0.41}

 41%|████      | 815/1976 [12:24:47<17:32:13, 54.38s/it]
 41%|████▏     | 816/1976 [12:25:41<17:31:43, 54.40s/it]
 41%|████▏     | 817/1976 [12:26:36<17:31:23, 54.43s/it]
 41%|████▏     | 818/1976 [12:27:30<17:31:52, 54.50s/it]
 41%|████▏     | 819/1976 [12:28:25<17:29:13, 54.41s/it]
 41%|████▏     | 820/1976 [12:29:19<17:30:09, 54.51s/it]
                                                        
{'loss': 0.1039, 'learning_rate': 0.00014544627384471806, 'epoch': 0.41}

 41%|████▏     | 820/1976 [12:29:19<17:30:09, 54.51s/it]
 42%|████▏     | 821/1976 [12:30:13<17:25:47, 54.33s/it]
 42%|████▏     | 822/1976 [12:31:08<17:25:19, 54.35s/it]
 42%|████▏     | 823/1976 [12:32:02<17:23:05, 54.28s/it]
 42%|████▏     | 824/1976 [12:32:56<17:21:10, 54.23s/it]
 42%|████▏     | 825/1976 [12:33:50<17:21:00, 54.27s/it]
                                                        
{'loss': 0.1041, 'learning_rate': 0.00014465755240254463, 'epoch': 0.42}

 42%|████▏     | 825/1976 [12:33:50<17:21:00, 54.27s/it]
 42%|████▏     | 826/1976 [12:34:45<17:19:57, 54.26s/it]
 42%|████▏     | 827/1976 [12:35:39<17:21:36, 54.39s/it]
 42%|████▏     | 828/1976 [12:36:34<17:20:08, 54.36s/it]
 42%|████▏     | 829/1976 [12:37:28<17:22:08, 54.51s/it]
 42%|████▏     | 830/1976 [12:38:23<17:21:09, 54.51s/it]
                                                        
{'loss': 0.1023, 'learning_rate': 0.0001438653454343239, 'epoch': 0.42}

 42%|████▏     | 830/1976 [12:38:23<17:21:09, 54.51s/it]
 42%|████▏     | 831/1976 [12:39:17<17:18:07, 54.40s/it]
 42%|████▏     | 832/1976 [12:40:12<17:17:49, 54.43s/it]
 42%|████▏     | 833/1976 [12:41:06<17:14:16, 54.29s/it]
 42%|████▏     | 834/1976 [12:42:00<17:14:35, 54.36s/it]
 42%|████▏     | 835/1976 [12:42:54<17:11:19, 54.23s/it]
                                                        
{'loss': 0.1051, 'learning_rate': 0.00014306971477188223, 'epoch': 0.42}

 42%|████▏     | 835/1976 [12:42:54<17:11:19, 54.23s/it]
 42%|████▏     | 836/1976 [12:43:49<17:13:51, 54.41s/it]
 42%|████▏     | 837/1976 [12:44:43<17:13:38, 54.45s/it]
 42%|████▏     | 838/1976 [12:45:38<17:13:20, 54.48s/it]
 42%|████▏     | 839/1976 [12:46:33<17:14:09, 54.57s/it]
 43%|████▎     | 840/1976 [12:47:27<17:11:43, 54.49s/it]
                                                        
{'loss': 0.1026, 'learning_rate': 0.00014227072251426546, 'epoch': 0.42}

 43%|████▎     | 840/1976 [12:47:27<17:11:43, 54.49s/it]
 43%|████▎     | 841/1976 [12:48:22<17:11:23, 54.52s/it]
 43%|████▎     | 842/1976 [12:49:16<17:07:22, 54.36s/it]
 43%|████▎     | 843/1976 [12:50:10<17:07:26, 54.41s/it]
 43%|████▎     | 844/1976 [12:51:04<17:05:29, 54.36s/it]
 43%|████▎     | 845/1976 [12:51:59<17:03:53, 54.32s/it]
                                                        
{'loss': 0.1018, 'learning_rate': 0.00014146843102289243, 'epoch': 0.43}

 43%|████▎     | 845/1976 [12:51:59<17:03:53, 54.32s/it]
 43%|████▎     | 846/1976 [12:52:53<17:05:38, 54.46s/it]
 43%|████▎     | 847/1976 [12:53:48<17:03:56, 54.42s/it]
 43%|████▎     | 848/1976 [12:54:43<17:06:05, 54.58s/it]
 43%|████▎     | 849/1976 [12:55:37<17:04:03, 54.52s/it]
 43%|████▎     | 850/1976 [12:56:32<17:05:27, 54.64s/it]
                                                        
{'loss': 0.1027, 'learning_rate': 0.0001406629029166873, 'epoch': 0.43}

 43%|████▎     | 850/1976 [12:56:32<17:05:27, 54.64s/it]
 43%|████▎     | 851/1976 [12:57:26<17:02:17, 54.52s/it]
 43%|████▎     | 852/1976 [12:58:20<16:59:58, 54.45s/it]
 43%|████▎     | 853/1976 [12:59:15<16:59:07, 54.45s/it]
 43%|████▎     | 854/1976 [13:00:09<16:55:30, 54.31s/it]
 43%|████▎     | 855/1976 [13:01:03<16:55:33, 54.36s/it]
                                                        
{'loss': 0.1019, 'learning_rate': 0.00013985420106719246, 'epoch': 0.43}

 43%|████▎     | 855/1976 [13:01:03<16:55:33, 54.36s/it]
 43%|████▎     | 856/1976 [13:01:58<16:53:49, 54.31s/it]
 43%|████▎     | 857/1976 [13:02:52<16:54:53, 54.42s/it]
 43%|████▎     | 858/1976 [13:03:47<16:54:08, 54.43s/it]
 43%|████▎     | 859/1976 [13:04:41<16:53:17, 54.43s/it]
 44%|████▎     | 860/1976 [13:05:36<16:54:08, 54.52s/it]
                                                        
{'loss': 0.1027, 'learning_rate': 0.00013904238859366116, 'epoch': 0.44}

 44%|████▎     | 860/1976 [13:05:36<16:54:08, 54.52s/it]
 44%|████▎     | 861/1976 [13:06:30<16:49:23, 54.32s/it]
 44%|████▎     | 862/1976 [13:07:24<16:48:38, 54.33s/it]
 44%|████▎     | 863/1976 [13:08:18<16:44:51, 54.17s/it]
 44%|████▎     | 864/1976 [13:09:12<16:44:41, 54.21s/it]
 44%|████▍     | 865/1976 [13:10:06<16:43:13, 54.18s/it]
                                                        
{'loss': 0.1023, 'learning_rate': 0.00013822752885813116, 'epoch': 0.44}

 44%|████▍     | 865/1976 [13:10:06<16:43:13, 54.18s/it]
 44%|████▍     | 866/1976 [13:11:01<16:43:53, 54.26s/it]
 44%|████▍     | 867/1976 [13:11:55<16:45:10, 54.38s/it]
 44%|████▍     | 868/1976 [13:12:50<16:43:16, 54.33s/it]
 44%|████▍     | 869/1976 [13:13:44<16:44:47, 54.46s/it]
 44%|████▍     | 870/1976 [13:14:39<16:42:25, 54.38s/it]
                                                        
{'loss': 0.1044, 'learning_rate': 0.00013740968546047935, 'epoch': 0.44}

 44%|████▍     | 870/1976 [13:14:39<16:42:25, 54.38s/it]
 44%|████▍     | 871/1976 [13:15:33<16:41:02, 54.36s/it]
 44%|████▍     | 872/1976 [13:16:27<16:38:06, 54.24s/it]
 44%|████▍     | 873/1976 [13:17:21<16:36:11, 54.19s/it]
 44%|████▍     | 874/1976 [13:18:15<16:35:39, 54.21s/it]
 44%|████▍     | 875/1976 [13:19:09<16:32:38, 54.09s/it]
                                                        
{'loss': 0.1021, 'learning_rate': 0.00013658892223345758, 'epoch': 0.44}

 44%|████▍     | 875/1976 [13:19:09<16:32:38, 54.09s/it]
 44%|████▍     | 876/1976 [13:20:04<16:35:06, 54.28s/it]
 44%|████▍     | 877/1976 [13:20:58<16:33:47, 54.26s/it]
 44%|████▍     | 878/1976 [13:21:53<16:35:21, 54.39s/it]
 44%|████▍     | 879/1976 [13:22:47<16:34:48, 54.41s/it]
 45%|████▍     | 880/1976 [13:23:42<16:34:32, 54.45s/it]
                                                        
{'loss': 0.1022, 'learning_rate': 0.00013576530323771086, 'epoch': 0.45}

 45%|████▍     | 880/1976 [13:23:42<16:34:32, 54.45s/it]
 45%|████▍     | 881/1976 [13:24:36<16:33:15, 54.43s/it]
 45%|████▍     | 882/1976 [13:25:30<16:29:00, 54.24s/it]
 45%|████▍     | 883/1976 [13:26:24<16:27:56, 54.23s/it]
 45%|████▍     | 884/1976 [13:27:18<16:24:52, 54.11s/it]
 45%|████▍     | 885/1976 [13:28:12<16:24:55, 54.17s/it]
                                                        
{'loss': 0.1022, 'learning_rate': 0.000134938892756777, 'epoch': 0.45}

 45%|████▍     | 885/1976 [13:28:12<16:24:55, 54.17s/it]
 45%|████▍     | 886/1976 [13:29:07<16:25:48, 54.27s/it]
 45%|████▍     | 887/1976 [13:30:01<16:26:31, 54.35s/it]
 45%|████▍     | 888/1976 [13:30:56<16:28:18, 54.50s/it]
 45%|████▍     | 889/1976 [13:31:50<16:26:12, 54.44s/it]
 45%|████▌     | 890/1976 [13:32:45<16:26:55, 54.53s/it]
                                                        
{'loss': 0.1026, 'learning_rate': 0.0001341097552920697, 'epoch': 0.45}

 45%|████▌     | 890/1976 [13:32:45<16:26:55, 54.53s/it]
 45%|████▌     | 891/1976 [13:33:39<16:22:53, 54.35s/it]
 45%|████▌     | 892/1976 [13:34:33<16:22:47, 54.40s/it]
 45%|████▌     | 893/1976 [13:35:27<16:19:58, 54.29s/it]
 45%|████▌     | 894/1976 [13:36:22<16:18:15, 54.25s/it]
 45%|████▌     | 895/1976 [13:37:16<16:18:07, 54.29s/it]
                                                        
{'loss': 0.1026, 'learning_rate': 0.00013327795555784374, 'epoch': 0.45}

 45%|████▌     | 895/1976 [13:37:16<16:18:07, 54.29s/it]
 45%|████▌     | 896/1976 [13:38:10<16:17:03, 54.28s/it]
 45%|████▌     | 897/1976 [13:39:05<16:19:04, 54.44s/it]
 45%|████▌     | 898/1976 [13:39:59<16:17:08, 54.39s/it]
 45%|████▌     | 899/1976 [13:40:54<16:18:05, 54.49s/it]
 46%|████▌     | 900/1976 [13:41:49<16:17:11, 54.49s/it]
                                                        
{'loss': 0.1024, 'learning_rate': 0.00013244355847614453, 'epoch': 0.46}

 46%|████▌     | 900/1976 [13:41:49<16:17:11, 54.49s/it][INFO|trainer.py:2889] 2024-02-11 18:19:23,637 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-900
[INFO|tokenization_utils_base.py:2432] 2024-02-11 18:19:23,923 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 18:19:23,923 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-900/special_tokens_map.json
[2024-02-11 18:19:25,026] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step900 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 18:19:29,325] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 18:19:29,325] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 18:19:41,348] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 18:19:41,771] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 18:19:42,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 18:19:42,188] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 18:19:42,365] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step900 is ready now!
[INFO|trainer.py:2979] 2024-02-11 18:19:42,370 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-800] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 46%|████▌     | 901/1976 [13:43:29<20:24:41, 68.36s/it]
 46%|████▌     | 902/1976 [13:44:24<19:09:44, 64.23s/it]
 46%|████▌     | 903/1976 [13:45:18<18:13:59, 61.17s/it]
 46%|████▌     | 904/1976 [13:46:13<17:37:38, 59.20s/it]
 46%|████▌     | 905/1976 [13:47:07<17:10:51, 57.75s/it]
                                                        
{'loss': 0.1032, 'learning_rate': 0.00013160662917174044, 'epoch': 0.46}

 46%|████▌     | 905/1976 [13:47:07<17:10:51, 57.75s/it]
 46%|████▌     | 906/1976 [13:48:02<16:53:44, 56.85s/it]
 46%|████▌     | 907/1976 [13:48:57<16:42:44, 56.28s/it]
 46%|████▌     | 908/1976 [13:49:51<16:31:40, 55.71s/it]
 46%|████▌     | 909/1976 [13:50:46<16:26:51, 55.49s/it]
 46%|████▌     | 910/1976 [13:51:40<16:20:09, 55.17s/it]
                                                        
{'loss': 0.1018, 'learning_rate': 0.00013076723296704036, 'epoch': 0.46}

 46%|████▌     | 910/1976 [13:51:40<16:20:09, 55.17s/it]
 46%|████▌     | 911/1976 [13:52:35<16:16:20, 55.01s/it]
 46%|████▌     | 912/1976 [13:53:29<16:11:55, 54.81s/it]
 46%|████▌     | 913/1976 [13:54:24<16:08:31, 54.67s/it]
 46%|████▋     | 914/1976 [13:55:18<16:07:09, 54.64s/it]
 46%|████▋     | 915/1976 [13:56:12<16:03:10, 54.47s/it]
                                                        
{'loss': 0.1013, 'learning_rate': 0.00012992543537699463, 'epoch': 0.46}

 46%|████▋     | 915/1976 [13:56:12<16:03:10, 54.47s/it]
 46%|████▋     | 916/1976 [13:57:07<16:04:48, 54.61s/it]
 46%|████▋     | 917/1976 [13:58:02<16:02:51, 54.55s/it]
 46%|████▋     | 918/1976 [13:58:57<16:04:25, 54.69s/it]
 47%|████▋     | 919/1976 [13:59:51<16:03:26, 54.69s/it]
 47%|████▋     | 920/1976 [14:00:46<16:02:43, 54.70s/it]
                                                        
{'loss': 0.1015, 'learning_rate': 0.0001290813021039822, 'epoch': 0.47}

 47%|████▋     | 920/1976 [14:00:46<16:02:43, 54.70s/it]
 47%|████▋     | 921/1976 [14:01:41<16:01:17, 54.67s/it]
 47%|████▋     | 922/1976 [14:02:35<15:56:56, 54.48s/it]
 47%|████▋     | 923/1976 [14:03:29<15:56:17, 54.49s/it]
 47%|████▋     | 924/1976 [14:04:23<15:52:39, 54.33s/it]
 47%|████▋     | 925/1976 [14:05:18<15:52:44, 54.39s/it]
                                                        
{'loss': 0.1004, 'learning_rate': 0.00012823489903268211, 'epoch': 0.47}

 47%|████▋     | 925/1976 [14:05:18<15:52:44, 54.39s/it]
 47%|████▋     | 926/1976 [14:06:12<15:52:44, 54.44s/it]
 47%|████▋     | 927/1976 [14:07:07<15:52:54, 54.50s/it]
 47%|████▋     | 928/1976 [14:08:02<15:54:03, 54.62s/it]
 47%|████▋     | 929/1976 [14:08:56<15:51:32, 54.53s/it]
 47%|████▋     | 930/1976 [14:09:51<15:52:28, 54.64s/it]
                                                        
{'loss': 0.0988, 'learning_rate': 0.00012738629222493157, 'epoch': 0.47}

 47%|████▋     | 930/1976 [14:09:51<15:52:28, 54.64s/it]
 47%|████▋     | 931/1976 [14:10:45<15:47:46, 54.42s/it]
 47%|████▋     | 932/1976 [14:11:39<15:47:12, 54.44s/it]
 47%|████▋     | 933/1976 [14:12:34<15:44:47, 54.35s/it]
 47%|████▋     | 934/1976 [14:13:28<15:42:59, 54.30s/it]
 47%|████▋     | 935/1976 [14:14:22<15:43:15, 54.37s/it]
                                                        
{'loss': 0.099, 'learning_rate': 0.0001265355479145694, 'epoch': 0.47}

 47%|████▋     | 935/1976 [14:14:22<15:43:15, 54.37s/it]
 47%|████▋     | 936/1976 [14:15:17<15:42:07, 54.35s/it]
 47%|████▋     | 937/1976 [14:16:12<15:44:11, 54.52s/it]
 47%|████▋     | 938/1976 [14:17:06<15:42:18, 54.47s/it]
 48%|████▊     | 939/1976 [14:18:01<15:43:25, 54.59s/it]
 48%|████▊     | 940/1976 [14:18:55<15:42:40, 54.59s/it]
                                                        
{'loss': 0.1015, 'learning_rate': 0.0001256827325022668, 'epoch': 0.48}

 48%|████▊     | 940/1976 [14:18:55<15:42:40, 54.59s/it]
 48%|████▊     | 941/1976 [14:19:50<15:40:28, 54.52s/it]
 48%|████▊     | 942/1976 [14:20:44<15:40:19, 54.56s/it]
 48%|████▊     | 943/1976 [14:21:38<15:36:46, 54.41s/it]
 48%|████▊     | 944/1976 [14:22:33<15:36:59, 54.48s/it]
 48%|████▊     | 945/1976 [14:23:27<15:33:38, 54.33s/it]
                                                        
{'loss': 0.099, 'learning_rate': 0.0001248279125503447, 'epoch': 0.48}

 48%|████▊     | 945/1976 [14:23:27<15:33:38, 54.33s/it]
 48%|████▊     | 946/1976 [14:24:22<15:36:17, 54.54s/it]
 48%|████▊     | 947/1976 [14:25:17<15:35:59, 54.58s/it]
 48%|████▊     | 948/1976 [14:26:11<15:35:52, 54.62s/it]
 48%|████▊     | 949/1976 [14:27:06<15:36:47, 54.73s/it]
 48%|████▊     | 950/1976 [14:28:01<15:34:10, 54.63s/it]
                                                        
{'loss': 0.1013, 'learning_rate': 0.00012397115477757849, 'epoch': 0.48}

 48%|████▊     | 950/1976 [14:28:01<15:34:10, 54.63s/it]
 48%|████▊     | 951/1976 [14:28:55<15:32:47, 54.60s/it]
 48%|████▊     | 952/1976 [14:29:49<15:28:48, 54.42s/it]
 48%|████▊     | 953/1976 [14:30:44<15:28:31, 54.46s/it]
 48%|████▊     | 954/1976 [14:31:38<15:26:17, 54.38s/it]
 48%|████▊     | 955/1976 [14:32:32<15:24:52, 54.35s/it]
                                                        
{'loss': 0.0999, 'learning_rate': 0.0001231125260539906, 'epoch': 0.48}

 48%|████▊     | 955/1976 [14:32:32<15:24:52, 54.35s/it]
 48%|████▊     | 956/1976 [14:33:27<15:26:55, 54.52s/it]
 48%|████▊     | 957/1976 [14:34:22<15:25:03, 54.47s/it]
 48%|████▊     | 958/1976 [14:35:17<15:26:40, 54.62s/it]
 49%|████▊     | 959/1976 [14:36:11<15:24:23, 54.54s/it]
 49%|████▊     | 960/1976 [14:37:06<15:25:20, 54.65s/it]
                                                        
{'loss': 0.1022, 'learning_rate': 0.00012225209339563145, 'epoch': 0.49}

 49%|████▊     | 960/1976 [14:37:06<15:25:20, 54.65s/it]
 49%|████▊     | 961/1976 [14:38:00<15:22:33, 54.54s/it]
 49%|████▊     | 962/1976 [14:38:54<15:19:51, 54.43s/it]
 49%|████▊     | 963/1976 [14:39:49<15:19:50, 54.48s/it]
 49%|████▉     | 964/1976 [14:40:43<15:16:24, 54.33s/it]
 49%|████▉     | 965/1976 [14:41:37<15:15:41, 54.34s/it]
                                                        
{'loss': 0.1009, 'learning_rate': 0.00012138992395934865, 'epoch': 0.49}

 49%|████▉     | 965/1976 [14:41:37<15:15:41, 54.34s/it]
 49%|████▉     | 966/1976 [14:42:32<15:14:51, 54.35s/it]
 49%|████▉     | 967/1976 [14:43:27<15:16:27, 54.50s/it]
 49%|████▉     | 968/1976 [14:44:21<15:16:21, 54.55s/it]
 49%|████▉     | 969/1976 [14:45:16<15:16:33, 54.61s/it]
 49%|████▉     | 970/1976 [14:46:11<15:17:31, 54.72s/it]
                                                        
{'loss': 0.1018, 'learning_rate': 0.00012052608503754552, 'epoch': 0.49}

 49%|████▉     | 970/1976 [14:46:11<15:17:31, 54.72s/it]
 49%|████▉     | 971/1976 [14:47:05<15:13:08, 54.52s/it]
 49%|████▉     | 972/1976 [14:48:00<15:12:44, 54.55s/it]
 49%|████▉     | 973/1976 [14:48:54<15:09:34, 54.41s/it]
 49%|████▉     | 974/1976 [14:49:48<15:09:38, 54.47s/it]
 49%|████▉     | 975/1976 [14:50:43<15:08:03, 54.43s/it]
                                                        
{'loss': 0.1003, 'learning_rate': 0.00011966064405292887, 'epoch': 0.49}

 49%|████▉     | 975/1976 [14:50:43<15:08:03, 54.43s/it]
 49%|████▉     | 976/1976 [14:51:37<15:08:46, 54.53s/it]
 49%|████▉     | 977/1976 [14:52:32<15:09:36, 54.63s/it]
 49%|████▉     | 978/1976 [14:53:27<15:07:28, 54.56s/it]
 50%|████▉     | 979/1976 [14:54:22<15:08:29, 54.67s/it]
 50%|████▉     | 980/1976 [14:55:16<15:05:53, 54.57s/it]
                                                        
{'loss': 0.1012, 'learning_rate': 0.00011879366855324663, 'epoch': 0.5}

 50%|████▉     | 980/1976 [14:55:16<15:05:53, 54.57s/it]
 50%|████▉     | 981/1976 [14:56:10<15:04:45, 54.56s/it]
 50%|████▉     | 982/1976 [14:57:05<15:02:23, 54.47s/it]
 50%|████▉     | 983/1976 [14:57:59<15:00:35, 54.42s/it]
 50%|████▉     | 984/1976 [14:58:53<14:59:20, 54.40s/it]
 50%|████▉     | 985/1976 [14:59:47<14:56:18, 54.27s/it]
                                                        
{'loss': 0.1004, 'learning_rate': 0.0001179252262060159, 'epoch': 0.5}

 50%|████▉     | 985/1976 [14:59:47<14:56:18, 54.27s/it]
 50%|████▉     | 986/1976 [15:00:42<14:58:02, 54.43s/it]
 50%|████▉     | 987/1976 [15:01:36<14:56:32, 54.39s/it]
 50%|█████     | 988/1976 [15:02:31<14:57:47, 54.52s/it]
 50%|█████     | 989/1976 [15:03:26<14:56:52, 54.52s/it]
 50%|█████     | 990/1976 [15:04:20<14:55:57, 54.52s/it]
                                                        
{'loss': 0.1015, 'learning_rate': 0.00011705538479324114, 'epoch': 0.5}

 50%|█████     | 990/1976 [15:04:20<14:55:57, 54.52s/it]
 50%|█████     | 991/1976 [15:05:15<14:54:09, 54.47s/it]
 50%|█████     | 992/1976 [15:06:09<14:50:28, 54.30s/it]
 50%|█████     | 993/1976 [15:07:03<14:49:59, 54.32s/it]
 50%|█████     | 994/1976 [15:07:57<14:47:15, 54.21s/it]
 50%|█████     | 995/1976 [15:08:51<14:47:05, 54.26s/it]
                                                        
{'loss': 0.1002, 'learning_rate': 0.00011618421220612425, 'epoch': 0.5}

 50%|█████     | 995/1976 [15:08:51<14:47:05, 54.26s/it]
 50%|█████     | 996/1976 [15:09:46<14:47:11, 54.32s/it]
 50%|█████     | 997/1976 [15:10:40<14:47:08, 54.37s/it]
 51%|█████     | 998/1976 [15:11:35<14:47:41, 54.46s/it]
 51%|█████     | 999/1976 [15:12:29<14:45:38, 54.39s/it]
 51%|█████     | 1000/1976 [15:13:24<14:45:53, 54.46s/it]
                                                         
{'loss': 0.0987, 'learning_rate': 0.0001153117764397652, 'epoch': 0.51}

 51%|█████     | 1000/1976 [15:13:24<14:45:53, 54.46s/it][INFO|trainer.py:2889] 2024-02-11 19:50:58,614 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1000
[INFO|tokenization_utils_base.py:2432] 2024-02-11 19:50:58,897 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 19:50:58,897 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1000/special_tokens_map.json
[2024-02-11 19:50:59,993] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 19:51:04,278] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 19:51:04,278] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 19:51:15,859] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 19:51:16,289] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 19:51:16,746] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 19:51:16,746] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 19:51:16,868] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
[INFO|trainer.py:2979] 2024-02-11 19:51:16,873 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-900] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 51%|█████     | 1001/1976 [15:15:04<18:26:17, 68.08s/it]
 51%|█████     | 1002/1976 [15:15:58<17:18:34, 63.98s/it]
 51%|█████     | 1003/1976 [15:16:52<16:30:36, 61.09s/it]
 51%|█████     | 1004/1976 [15:17:46<15:54:24, 58.91s/it]
 51%|█████     | 1005/1976 [15:18:40<15:30:56, 57.52s/it]
                                                         
{'loss': 0.0987, 'learning_rate': 0.00011443814558785539, 'epoch': 0.51}

 51%|█████     | 1005/1976 [15:18:40<15:30:56, 57.52s/it]
 51%|█████     | 1006/1976 [15:19:35<15:13:47, 56.52s/it]
 51%|█████     | 1007/1976 [15:20:29<15:04:07, 55.98s/it]
 51%|█████     | 1008/1976 [15:21:24<14:55:33, 55.51s/it]
 51%|█████     | 1009/1976 [15:22:18<14:49:38, 55.20s/it]
 51%|█████     | 1010/1976 [15:23:13<14:46:19, 55.05s/it]
                                                         
{'loss': 0.0999, 'learning_rate': 0.00011356338783736255, 'epoch': 0.51}

 51%|█████     | 1010/1976 [15:23:13<14:46:19, 55.05s/it]
 51%|█████     | 1011/1976 [15:24:07<14:39:51, 54.71s/it]
 51%|█████     | 1012/1976 [15:25:01<14:37:08, 54.59s/it]
 51%|█████▏    | 1013/1976 [15:25:55<14:32:34, 54.37s/it]
 51%|█████▏    | 1014/1976 [15:26:49<14:31:42, 54.37s/it]
 51%|█████▏    | 1015/1976 [15:27:43<14:29:32, 54.29s/it]
                                                         
{'loss': 0.0993, 'learning_rate': 0.00011268757146320923, 'epoch': 0.51}

 51%|█████▏    | 1015/1976 [15:27:43<14:29:32, 54.29s/it]
 51%|█████▏    | 1016/1976 [15:28:38<14:29:31, 54.35s/it]
 51%|█████▏    | 1017/1976 [15:29:33<14:31:15, 54.51s/it]
 52%|█████▏    | 1018/1976 [15:30:27<14:29:15, 54.44s/it]
 52%|█████▏    | 1019/1976 [15:31:22<14:30:24, 54.57s/it]
 52%|█████▏    | 1020/1976 [15:32:16<14:27:56, 54.47s/it]
                                                         
{'loss': 0.1, 'learning_rate': 0.00011181076482294333, 'epoch': 0.52}

 52%|█████▏    | 1020/1976 [15:32:16<14:27:56, 54.47s/it]
 52%|█████▏    | 1021/1976 [15:33:11<14:27:05, 54.48s/it]
 52%|█████▏    | 1022/1976 [15:34:05<14:25:07, 54.41s/it]
 52%|█████▏    | 1023/1976 [15:34:59<14:23:39, 54.38s/it]
 52%|█████▏    | 1024/1976 [15:35:54<14:23:34, 54.43s/it]
 52%|█████▏    | 1025/1976 [15:36:48<14:20:52, 54.31s/it]
                                                         
{'loss': 0.0989, 'learning_rate': 0.00011093303635140339, 'epoch': 0.52}

 52%|█████▏    | 1025/1976 [15:36:48<14:20:52, 54.31s/it]
 52%|█████▏    | 1026/1976 [15:37:43<14:23:03, 54.51s/it]
 52%|█████▏    | 1027/1976 [15:38:37<14:21:09, 54.45s/it]
 52%|█████▏    | 1028/1976 [15:39:32<14:21:48, 54.55s/it]
 52%|█████▏    | 1029/1976 [15:40:26<14:20:30, 54.52s/it]
 52%|█████▏    | 1030/1976 [15:41:21<14:19:36, 54.52s/it]
                                                         
{'loss': 0.0994, 'learning_rate': 0.00011005445455537678, 'epoch': 0.52}

 52%|█████▏    | 1030/1976 [15:41:21<14:19:36, 54.52s/it]
 52%|█████▏    | 1031/1976 [15:42:15<14:17:30, 54.45s/it]
 52%|█████▏    | 1032/1976 [15:43:09<14:13:48, 54.27s/it]
 52%|█████▏    | 1033/1976 [15:44:03<14:13:14, 54.29s/it]
 52%|█████▏    | 1034/1976 [15:44:57<14:10:08, 54.15s/it]
 52%|█████▏    | 1035/1976 [15:45:52<14:10:14, 54.21s/it]
                                                         
{'loss': 0.0978, 'learning_rate': 0.00010917508800825304, 'epoch': 0.52}

 52%|█████▏    | 1035/1976 [15:45:52<14:10:14, 54.21s/it]
 52%|█████▏    | 1036/1976 [15:46:46<14:10:21, 54.28s/it]
 52%|█████▏    | 1037/1976 [15:47:40<14:10:00, 54.31s/it]
 53%|█████▎    | 1038/1976 [15:48:35<14:10:36, 54.41s/it]
 53%|█████▎    | 1039/1976 [15:49:29<14:08:40, 54.34s/it]
 53%|█████▎    | 1040/1976 [15:50:24<14:09:05, 54.43s/it]
                                                         
{'loss': 0.1002, 'learning_rate': 0.00010829500534467154, 'epoch': 0.53}

 53%|█████▎    | 1040/1976 [15:50:24<14:09:05, 54.43s/it]
 53%|█████▎    | 1041/1976 [15:51:18<14:05:24, 54.25s/it]
 53%|█████▎    | 1042/1976 [15:52:12<14:05:00, 54.28s/it]
 53%|█████▎    | 1043/1976 [15:53:06<14:02:43, 54.19s/it]
 53%|█████▎    | 1044/1976 [15:54:00<14:01:51, 54.20s/it]
 53%|█████▎    | 1045/1976 [15:54:55<14:01:55, 54.26s/it]
                                                         
{'loss': 0.1, 'learning_rate': 0.00010741427525516463, 'epoch': 0.53}

 53%|█████▎    | 1045/1976 [15:54:55<14:01:55, 54.26s/it]
 53%|█████▎    | 1046/1976 [15:55:49<14:00:47, 54.24s/it]
 53%|█████▎    | 1047/1976 [15:56:44<14:02:22, 54.41s/it]
 53%|█████▎    | 1048/1976 [15:57:38<14:00:54, 54.37s/it]
 53%|█████▎    | 1049/1976 [15:58:33<14:01:58, 54.50s/it]
 53%|█████▎    | 1050/1976 [15:59:27<14:01:03, 54.50s/it]
                                                         
{'loss': 0.1, 'learning_rate': 0.00010653296648079629, 'epoch': 0.53}

 53%|█████▎    | 1050/1976 [15:59:27<14:01:03, 54.50s/it]
 53%|█████▎    | 1051/1976 [16:00:21<13:58:44, 54.40s/it]
 53%|█████▎    | 1052/1976 [16:01:16<13:57:51, 54.41s/it]
 53%|█████▎    | 1053/1976 [16:02:10<13:54:36, 54.25s/it]
 53%|█████▎    | 1054/1976 [16:03:04<13:54:26, 54.30s/it]
 53%|█████▎    | 1055/1976 [16:03:58<13:51:39, 54.18s/it]
                                                         
{'loss': 0.0981, 'learning_rate': 0.00010565114780779692, 'epoch': 0.53}

 53%|█████▎    | 1055/1976 [16:03:58<13:51:39, 54.18s/it]
 53%|█████▎    | 1056/1976 [16:04:53<13:53:26, 54.35s/it]
 53%|█████▎    | 1057/1976 [16:05:47<13:53:31, 54.42s/it]
 54%|█████▎    | 1058/1976 [16:06:42<13:53:30, 54.48s/it]
 54%|█████▎    | 1059/1976 [16:07:37<13:54:11, 54.58s/it]
 54%|█████▎    | 1060/1976 [16:08:31<13:51:49, 54.49s/it]
                                                         
{'loss': 0.0947, 'learning_rate': 0.00010476888806219458, 'epoch': 0.54}

 54%|█████▎    | 1060/1976 [16:08:31<13:51:49, 54.49s/it]
 54%|█████▎    | 1061/1976 [16:09:25<13:50:52, 54.48s/it]
 54%|█████▎    | 1062/1976 [16:10:19<13:47:26, 54.32s/it]
 54%|█████▍    | 1063/1976 [16:11:14<13:47:16, 54.37s/it]
 54%|█████▍    | 1064/1976 [16:12:08<13:45:53, 54.33s/it]
 54%|█████▍    | 1065/1976 [16:13:02<13:44:11, 54.28s/it]
                                                         
{'loss': 0.0967, 'learning_rate': 0.00010388625610444305, 'epoch': 0.54}

 54%|█████▍    | 1065/1976 [16:13:02<13:44:11, 54.28s/it]
 54%|█████▍    | 1066/1976 [16:13:57<13:44:50, 54.39s/it]
 54%|█████▍    | 1067/1976 [16:14:51<13:43:17, 54.34s/it]
 54%|█████▍    | 1068/1976 [16:15:46<13:44:34, 54.49s/it]
 54%|█████▍    | 1069/1976 [16:16:40<13:42:38, 54.42s/it]
 54%|█████▍    | 1070/1976 [16:17:35<13:43:11, 54.52s/it]
                                                         
{'loss': 0.097, 'learning_rate': 0.00010300332082404732, 'epoch': 0.54}

 54%|█████▍    | 1070/1976 [16:17:35<13:43:11, 54.52s/it]
 54%|█████▍    | 1071/1976 [16:18:29<13:40:08, 54.37s/it]
 54%|█████▍    | 1072/1976 [16:19:23<13:38:31, 54.33s/it]
 54%|█████▍    | 1073/1976 [16:20:18<13:37:31, 54.32s/it]
 54%|█████▍    | 1074/1976 [16:21:12<13:35:00, 54.21s/it]
 54%|█████▍    | 1075/1976 [16:22:06<13:35:41, 54.32s/it]
                                                         
{'loss': 0.0979, 'learning_rate': 0.00010212015113418674, 'epoch': 0.54}

 54%|█████▍    | 1075/1976 [16:22:06<13:35:41, 54.32s/it]
 54%|█████▍    | 1076/1976 [16:23:00<13:34:42, 54.31s/it]
 55%|█████▍    | 1077/1976 [16:23:55<13:36:00, 54.46s/it]
 55%|█████▍    | 1078/1976 [16:24:50<13:35:05, 54.46s/it]
 55%|█████▍    | 1079/1976 [16:25:44<13:34:47, 54.50s/it]
 55%|█████▍    | 1080/1976 [16:26:39<13:35:10, 54.59s/it]
                                                         
{'loss': 0.0951, 'learning_rate': 0.00010123681596633629, 'epoch': 0.55}

 55%|█████▍    | 1080/1976 [16:26:39<13:35:10, 54.59s/it]
 55%|█████▍    | 1081/1976 [16:27:33<13:31:16, 54.39s/it]
 55%|█████▍    | 1082/1976 [16:28:27<13:30:54, 54.42s/it]
 55%|█████▍    | 1083/1976 [16:29:21<13:27:23, 54.25s/it]
 55%|█████▍    | 1084/1976 [16:30:16<13:26:42, 54.26s/it]
 55%|█████▍    | 1085/1976 [16:31:10<13:25:01, 54.21s/it]
                                                         
{'loss': 0.098, 'learning_rate': 0.00010035338426488665, 'epoch': 0.55}

 55%|█████▍    | 1085/1976 [16:31:10<13:25:01, 54.21s/it]
 55%|█████▍    | 1086/1976 [16:32:04<13:25:35, 54.31s/it]
 55%|█████▌    | 1087/1976 [16:32:59<13:26:10, 54.41s/it]
 55%|█████▌    | 1088/1976 [16:33:53<13:24:20, 54.35s/it]
 55%|█████▌    | 1089/1976 [16:34:48<13:25:16, 54.47s/it]
 55%|█████▌    | 1090/1976 [16:35:42<13:23:07, 54.39s/it]
                                                         
{'loss': 0.0971, 'learning_rate': 9.946992498176282e-05, 'epoch': 0.55}

 55%|█████▌    | 1090/1976 [16:35:42<13:23:07, 54.39s/it]
 55%|█████▌    | 1091/1976 [16:36:36<13:22:02, 54.38s/it]
 55%|█████▌    | 1092/1976 [16:37:30<13:19:45, 54.28s/it]
 55%|█████▌    | 1093/1976 [16:38:25<13:18:01, 54.23s/it]
 55%|█████▌    | 1094/1976 [16:39:19<13:17:14, 54.23s/it]
 55%|█████▌    | 1095/1976 [16:40:13<13:14:30, 54.11s/it]
                                                         
{'loss': 0.0956, 'learning_rate': 9.85865070710426e-05, 'epoch': 0.55}

 55%|█████▌    | 1095/1976 [16:40:13<13:14:30, 54.11s/it]
 55%|█████▌    | 1096/1976 [16:41:07<13:16:19, 54.30s/it]
 56%|█████▌    | 1097/1976 [16:42:02<13:15:04, 54.27s/it]
 56%|█████▌    | 1098/1976 [16:42:56<13:16:30, 54.43s/it]
 56%|█████▌    | 1099/1976 [16:43:51<13:15:51, 54.45s/it]
 56%|█████▌    | 1100/1976 [16:44:46<13:15:49, 54.51s/it]
                                                         
{'loss': 0.0958, 'learning_rate': 9.770319948357473e-05, 'epoch': 0.56}

 56%|█████▌    | 1100/1976 [16:44:46<13:15:49, 54.51s/it][INFO|trainer.py:2889] 2024-02-11 21:22:20,582 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1100
[INFO|tokenization_utils_base.py:2432] 2024-02-11 21:22:20,868 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 21:22:20,869 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1100/special_tokens_map.json
[2024-02-11 21:22:20,965] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1100 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 21:22:24,852] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1100/global_step1100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 21:22:24,852] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1100/global_step1100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 21:22:36,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1100/global_step1100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 21:22:36,891] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1100/global_step1100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 21:22:37,456] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1100/global_step1100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 21:22:37,457] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1100/global_step1100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 21:22:37,543] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1100 is ready now!
[INFO|trainer.py:2979] 2024-02-11 21:22:37,549 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-1000] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 56%|█████▌    | 1101/1976 [16:46:26<16:37:49, 68.42s/it]
 56%|█████▌    | 1102/1976 [16:47:20<15:33:29, 64.08s/it]
 56%|█████▌    | 1103/1976 [16:48:15<14:50:43, 61.22s/it]
 56%|█████▌    | 1104/1976 [16:49:09<14:18:21, 59.06s/it]
 56%|█████▌    | 1105/1976 [16:50:03<13:57:41, 57.71s/it]
                                                         
{'loss': 0.0997, 'learning_rate': 9.682007116159707e-05, 'epoch': 0.56}

 56%|█████▌    | 1105/1976 [16:50:03<13:57:41, 57.71s/it]
 56%|█████▌    | 1106/1976 [16:50:58<13:44:27, 56.86s/it]
 56%|█████▌    | 1107/1976 [16:51:53<13:32:19, 56.09s/it]
 56%|█████▌    | 1108/1976 [16:52:47<13:26:02, 55.72s/it]
 56%|█████▌    | 1109/1976 [16:53:42<13:19:02, 55.30s/it]
 56%|█████▌    | 1110/1976 [16:54:37<13:15:51, 55.14s/it]
                                                         
{'loss': 0.097, 'learning_rate': 9.593719103335599e-05, 'epoch': 0.56}

 56%|█████▌    | 1110/1976 [16:54:37<13:15:51, 55.14s/it]
 56%|█████▌    | 1111/1976 [16:55:31<13:10:48, 54.85s/it]
 56%|█████▋    | 1112/1976 [16:56:25<13:06:48, 54.64s/it]
 56%|█████▋    | 1113/1976 [16:57:19<13:04:44, 54.56s/it]
 56%|█████▋    | 1114/1976 [16:58:13<13:00:43, 54.34s/it]
 56%|█████▋    | 1115/1976 [16:59:07<12:59:43, 54.34s/it]
                                                         
{'loss': 0.0968, 'learning_rate': 9.505462800772612e-05, 'epoch': 0.56}

 56%|█████▋    | 1115/1976 [16:59:07<12:59:43, 54.34s/it]
 56%|█████▋    | 1116/1976 [17:00:02<12:58:15, 54.30s/it]
 57%|█████▋    | 1117/1976 [17:00:56<12:59:00, 54.41s/it]
 57%|█████▋    | 1118/1976 [17:01:51<12:58:22, 54.43s/it]
 57%|█████▋    | 1119/1976 [17:02:45<12:57:32, 54.44s/it]
 57%|█████▋    | 1120/1976 [17:03:40<12:57:19, 54.49s/it]
                                                         
{'loss': 0.0973, 'learning_rate': 9.417245096883231e-05, 'epoch': 0.57}

 57%|█████▋    | 1120/1976 [17:03:40<12:57:19, 54.49s/it]
 57%|█████▋    | 1121/1976 [17:04:34<12:53:30, 54.28s/it]
 57%|█████▋    | 1122/1976 [17:05:28<12:52:36, 54.28s/it]
 57%|█████▋    | 1123/1976 [17:06:22<12:49:52, 54.15s/it]
 57%|█████▋    | 1124/1976 [17:07:16<12:49:54, 54.22s/it]
 57%|█████▋    | 1125/1976 [17:08:10<12:48:19, 54.17s/it]
                                                         
{'loss': 0.0967, 'learning_rate': 9.329072877067309e-05, 'epoch': 0.57}

 57%|█████▋    | 1125/1976 [17:08:10<12:48:19, 54.17s/it]
 57%|█████▋    | 1126/1976 [17:09:05<12:48:42, 54.26s/it]
 57%|█████▋    | 1127/1976 [17:09:59<12:49:49, 54.40s/it]
 57%|█████▋    | 1128/1976 [17:10:54<12:48:13, 54.36s/it]
 57%|█████▋    | 1129/1976 [17:11:48<12:49:04, 54.48s/it]
 57%|█████▋    | 1130/1976 [17:12:43<12:47:10, 54.41s/it]
                                                         
{'loss': 0.0972, 'learning_rate': 9.240953023174663e-05, 'epoch': 0.57}

 57%|█████▋    | 1130/1976 [17:12:43<12:47:10, 54.41s/it]
 57%|█████▋    | 1131/1976 [17:13:37<12:45:51, 54.38s/it]
 57%|█████▋    | 1132/1976 [17:14:31<12:43:46, 54.30s/it]
 57%|█████▋    | 1133/1976 [17:15:25<12:42:16, 54.25s/it]
 57%|█████▋    | 1134/1976 [17:16:20<12:41:32, 54.27s/it]
 57%|█████▋    | 1135/1976 [17:17:13<12:38:49, 54.14s/it]
                                                         
{'loss': 0.097, 'learning_rate': 9.152892412967949e-05, 'epoch': 0.57}

 57%|█████▋    | 1135/1976 [17:17:13<12:38:49, 54.14s/it]
 57%|█████▋    | 1136/1976 [17:18:08<12:40:34, 54.33s/it]
 58%|█████▊    | 1137/1976 [17:19:02<12:39:18, 54.30s/it]
 58%|█████▊    | 1138/1976 [17:19:57<12:39:51, 54.40s/it]
 58%|█████▊    | 1139/1976 [17:20:51<12:39:07, 54.42s/it]
 58%|█████▊    | 1140/1976 [17:21:46<12:38:21, 54.43s/it]
                                                         
{'loss': 0.0956, 'learning_rate': 9.064897919585833e-05, 'epoch': 0.58}

 58%|█████▊    | 1140/1976 [17:21:46<12:38:21, 54.43s/it]
 58%|█████▊    | 1141/1976 [17:22:40<12:37:12, 54.41s/it]
 58%|█████▊    | 1142/1976 [17:23:34<12:33:47, 54.23s/it]
 58%|█████▊    | 1143/1976 [17:24:28<12:33:26, 54.27s/it]
 58%|█████▊    | 1144/1976 [17:25:22<12:30:46, 54.14s/it]
 58%|█████▊    | 1145/1976 [17:26:17<12:30:31, 54.19s/it]
                                                         
{'loss': 0.0962, 'learning_rate': 8.976976411006575e-05, 'epoch': 0.58}

 58%|█████▊    | 1145/1976 [17:26:17<12:30:31, 54.19s/it]
 58%|█████▊    | 1146/1976 [17:27:11<12:30:49, 54.28s/it]
 58%|█████▊    | 1147/1976 [17:28:06<12:30:56, 54.35s/it]
 58%|█████▊    | 1148/1976 [17:29:00<12:32:00, 54.49s/it]
 58%|█████▊    | 1149/1976 [17:29:55<12:30:20, 54.44s/it]
 58%|█████▊    | 1150/1976 [17:30:50<12:31:22, 54.58s/it]
                                                         
{'loss': 0.0963, 'learning_rate': 8.889134749511955e-05, 'epoch': 0.58}

 58%|█████▊    | 1150/1976 [17:30:50<12:31:22, 54.58s/it]
 58%|█████▊    | 1151/1976 [17:31:44<12:27:54, 54.39s/it]
 58%|█████▊    | 1152/1976 [17:32:38<12:27:31, 54.43s/it]
 58%|█████▊    | 1153/1976 [17:33:32<12:25:44, 54.37s/it]
 58%|█████▊    | 1154/1976 [17:34:27<12:24:28, 54.34s/it]
 58%|█████▊    | 1155/1976 [17:35:21<12:24:24, 54.40s/it]
                                                         
{'loss': 0.0959, 'learning_rate': 8.801379791151685e-05, 'epoch': 0.58}

 58%|█████▊    | 1155/1976 [17:35:21<12:24:24, 54.40s/it]
 59%|█████▊    | 1156/1976 [17:36:15<12:23:00, 54.37s/it]
 59%|█████▊    | 1157/1976 [17:37:10<12:23:48, 54.49s/it]
 59%|█████▊    | 1158/1976 [17:38:05<12:21:54, 54.42s/it]
 59%|█████▊    | 1159/1976 [17:38:59<12:22:01, 54.49s/it]
 59%|█████▊    | 1160/1976 [17:39:54<12:21:01, 54.49s/it]
                                                         
{'loss': 0.0973, 'learning_rate': 8.71371838520828e-05, 'epoch': 0.59}

 59%|█████▊    | 1160/1976 [17:39:54<12:21:01, 54.49s/it]
 59%|█████▉    | 1161/1976 [17:40:48<12:18:55, 54.40s/it]
 59%|█████▉    | 1162/1976 [17:41:42<12:17:29, 54.36s/it]
 59%|█████▉    | 1163/1976 [17:42:36<12:14:26, 54.20s/it]
 59%|█████▉    | 1164/1976 [17:43:30<12:14:20, 54.26s/it]
 59%|█████▉    | 1165/1976 [17:44:24<12:11:49, 54.14s/it]
                                                         
{'loss': 0.0953, 'learning_rate': 8.626157373662493e-05, 'epoch': 0.59}

 59%|█████▉    | 1165/1976 [17:44:24<12:11:49, 54.14s/it]
 59%|█████▉    | 1166/1976 [17:45:19<12:12:53, 54.29s/it]
 59%|█████▉    | 1167/1976 [17:46:13<12:12:35, 54.33s/it]
 59%|█████▉    | 1168/1976 [17:47:08<12:12:04, 54.36s/it]
 59%|█████▉    | 1169/1976 [17:48:02<12:12:03, 54.43s/it]
 59%|█████▉    | 1170/1976 [17:48:57<12:10:23, 54.37s/it]
                                                         
{'loss': 0.0963, 'learning_rate': 8.538703590659276e-05, 'epoch': 0.59}

 59%|█████▉    | 1170/1976 [17:48:57<12:10:23, 54.37s/it]
 59%|█████▉    | 1171/1976 [17:49:51<12:09:05, 54.34s/it]
 59%|█████▉    | 1172/1976 [17:50:45<12:06:21, 54.21s/it]
 59%|█████▉    | 1173/1976 [17:51:39<12:06:17, 54.27s/it]
 59%|█████▉    | 1174/1976 [17:52:33<12:04:29, 54.20s/it]
 59%|█████▉    | 1175/1976 [17:53:27<12:03:15, 54.18s/it]
                                                         
{'loss': 0.0935, 'learning_rate': 8.451363861974396e-05, 'epoch': 0.59}

 59%|█████▉    | 1175/1976 [17:53:27<12:03:15, 54.18s/it]
 60%|█████▉    | 1176/1976 [17:54:22<12:05:19, 54.40s/it]
 60%|█████▉    | 1177/1976 [17:55:17<12:04:12, 54.38s/it]
 60%|█████▉    | 1178/1976 [17:56:11<12:05:26, 54.54s/it]
 60%|█████▉    | 1179/1976 [17:57:06<12:03:38, 54.48s/it]
 60%|█████▉    | 1180/1976 [17:58:01<12:04:31, 54.61s/it]
                                                         
{'loss': 0.0944, 'learning_rate': 8.364145004481644e-05, 'epoch': 0.6}

 60%|█████▉    | 1180/1976 [17:58:01<12:04:31, 54.61s/it]
 60%|█████▉    | 1181/1976 [17:58:55<12:01:58, 54.49s/it]
 60%|█████▉    | 1182/1976 [17:59:49<12:00:10, 54.42s/it]
 60%|█████▉    | 1183/1976 [18:00:44<11:59:44, 54.46s/it]
 60%|█████▉    | 1184/1976 [18:01:38<11:56:42, 54.30s/it]
 60%|█████▉    | 1185/1976 [18:02:32<11:56:11, 54.33s/it]
                                                         
{'loss': 0.0942, 'learning_rate': 8.277053825620836e-05, 'epoch': 0.6}

 60%|█████▉    | 1185/1976 [18:02:32<11:56:11, 54.33s/it]
 60%|██████    | 1186/1976 [18:03:26<11:55:01, 54.31s/it]
 60%|██████    | 1187/1976 [18:04:21<11:55:46, 54.43s/it]
 60%|██████    | 1188/1976 [18:05:15<11:54:39, 54.42s/it]
 60%|██████    | 1189/1976 [18:06:10<11:54:16, 54.45s/it]
 60%|██████    | 1190/1976 [18:07:05<11:54:18, 54.53s/it]
                                                         
{'loss': 0.0953, 'learning_rate': 8.190097122866432e-05, 'epoch': 0.6}

 60%|██████    | 1190/1976 [18:07:05<11:54:18, 54.53s/it]
 60%|██████    | 1191/1976 [18:07:59<11:50:50, 54.33s/it]
 60%|██████    | 1192/1976 [18:08:53<11:50:18, 54.36s/it]
 60%|██████    | 1193/1976 [18:09:47<11:47:39, 54.23s/it]
 60%|██████    | 1194/1976 [18:10:41<11:46:53, 54.24s/it]
 60%|██████    | 1195/1976 [18:11:35<11:45:30, 54.20s/it]
                                                         
{'loss': 0.0941, 'learning_rate': 8.10328168319704e-05, 'epoch': 0.6}

 60%|██████    | 1195/1976 [18:11:35<11:45:30, 54.20s/it]
 61%|██████    | 1196/1976 [18:12:30<11:45:51, 54.30s/it]
 61%|██████    | 1197/1976 [18:13:25<11:46:45, 54.44s/it]
 61%|██████    | 1198/1976 [18:14:19<11:45:07, 54.38s/it]
 61%|██████    | 1199/1976 [18:15:14<11:46:27, 54.55s/it]
 61%|██████    | 1200/1976 [18:16:08<11:44:42, 54.49s/it]
                                                         
{'loss': 0.0935, 'learning_rate': 8.016614282565655e-05, 'epoch': 0.61}

 61%|██████    | 1200/1976 [18:16:08<11:44:42, 54.49s/it][INFO|trainer.py:2889] 2024-02-11 22:53:43,249 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1200
[INFO|tokenization_utils_base.py:2432] 2024-02-11 22:53:43,535 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-11 22:53:43,536 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1200/special_tokens_map.json
[2024-02-11 22:53:44,672] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1200 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-11 22:53:49,092] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-11 22:53:49,093] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-11 22:54:00,763] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-11 22:54:01,219] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-11 22:54:01,824] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-11 22:54:01,825] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-11 22:54:01,891] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!
[INFO|trainer.py:2979] 2024-02-11 22:54:01,897 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-1100] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 61%|██████    | 1201/1976 [18:17:49<14:44:52, 68.51s/it]
 61%|██████    | 1202/1976 [18:18:44<13:49:38, 64.31s/it]
 61%|██████    | 1203/1976 [18:19:38<13:08:32, 61.21s/it]
 61%|██████    | 1204/1976 [18:20:32<12:41:37, 59.19s/it]
 61%|██████    | 1205/1976 [18:21:26<12:20:19, 57.61s/it]
                                                         
{'loss': 0.095, 'learning_rate': 7.930101685370823e-05, 'epoch': 0.61}

 61%|██████    | 1205/1976 [18:21:26<12:20:19, 57.61s/it]
 61%|██████    | 1206/1976 [18:22:21<12:08:09, 56.74s/it]
 61%|██████    | 1207/1976 [18:23:15<11:58:24, 56.05s/it]
 61%|██████    | 1208/1976 [18:24:10<11:51:25, 55.58s/it]
 61%|██████    | 1209/1976 [18:25:04<11:46:54, 55.30s/it]
 61%|██████    | 1210/1976 [18:25:59<11:41:53, 54.98s/it]
                                                         
{'loss': 0.0968, 'learning_rate': 7.843750643928659e-05, 'epoch': 0.61}

 61%|██████    | 1210/1976 [18:25:59<11:41:53, 54.98s/it]
 61%|██████▏   | 1211/1976 [18:26:53<11:38:31, 54.79s/it]
 61%|██████▏   | 1212/1976 [18:27:47<11:34:06, 54.51s/it]
 61%|██████▏   | 1213/1976 [18:28:41<11:32:28, 54.45s/it]
 61%|██████▏   | 1214/1976 [18:29:35<11:30:05, 54.34s/it]
 61%|██████▏   | 1215/1976 [18:30:29<11:28:06, 54.25s/it]
                                                         
{'loss': 0.0954, 'learning_rate': 7.757567897945849e-05, 'epoch': 0.61}

 61%|██████▏   | 1215/1976 [18:30:29<11:28:06, 54.25s/it]
 62%|██████▏   | 1216/1976 [18:31:24<11:28:32, 54.36s/it]
 62%|██████▏   | 1217/1976 [18:32:18<11:27:11, 54.32s/it]
 62%|██████▏   | 1218/1976 [18:33:13<11:28:05, 54.47s/it]
 62%|██████▏   | 1219/1976 [18:34:07<11:26:06, 54.38s/it]
 62%|██████▏   | 1220/1976 [18:35:02<11:26:03, 54.45s/it]
                                                         
{'loss': 0.0945, 'learning_rate': 7.671560173993587e-05, 'epoch': 0.62}

 62%|██████▏   | 1220/1976 [18:35:02<11:26:03, 54.45s/it]
 62%|██████▏   | 1221/1976 [18:35:56<11:24:05, 54.37s/it]
 62%|██████▏   | 1222/1976 [18:36:50<11:22:17, 54.29s/it]
 62%|██████▏   | 1223/1976 [18:37:44<11:21:48, 54.33s/it]
 62%|██████▏   | 1224/1976 [18:38:38<11:19:08, 54.19s/it]
 62%|██████▏   | 1225/1976 [18:39:33<11:18:42, 54.22s/it]
                                                         
{'loss': 0.0942, 'learning_rate': 7.585734184982593e-05, 'epoch': 0.62}

 62%|██████▏   | 1225/1976 [18:39:33<11:18:42, 54.22s/it]
 62%|██████▏   | 1226/1976 [18:40:27<11:17:55, 54.23s/it]
 62%|██████▏   | 1227/1976 [18:41:22<11:19:31, 54.43s/it]
 62%|██████▏   | 1228/1976 [18:42:16<11:18:47, 54.45s/it]
 62%|██████▏   | 1229/1976 [18:43:11<11:18:29, 54.50s/it]
 62%|██████▏   | 1230/1976 [18:44:06<11:19:00, 54.61s/it]
                                                         
{'loss': 0.0924, 'learning_rate': 7.500096629639156e-05, 'epoch': 0.62}

 62%|██████▏   | 1230/1976 [18:44:06<11:19:00, 54.61s/it]
 62%|██████▏   | 1231/1976 [18:45:00<11:15:46, 54.42s/it]
 62%|██████▏   | 1232/1976 [18:45:54<11:15:13, 54.45s/it]
 62%|██████▏   | 1233/1976 [18:46:48<11:12:14, 54.29s/it]
 62%|██████▏   | 1234/1976 [18:47:42<11:11:25, 54.29s/it]
 62%|██████▎   | 1235/1976 [18:48:37<11:09:36, 54.22s/it]
                                                         
{'loss': 0.0949, 'learning_rate': 7.414654191982297e-05, 'epoch': 0.62}

 62%|██████▎   | 1235/1976 [18:48:37<11:09:36, 54.22s/it]
 63%|██████▎   | 1236/1976 [18:49:31<11:09:55, 54.32s/it]
 63%|██████▎   | 1237/1976 [18:50:26<11:10:39, 54.45s/it]
 63%|██████▎   | 1238/1976 [18:51:20<11:09:06, 54.40s/it]
 63%|██████▎   | 1239/1976 [18:52:15<11:09:49, 54.53s/it]
 63%|██████▎   | 1240/1976 [18:53:10<11:09:18, 54.56s/it]
                                                         
{'loss': 0.0937, 'learning_rate': 7.329413540802088e-05, 'epoch': 0.63}

 63%|██████▎   | 1240/1976 [18:53:10<11:09:18, 54.56s/it]
 63%|██████▎   | 1241/1976 [18:54:04<11:06:49, 54.43s/it]
 63%|██████▎   | 1242/1976 [18:54:58<11:04:52, 54.35s/it]
 63%|██████▎   | 1243/1976 [18:55:52<11:03:16, 54.29s/it]
 63%|██████▎   | 1244/1976 [18:56:46<11:02:51, 54.33s/it]
 63%|██████▎   | 1245/1976 [18:57:40<11:00:17, 54.20s/it]
                                                         
{'loss': 0.0943, 'learning_rate': 7.24438132913914e-05, 'epoch': 0.63}

 63%|██████▎   | 1245/1976 [18:57:40<11:00:17, 54.20s/it]
 63%|██████▎   | 1246/1976 [18:58:35<11:01:21, 54.36s/it]
 63%|██████▎   | 1247/1976 [18:59:30<11:01:00, 54.40s/it]
 63%|██████▎   | 1248/1976 [19:00:24<11:00:37, 54.45s/it]
 63%|██████▎   | 1249/1976 [19:01:19<10:59:50, 54.46s/it]
 63%|██████▎   | 1250/1976 [19:02:13<10:59:02, 54.47s/it]
                                                         
{'loss': 0.0946, 'learning_rate': 7.159564193765351e-05, 'epoch': 0.63}

 63%|██████▎   | 1250/1976 [19:02:13<10:59:02, 54.47s/it]
 63%|██████▎   | 1251/1976 [19:03:08<10:58:10, 54.47s/it]
 63%|██████▎   | 1252/1976 [19:04:02<10:55:37, 54.33s/it]
 63%|██████▎   | 1253/1976 [19:04:56<10:55:11, 54.37s/it]
 63%|██████▎   | 1254/1976 [19:05:50<10:53:45, 54.33s/it]
 64%|██████▎   | 1255/1976 [19:06:45<10:52:42, 54.32s/it]
                                                         
{'loss': 0.0929, 'learning_rate': 7.07496875466589e-05, 'epoch': 0.63}

 64%|██████▎   | 1255/1976 [19:06:45<10:52:42, 54.32s/it]
 64%|██████▎   | 1256/1976 [19:07:39<10:52:57, 54.41s/it]
 64%|██████▎   | 1257/1976 [19:08:34<10:52:56, 54.49s/it]
 64%|██████▎   | 1258/1976 [19:09:29<10:53:29, 54.61s/it]
 64%|██████▎   | 1259/1976 [19:10:23<10:51:34, 54.52s/it]
 64%|██████▍   | 1260/1976 [19:11:18<10:51:53, 54.63s/it]
                                                         
{'loss': 0.094, 'learning_rate': 6.990601614522516e-05, 'epoch': 0.64}

 64%|██████▍   | 1260/1976 [19:11:18<10:51:53, 54.63s/it]
 64%|██████▍   | 1261/1976 [19:12:12<10:49:25, 54.50s/it]
 64%|██████▍   | 1262/1976 [19:13:06<10:47:19, 54.40s/it]
 64%|██████▍   | 1263/1976 [19:14:00<10:45:35, 54.33s/it]
 64%|██████▍   | 1264/1976 [19:14:55<10:44:16, 54.29s/it]
 64%|██████▍   | 1265/1976 [19:15:49<10:43:43, 54.32s/it]
                                                         
{'loss': 0.0948, 'learning_rate': 6.906469358198219e-05, 'epoch': 0.64}

 64%|██████▍   | 1265/1976 [19:15:49<10:43:43, 54.32s/it]
 64%|██████▍   | 1266/1976 [19:16:43<10:42:45, 54.32s/it]
 64%|██████▍   | 1267/1976 [19:17:38<10:43:43, 54.48s/it]
 64%|██████▍   | 1268/1976 [19:18:33<10:43:01, 54.49s/it]
 64%|██████▍   | 1269/1976 [19:19:27<10:41:55, 54.48s/it]
 64%|██████▍   | 1270/1976 [19:20:22<10:40:54, 54.47s/it]
                                                         
{'loss': 0.0919, 'learning_rate': 6.822578552223303e-05, 'epoch': 0.64}

 64%|██████▍   | 1270/1976 [19:20:22<10:40:54, 54.47s/it]
 64%|██████▍   | 1271/1976 [19:21:16<10:38:51, 54.37s/it]
 64%|██████▍   | 1272/1976 [19:22:10<10:38:03, 54.38s/it]
 64%|██████▍   | 1273/1976 [19:23:04<10:35:19, 54.22s/it]
 64%|██████▍   | 1274/1976 [19:23:58<10:34:53, 54.26s/it]
 65%|██████▍   | 1275/1976 [19:24:53<10:33:27, 54.22s/it]
                                                         
{'loss': 0.0931, 'learning_rate': 6.738935744282839e-05, 'epoch': 0.65}

 65%|██████▍   | 1275/1976 [19:24:53<10:33:27, 54.22s/it]
 65%|██████▍   | 1276/1976 [19:25:47<10:33:57, 54.34s/it]
 65%|██████▍   | 1277/1976 [19:26:42<10:33:40, 54.39s/it]
 65%|██████▍   | 1278/1976 [19:27:36<10:33:36, 54.47s/it]
 65%|██████▍   | 1279/1976 [19:28:31<10:34:13, 54.60s/it]
 65%|██████▍   | 1280/1976 [19:29:26<10:32:23, 54.52s/it]
                                                         
{'loss': 0.0946, 'learning_rate': 6.65554746270564e-05, 'epoch': 0.65}

 65%|██████▍   | 1280/1976 [19:29:26<10:32:23, 54.52s/it]
 65%|██████▍   | 1281/1976 [19:30:20<10:31:39, 54.53s/it]
 65%|██████▍   | 1282/1976 [19:31:14<10:29:40, 54.44s/it]
 65%|██████▍   | 1283/1976 [19:32:09<10:28:20, 54.40s/it]
 65%|██████▍   | 1284/1976 [19:33:03<10:26:36, 54.33s/it]
 65%|██████▌   | 1285/1976 [19:33:57<10:25:43, 54.33s/it]
                                                         
{'loss': 0.0928, 'learning_rate': 6.572420215954699e-05, 'epoch': 0.65}

 65%|██████▌   | 1285/1976 [19:33:57<10:25:43, 54.33s/it]
 65%|██████▌   | 1286/1976 [19:34:52<10:26:32, 54.48s/it]
 65%|██████▌   | 1287/1976 [19:35:46<10:24:55, 54.42s/it]
 65%|██████▌   | 1288/1976 [19:36:41<10:25:32, 54.55s/it]
 65%|██████▌   | 1289/1976 [19:37:36<10:24:14, 54.52s/it]
 65%|██████▌   | 1290/1976 [19:38:30<10:23:16, 54.51s/it]
                                                         
{'loss': 0.0933, 'learning_rate': 6.489560492119225e-05, 'epoch': 0.65}

 65%|██████▌   | 1290/1976 [19:38:30<10:23:16, 54.51s/it]
 65%|██████▌   | 1291/1976 [19:39:24<10:21:08, 54.41s/it]
 65%|██████▌   | 1292/1976 [19:40:18<10:19:30, 54.34s/it]
 65%|██████▌   | 1293/1976 [19:41:13<10:18:41, 54.35s/it]
 65%|██████▌   | 1294/1976 [19:42:07<10:16:17, 54.22s/it]
 66%|██████▌   | 1295/1976 [19:43:01<10:16:20, 54.30s/it]
                                                         
{'loss': 0.0929, 'learning_rate': 6.406974758408238e-05, 'epoch': 0.66}

 66%|██████▌   | 1295/1976 [19:43:01<10:16:20, 54.30s/it]
 66%|██████▌   | 1296/1976 [19:43:56<10:16:11, 54.37s/it]
 66%|██████▌   | 1297/1976 [19:44:50<10:15:36, 54.40s/it]
 66%|██████▌   | 1298/1976 [19:45:45<10:14:55, 54.42s/it]
 66%|██████▌   | 1299/1976 [19:46:39<10:14:09, 54.43s/it]
 66%|██████▌   | 1300/1976 [19:47:34<10:14:06, 54.51s/it]
                                                         
{'loss': 0.0926, 'learning_rate': 6.324669460645806e-05, 'epoch': 0.66}

 66%|██████▌   | 1300/1976 [19:47:34<10:14:06, 54.51s/it][INFO|trainer.py:2889] 2024-02-12 00:25:08,722 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1300
[INFO|tokenization_utils_base.py:2432] 2024-02-12 00:25:09,004 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-12 00:25:09,004 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1300/special_tokens_map.json
[2024-02-12 00:25:09,099] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1300 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-12 00:25:12,970] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1300/global_step1300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-12 00:25:12,970] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1300/global_step1300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-12 00:25:24,654] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1300/global_step1300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-12 00:25:24,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1300/global_step1300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-12 00:25:25,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1300/global_step1300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-12 00:25:25,546] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1300/global_step1300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-12 00:25:25,631] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1300 is ready now!
[INFO|trainer.py:2979] 2024-02-12 00:25:25,637 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-1200] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 66%|██████▌   | 1301/1976 [19:49:13<12:42:39, 67.79s/it]
 66%|██████▌   | 1302/1976 [19:50:08<11:59:32, 64.05s/it]
 66%|██████▌   | 1303/1976 [19:51:02<11:25:26, 61.11s/it]
 66%|██████▌   | 1304/1976 [19:51:56<11:01:23, 59.05s/it]
 66%|██████▌   | 1305/1976 [19:52:51<10:44:48, 57.66s/it]
                                                         
{'loss': 0.093, 'learning_rate': 6.24265102276794e-05, 'epoch': 0.66}

 66%|██████▌   | 1305/1976 [19:52:51<10:44:48, 57.66s/it]
 66%|██████▌   | 1306/1976 [19:53:45<10:32:34, 56.65s/it]
 66%|██████▌   | 1307/1976 [19:54:40<10:25:46, 56.12s/it]
 66%|██████▌   | 1308/1976 [19:55:35<10:19:39, 55.66s/it]
 66%|██████▌   | 1309/1976 [19:56:29<10:15:33, 55.37s/it]
 66%|██████▋   | 1310/1976 [19:57:24<10:11:56, 55.13s/it]
                                                         
{'loss': 0.0934, 'learning_rate': 6.160925846321212e-05, 'epoch': 0.66}

 66%|██████▋   | 1310/1976 [19:57:24<10:11:56, 55.13s/it]
 66%|██████▋   | 1311/1976 [19:58:18<10:08:17, 54.88s/it]
 66%|██████▋   | 1312/1976 [19:59:13<10:06:21, 54.79s/it]
 66%|██████▋   | 1313/1976 [20:00:07<10:02:35, 54.53s/it]
 66%|██████▋   | 1314/1976 [20:01:01<10:01:09, 54.49s/it]
 67%|██████▋   | 1315/1976 [20:01:55<9:59:18, 54.40s/it] 
                                                        
{'loss': 0.0927, 'learning_rate': 6.079500309963121e-05, 'epoch': 0.67}

 67%|██████▋   | 1315/1976 [20:01:55<9:59:18, 54.40s/it]
 67%|██████▋   | 1316/1976 [20:02:50<9:58:55, 54.45s/it]
 67%|██████▋   | 1317/1976 [20:03:44<9:58:31, 54.49s/it]
 67%|██████▋   | 1318/1976 [20:04:39<9:58:09, 54.54s/it]
 67%|██████▋   | 1319/1976 [20:05:34<9:57:59, 54.61s/it]
 67%|██████▋   | 1320/1976 [20:06:28<9:56:08, 54.52s/it]
                                                        
{'loss': 0.0918, 'learning_rate': 5.9983807689642204e-05, 'epoch': 0.67}

 67%|██████▋   | 1320/1976 [20:06:28<9:56:08, 54.52s/it]
 67%|██████▋   | 1321/1976 [20:07:23<9:54:50, 54.49s/it]
 67%|██████▋   | 1322/1976 [20:08:17<9:52:49, 54.39s/it]
 67%|██████▋   | 1323/1976 [20:09:11<9:51:18, 54.33s/it]
 67%|██████▋   | 1324/1976 [20:10:05<9:50:15, 54.32s/it]
 67%|██████▋   | 1325/1976 [20:10:59<9:49:05, 54.29s/it]
                                                        
{'loss': 0.0931, 'learning_rate': 5.917573554712097e-05, 'epoch': 0.67}

 67%|██████▋   | 1325/1976 [20:10:59<9:49:05, 54.29s/it]
 67%|██████▋   | 1326/1976 [20:11:54<9:49:37, 54.43s/it]
 67%|██████▋   | 1327/1976 [20:12:49<9:48:51, 54.44s/it]
 67%|██████▋   | 1328/1976 [20:13:43<9:48:18, 54.47s/it]
 67%|██████▋   | 1329/1976 [20:14:38<9:47:41, 54.50s/it]
 67%|██████▋   | 1330/1976 [20:15:32<9:46:59, 54.52s/it]
                                                        
{'loss': 0.0923, 'learning_rate': 5.837084974217212e-05, 'epoch': 0.67}

 67%|██████▋   | 1330/1976 [20:15:32<9:46:59, 54.52s/it]
 67%|██████▋   | 1331/1976 [20:16:27<9:45:01, 54.42s/it]
 67%|██████▋   | 1332/1976 [20:17:21<9:43:29, 54.36s/it]
 67%|██████▋   | 1333/1976 [20:18:15<9:43:18, 54.43s/it]
 68%|██████▊   | 1334/1976 [20:19:10<9:41:58, 54.39s/it]
 68%|██████▊   | 1335/1976 [20:20:04<9:40:32, 54.34s/it]
                                                        
{'loss': 0.0921, 'learning_rate': 5.756921309620631e-05, 'epoch': 0.68}

 68%|██████▊   | 1335/1976 [20:20:04<9:40:32, 54.34s/it]
 68%|██████▊   | 1336/1976 [20:20:58<9:40:28, 54.42s/it]
 68%|██████▊   | 1337/1976 [20:21:53<9:40:09, 54.48s/it]
 68%|██████▊   | 1338/1976 [20:22:48<9:39:45, 54.52s/it]
 68%|██████▊   | 1339/1976 [20:23:42<9:39:10, 54.55s/it]
 68%|██████▊   | 1340/1976 [20:24:37<9:39:19, 54.65s/it]
                                                        
{'loss': 0.0921, 'learning_rate': 5.67708881770371e-05, 'epoch': 0.68}

 68%|██████▊   | 1340/1976 [20:24:37<9:39:19, 54.65s/it]
 68%|██████▊   | 1341/1976 [20:25:31<9:36:48, 54.50s/it]
 68%|██████▊   | 1342/1976 [20:26:26<9:34:55, 54.41s/it]
 68%|██████▊   | 1343/1976 [20:27:20<9:33:14, 54.34s/it]
 68%|██████▊   | 1344/1976 [20:28:14<9:32:13, 54.33s/it]
 68%|██████▊   | 1345/1976 [20:29:08<9:30:58, 54.29s/it]
                                                        
{'loss': 0.0905, 'learning_rate': 5.597593729399725e-05, 'epoch': 0.68}

 68%|██████▊   | 1345/1976 [20:29:08<9:30:58, 54.29s/it]
 68%|██████▊   | 1346/1976 [20:30:03<9:31:01, 54.38s/it]
 68%|██████▊   | 1347/1976 [20:30:58<9:31:18, 54.50s/it]
 68%|██████▊   | 1348/1976 [20:31:52<9:30:27, 54.50s/it]
 68%|██████▊   | 1349/1976 [20:32:47<9:29:47, 54.53s/it]
 68%|██████▊   | 1350/1976 [20:33:41<9:28:45, 54.51s/it]
                                                        
{'loss': 0.093, 'learning_rate': 5.5184422493075985e-05, 'epoch': 0.68}

 68%|██████▊   | 1350/1976 [20:33:41<9:28:45, 54.51s/it]
 68%|██████▊   | 1351/1976 [20:34:35<9:27:07, 54.44s/it]
 68%|██████▊   | 1352/1976 [20:35:30<9:25:35, 54.38s/it]
 68%|██████▊   | 1353/1976 [20:36:24<9:24:03, 54.32s/it]
 69%|██████▊   | 1354/1976 [20:37:18<9:23:20, 54.34s/it]
 69%|██████▊   | 1355/1976 [20:38:12<9:21:48, 54.28s/it]
                                                        
{'loss': 0.0916, 'learning_rate': 5.4396405552075834e-05, 'epoch': 0.69}

 69%|██████▊   | 1355/1976 [20:38:12<9:21:48, 54.28s/it]
 69%|██████▊   | 1356/1976 [20:39:07<9:22:07, 54.40s/it]
 69%|██████▊   | 1357/1976 [20:40:02<9:21:27, 54.42s/it]
 69%|██████▊   | 1358/1976 [20:40:56<9:21:03, 54.47s/it]
 69%|██████▉   | 1359/1976 [20:41:51<9:20:39, 54.52s/it]
 69%|██████▉   | 1360/1976 [20:42:45<9:19:55, 54.54s/it]
                                                        
{'loss': 0.0927, 'learning_rate': 5.361194797579108e-05, 'epoch': 0.69}

 69%|██████▉   | 1360/1976 [20:42:45<9:19:55, 54.54s/it]
 69%|██████▉   | 1361/1976 [20:43:40<9:18:43, 54.51s/it]
 69%|██████▉   | 1362/1976 [20:44:34<9:16:49, 54.41s/it]
 69%|██████▉   | 1363/1976 [20:45:28<9:15:39, 54.39s/it]
 69%|██████▉   | 1364/1976 [20:46:23<9:14:24, 54.35s/it]
 69%|██████▉   | 1365/1976 [20:47:17<9:13:09, 54.32s/it]
                                                        
{'loss': 0.0929, 'learning_rate': 5.283111099120724e-05, 'epoch': 0.69}

 69%|██████▉   | 1365/1976 [20:47:17<9:13:09, 54.32s/it]
 69%|██████▉   | 1366/1976 [20:48:11<9:12:49, 54.38s/it]
 69%|██████▉   | 1367/1976 [20:49:06<9:12:35, 54.44s/it]
 69%|██████▉   | 1368/1976 [20:50:01<9:12:42, 54.54s/it]
 69%|██████▉   | 1369/1976 [20:50:55<9:12:02, 54.57s/it]
 69%|██████▉   | 1370/1976 [20:51:50<9:11:29, 54.60s/it]
                                                        
{'loss': 0.0918, 'learning_rate': 5.2053955542722365e-05, 'epoch': 0.69}

 69%|██████▉   | 1370/1976 [20:51:50<9:11:29, 54.60s/it]
 69%|██████▉   | 1371/1976 [20:52:44<9:09:22, 54.48s/it]
 69%|██████▉   | 1372/1976 [20:53:39<9:07:53, 54.43s/it]
 69%|██████▉   | 1373/1976 [20:54:33<9:06:24, 54.37s/it]
 70%|██████▉   | 1374/1976 [20:55:27<9:05:45, 54.39s/it]
 70%|██████▉   | 1375/1976 [20:56:22<9:05:34, 54.47s/it]
                                                        
{'loss': 0.0916, 'learning_rate': 5.1280542287390235e-05, 'epoch': 0.7}

 70%|██████▉   | 1375/1976 [20:56:22<9:05:34, 54.47s/it]
 70%|██████▉   | 1376/1976 [20:57:17<9:05:30, 54.55s/it]
 70%|██████▉   | 1377/1976 [20:58:11<9:05:21, 54.63s/it]
 70%|██████▉   | 1378/1976 [20:59:06<9:04:23, 54.62s/it]
 70%|██████▉   | 1379/1976 [21:00:01<9:04:03, 54.68s/it]
 70%|██████▉   | 1380/1976 [21:00:55<9:03:02, 54.67s/it]
                                                        
{'loss': 0.0915, 'learning_rate': 5.051093159018614e-05, 'epoch': 0.7}

 70%|██████▉   | 1380/1976 [21:00:55<9:03:02, 54.67s/it]
 70%|██████▉   | 1381/1976 [21:01:50<9:01:23, 54.59s/it]
 70%|██████▉   | 1382/1976 [21:02:45<9:00:49, 54.63s/it]
 70%|██████▉   | 1383/1976 [21:03:39<8:58:48, 54.52s/it]
 70%|███████   | 1384/1976 [21:04:33<8:57:34, 54.48s/it]
 70%|███████   | 1385/1976 [21:05:28<8:56:04, 54.42s/it]
                                                        
{'loss': 0.0922, 'learning_rate': 4.9745183519295334e-05, 'epoch': 0.7}

 70%|███████   | 1385/1976 [21:05:28<8:56:04, 54.42s/it]
 70%|███████   | 1386/1976 [21:06:22<8:56:21, 54.54s/it]
 70%|███████   | 1387/1976 [21:07:17<8:55:47, 54.58s/it]
 70%|███████   | 1388/1976 [21:08:12<8:55:40, 54.66s/it]
 70%|███████   | 1389/1976 [21:09:07<8:56:03, 54.79s/it]
 70%|███████   | 1390/1976 [21:10:02<8:54:46, 54.75s/it]
                                                        
{'loss': 0.0916, 'learning_rate': 4.898335784142476e-05, 'epoch': 0.7}

 70%|███████   | 1390/1976 [21:10:02<8:54:46, 54.75s/it]
 70%|███████   | 1391/1976 [21:10:56<8:52:43, 54.64s/it]
 70%|███████   | 1392/1976 [21:11:50<8:50:56, 54.55s/it]
 70%|███████   | 1393/1976 [21:12:45<8:49:15, 54.47s/it]
 71%|███████   | 1394/1976 [21:13:39<8:47:34, 54.39s/it]
 71%|███████   | 1395/1976 [21:14:33<8:46:29, 54.37s/it]
                                                        
{'loss': 0.0914, 'learning_rate': 4.822551401713821e-05, 'epoch': 0.71}

 71%|███████   | 1395/1976 [21:14:33<8:46:29, 54.37s/it]
 71%|███████   | 1396/1976 [21:15:28<8:47:11, 54.54s/it]
 71%|███████   | 1397/1976 [21:16:23<8:46:26, 54.55s/it]
 71%|███████   | 1398/1976 [21:17:17<8:45:57, 54.60s/it]
 71%|███████   | 1399/1976 [21:18:12<8:45:16, 54.62s/it]
 71%|███████   | 1400/1976 [21:19:07<8:44:14, 54.61s/it]
                                                        
{'loss': 0.0915, 'learning_rate': 4.747171119621544e-05, 'epoch': 0.71}

 71%|███████   | 1400/1976 [21:19:07<8:44:14, 54.61s/it][INFO|trainer.py:2889] 2024-02-12 01:56:41,758 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1400
[INFO|tokenization_utils_base.py:2432] 2024-02-12 01:56:42,043 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-12 01:56:42,044 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1400/special_tokens_map.json
[2024-02-12 01:56:43,172] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1400 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-12 01:56:47,569] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1400/global_step1400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-12 01:56:47,569] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1400/global_step1400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-12 01:56:59,100] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1400/global_step1400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-12 01:56:59,607] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-12 01:57:00,236] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-12 01:57:00,237] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-12 01:57:00,303] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1400 is ready now!
[INFO|trainer.py:2979] 2024-02-12 01:57:00,310 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-1300] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 71%|███████   | 1401/1976 [21:20:48<10:56:29, 68.50s/it]
 71%|███████   | 1402/1976 [21:21:42<10:13:44, 64.15s/it]
 71%|███████   | 1403/1976 [21:22:36<9:45:21, 61.29s/it] 
 71%|███████   | 1404/1976 [21:23:31<9:24:30, 59.21s/it]
 71%|███████   | 1405/1976 [21:24:25<9:09:38, 57.76s/it]
                                                        
{'loss': 0.0914, 'learning_rate': 4.672200821303555e-05, 'epoch': 0.71}

 71%|███████   | 1405/1976 [21:24:25<9:09:38, 57.76s/it]
 71%|███████   | 1406/1976 [21:25:20<9:00:15, 56.87s/it]
 71%|███████   | 1407/1976 [21:26:15<8:53:40, 56.28s/it]
 71%|███████▏  | 1408/1976 [21:27:10<8:49:39, 55.95s/it]
 71%|███████▏  | 1409/1976 [21:28:05<8:45:26, 55.60s/it]
 71%|███████▏  | 1410/1976 [21:28:59<8:42:19, 55.37s/it]
                                                        
{'loss': 0.092, 'learning_rate': 4.5976463581984894e-05, 'epoch': 0.71}

 71%|███████▏  | 1410/1976 [21:28:59<8:42:19, 55.37s/it]
 71%|███████▏  | 1411/1976 [21:29:54<8:38:36, 55.07s/it]
 71%|███████▏  | 1412/1976 [21:30:48<8:35:39, 54.86s/it]
 72%|███████▏  | 1413/1976 [21:31:42<8:32:57, 54.67s/it]
 72%|███████▏  | 1414/1976 [21:32:37<8:31:07, 54.57s/it]
 72%|███████▏  | 1415/1976 [21:33:31<8:30:00, 54.55s/it]
                                                        
{'loss': 0.0916, 'learning_rate': 4.523513549289008e-05, 'epoch': 0.72}

 72%|███████▏  | 1415/1976 [21:33:31<8:30:00, 54.55s/it]
 72%|███████▏  | 1416/1976 [21:34:26<8:29:17, 54.57s/it]
 72%|███████▏  | 1417/1976 [21:35:20<8:28:40, 54.60s/it]
 72%|███████▏  | 1418/1976 [21:36:15<8:27:44, 54.60s/it]
 72%|███████▏  | 1419/1976 [21:37:10<8:26:44, 54.59s/it]
 72%|███████▏  | 1420/1976 [21:38:04<8:25:38, 54.57s/it]
                                                        
{'loss': 0.0912, 'learning_rate': 4.449808180647619e-05, 'epoch': 0.72}

 72%|███████▏  | 1420/1976 [21:38:04<8:25:38, 54.57s/it]
 72%|███████▏  | 1421/1976 [21:38:58<8:23:47, 54.46s/it]
 72%|███████▏  | 1422/1976 [21:39:53<8:22:48, 54.46s/it]
 72%|███████▏  | 1423/1976 [21:40:47<8:20:55, 54.35s/it]
 72%|███████▏  | 1424/1976 [21:41:41<8:19:34, 54.30s/it]
 72%|███████▏  | 1425/1976 [21:42:35<8:18:26, 54.28s/it]
                                                        
{'loss': 0.0914, 'learning_rate': 4.376536004985095e-05, 'epoch': 0.72}

 72%|███████▏  | 1425/1976 [21:42:35<8:18:26, 54.28s/it]
 72%|███████▏  | 1426/1976 [21:43:30<8:18:39, 54.40s/it]
 72%|███████▏  | 1427/1976 [21:44:25<8:18:33, 54.49s/it]
 72%|███████▏  | 1428/1976 [21:45:19<8:17:58, 54.52s/it]
 72%|███████▏  | 1429/1976 [21:46:14<8:18:24, 54.67s/it]
 72%|███████▏  | 1430/1976 [21:47:09<8:17:24, 54.66s/it]
                                                        
{'loss': 0.0922, 'learning_rate': 4.303702741201431e-05, 'epoch': 0.72}

 72%|███████▏  | 1430/1976 [21:47:09<8:17:24, 54.66s/it]
 72%|███████▏  | 1431/1976 [21:48:03<8:15:16, 54.53s/it]
 72%|███████▏  | 1432/1976 [21:48:57<8:13:32, 54.43s/it]
 73%|███████▎  | 1433/1976 [21:49:52<8:12:31, 54.42s/it]
 73%|███████▎  | 1434/1976 [21:50:46<8:11:37, 54.42s/it]
 73%|███████▎  | 1435/1976 [21:51:41<8:10:59, 54.45s/it]
                                                        
{'loss': 0.0913, 'learning_rate': 4.2313140739395276e-05, 'epoch': 0.73}

 73%|███████▎  | 1435/1976 [21:51:41<8:10:59, 54.45s/it]
 73%|███████▎  | 1436/1976 [21:52:36<8:11:40, 54.63s/it]
 73%|███████▎  | 1437/1976 [21:53:31<8:11:05, 54.67s/it]
 73%|███████▎  | 1438/1976 [21:54:25<8:10:42, 54.73s/it]
 73%|███████▎  | 1439/1976 [21:55:20<8:09:49, 54.73s/it]
 73%|███████▎  | 1440/1976 [21:56:15<8:08:32, 54.69s/it]
                                                        
{'loss': 0.0905, 'learning_rate': 4.1593756531414784e-05, 'epoch': 0.73}

 73%|███████▎  | 1440/1976 [21:56:15<8:08:32, 54.69s/it]
 73%|███████▎  | 1441/1976 [21:57:09<8:06:13, 54.53s/it]
 73%|███████▎  | 1442/1976 [21:58:03<8:04:23, 54.43s/it]
 73%|███████▎  | 1443/1976 [21:58:57<8:03:05, 54.38s/it]
 73%|███████▎  | 1444/1976 [21:59:51<8:01:31, 54.31s/it]
 73%|███████▎  | 1445/1976 [22:00:46<8:00:17, 54.27s/it]
                                                        
{'loss': 0.09, 'learning_rate': 4.087893093607614e-05, 'epoch': 0.73}

 73%|███████▎  | 1445/1976 [22:00:46<8:00:17, 54.27s/it]
 73%|███████▎  | 1446/1976 [22:01:40<8:00:16, 54.37s/it]
 73%|███████▎  | 1447/1976 [22:02:35<7:59:51, 54.43s/it]
 73%|███████▎  | 1448/1976 [22:03:29<7:59:10, 54.45s/it]
 73%|███████▎  | 1449/1976 [22:04:24<7:58:22, 54.46s/it]
 73%|███████▎  | 1450/1976 [22:05:19<7:58:12, 54.55s/it]
                                                        
{'loss': 0.0905, 'learning_rate': 4.016871974558229e-05, 'epoch': 0.73}

 73%|███████▎  | 1450/1976 [22:05:19<7:58:12, 54.55s/it]
 73%|███████▎  | 1451/1976 [22:06:13<7:56:10, 54.42s/it]
 73%|███████▎  | 1452/1976 [22:07:07<7:54:33, 54.34s/it]
 74%|███████▎  | 1453/1976 [22:08:01<7:53:22, 54.31s/it]
 74%|███████▎  | 1454/1976 [22:08:55<7:52:02, 54.26s/it]
 74%|███████▎  | 1455/1976 [22:09:49<7:51:03, 54.25s/it]
                                                        
{'loss': 0.0903, 'learning_rate': 3.9463178391981625e-05, 'epoch': 0.74}

 74%|███████▎  | 1455/1976 [22:09:49<7:51:03, 54.25s/it]
 74%|███████▎  | 1456/1976 [22:10:44<7:51:03, 54.35s/it]
 74%|███████▎  | 1457/1976 [22:11:39<7:51:07, 54.47s/it]
 74%|███████▍  | 1458/1976 [22:12:33<7:50:32, 54.50s/it]
 74%|███████▍  | 1459/1976 [22:13:28<7:49:27, 54.48s/it]
 74%|███████▍  | 1460/1976 [22:14:22<7:48:49, 54.52s/it]
                                                        
{'loss': 0.0909, 'learning_rate': 3.8762361942841244e-05, 'epoch': 0.74}

 74%|███████▍  | 1460/1976 [22:14:22<7:48:49, 54.52s/it]
 74%|███████▍  | 1461/1976 [22:15:17<7:46:58, 54.40s/it]
 74%|███████▍  | 1462/1976 [22:16:11<7:45:30, 54.34s/it]
 74%|███████▍  | 1463/1976 [22:17:05<7:44:18, 54.30s/it]
 74%|███████▍  | 1464/1976 [22:17:59<7:43:29, 54.32s/it]
 74%|███████▍  | 1465/1976 [22:18:53<7:42:05, 54.26s/it]
                                                        
{'loss': 0.0903, 'learning_rate': 3.806632509694915e-05, 'epoch': 0.74}

 74%|███████▍  | 1465/1976 [22:18:53<7:42:05, 54.26s/it]
 74%|███████▍  | 1466/1976 [22:19:48<7:41:43, 54.32s/it]
 74%|███████▍  | 1467/1976 [22:20:42<7:41:13, 54.37s/it]
 74%|███████▍  | 1468/1976 [22:21:37<7:40:33, 54.40s/it]
 74%|███████▍  | 1469/1976 [22:22:31<7:40:11, 54.46s/it]
 74%|███████▍  | 1470/1976 [22:23:26<7:39:16, 54.46s/it]
                                                        
{'loss': 0.0906, 'learning_rate': 3.737512218004472e-05, 'epoch': 0.74}

 74%|███████▍  | 1470/1976 [22:23:26<7:39:16, 54.46s/it]
 74%|███████▍  | 1471/1976 [22:24:20<7:38:19, 54.45s/it]
 74%|███████▍  | 1472/1976 [22:25:15<7:36:45, 54.38s/it]
 75%|███████▍  | 1473/1976 [22:26:09<7:35:18, 54.31s/it]
 75%|███████▍  | 1474/1976 [22:27:03<7:34:04, 54.27s/it]
 75%|███████▍  | 1475/1976 [22:27:57<7:32:50, 54.23s/it]
                                                        
{'loss': 0.0909, 'learning_rate': 3.668880714057882e-05, 'epoch': 0.75}

 75%|███████▍  | 1475/1976 [22:27:57<7:32:50, 54.23s/it]
 75%|███████▍  | 1476/1976 [22:28:51<7:32:34, 54.31s/it]
 75%|███████▍  | 1477/1976 [22:29:46<7:32:18, 54.39s/it]
 75%|███████▍  | 1478/1976 [22:30:41<7:32:23, 54.50s/it]
 75%|███████▍  | 1479/1976 [22:31:35<7:31:24, 54.50s/it]
 75%|███████▍  | 1480/1976 [22:32:30<7:30:27, 54.49s/it]
                                                        
{'loss': 0.0894, 'learning_rate': 3.6007433545502974e-05, 'epoch': 0.75}

 75%|███████▍  | 1480/1976 [22:32:30<7:30:27, 54.49s/it]
 75%|███████▍  | 1481/1976 [22:33:24<7:28:47, 54.40s/it]
 75%|███████▌  | 1482/1976 [22:34:18<7:27:28, 54.35s/it]
 75%|███████▌  | 1483/1976 [22:35:12<7:26:17, 54.32s/it]
 75%|███████▌  | 1484/1976 [22:36:07<7:24:48, 54.25s/it]
 75%|███████▌  | 1485/1976 [22:37:01<7:24:12, 54.28s/it]
                                                        
{'loss': 0.0899, 'learning_rate': 3.5331054576088695e-05, 'epoch': 0.75}

 75%|███████▌  | 1485/1976 [22:37:01<7:24:12, 54.28s/it]
 75%|███████▌  | 1486/1976 [22:37:55<7:23:56, 54.36s/it]
 75%|███████▌  | 1487/1976 [22:38:50<7:23:17, 54.39s/it]
 75%|███████▌  | 1488/1976 [22:39:44<7:22:39, 54.42s/it]
 75%|███████▌  | 1489/1976 [22:40:39<7:22:02, 54.46s/it]
 75%|███████▌  | 1490/1976 [22:41:34<7:21:28, 54.50s/it]
                                                        
{'loss': 0.0911, 'learning_rate': 3.465972302377625e-05, 'epoch': 0.75}

 75%|███████▌  | 1490/1976 [22:41:34<7:21:28, 54.50s/it]
 75%|███████▌  | 1491/1976 [22:42:28<7:19:43, 54.40s/it]
 76%|███████▌  | 1492/1976 [22:43:22<7:18:47, 54.40s/it]
 76%|███████▌  | 1493/1976 [22:44:16<7:17:34, 54.36s/it]
 76%|███████▌  | 1494/1976 [22:45:11<7:16:25, 54.33s/it]
 76%|███████▌  | 1495/1976 [22:46:05<7:15:20, 54.31s/it]
                                                        
{'loss': 0.0909, 'learning_rate': 3.399349128605467e-05, 'epoch': 0.76}

 76%|███████▌  | 1495/1976 [22:46:05<7:15:20, 54.31s/it]
 76%|███████▌  | 1496/1976 [22:46:59<7:15:03, 54.38s/it]
 76%|███████▌  | 1497/1976 [22:47:54<7:14:54, 54.48s/it]
 76%|███████▌  | 1498/1976 [22:48:49<7:14:26, 54.53s/it]
 76%|███████▌  | 1499/1976 [22:49:44<7:14:45, 54.69s/it]
 76%|███████▌  | 1500/1976 [22:50:39<7:13:56, 54.70s/it]
                                                        
{'loss': 0.0899, 'learning_rate': 3.333241136237206e-05, 'epoch': 0.76}

 76%|███████▌  | 1500/1976 [22:50:39<7:13:56, 54.70s/it][INFO|trainer.py:2889] 2024-02-12 03:28:15,394 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1500
[INFO|tokenization_utils_base.py:2432] 2024-02-12 03:28:15,680 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-12 03:28:15,680 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1500/special_tokens_map.json
[2024-02-12 03:28:16,959] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1500 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-12 03:28:21,213] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-12 03:28:21,213] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-12 03:28:33,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-12 03:28:34,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-12 03:28:34,861] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-12 03:28:34,861] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-12 03:28:34,960] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1500 is ready now!
[INFO|trainer.py:2979] 2024-02-12 03:28:34,968 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-1400] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 76%|███████▌  | 1501/1976 [22:52:22<9:09:16, 69.38s/it]
 76%|███████▌  | 1502/1976 [22:53:17<8:32:24, 64.86s/it]
 76%|███████▌  | 1503/1976 [22:54:11<8:06:17, 61.69s/it]
 76%|███████▌  | 1504/1976 [22:55:05<7:48:20, 59.53s/it]
 76%|███████▌  | 1505/1976 [22:56:00<7:35:11, 57.99s/it]
                                                        
{'loss': 0.0899, 'learning_rate': 3.267653485007689e-05, 'epoch': 0.76}

 76%|███████▌  | 1505/1976 [22:56:00<7:35:11, 57.99s/it]
 76%|███████▌  | 1506/1976 [22:56:54<7:26:18, 56.97s/it]
 76%|███████▋  | 1507/1976 [22:57:49<7:19:51, 56.27s/it]
 76%|███████▋  | 1508/1976 [22:58:44<7:15:11, 55.79s/it]
 76%|███████▋  | 1509/1976 [22:59:38<7:11:31, 55.44s/it]
 76%|███████▋  | 1510/1976 [23:00:33<7:08:54, 55.22s/it]
                                                        
{'loss': 0.09, 'learning_rate': 3.2025912940390825e-05, 'epoch': 0.76}

 76%|███████▋  | 1510/1976 [23:00:33<7:08:54, 55.22s/it]
 76%|███████▋  | 1511/1976 [23:01:28<7:06:44, 55.06s/it]
 77%|███████▋  | 1512/1976 [23:02:22<7:04:03, 54.84s/it]
 77%|███████▋  | 1513/1976 [23:03:16<7:01:56, 54.68s/it]
 77%|███████▋  | 1514/1976 [23:04:11<7:00:11, 54.57s/it]
 77%|███████▋  | 1515/1976 [23:05:05<6:58:34, 54.48s/it]
                                                        
{'loss': 0.0904, 'learning_rate': 3.13805964144134e-05, 'epoch': 0.77}

 77%|███████▋  | 1515/1976 [23:05:05<6:58:34, 54.48s/it]
 77%|███████▋  | 1516/1976 [23:05:59<6:57:48, 54.50s/it]
 77%|███████▋  | 1517/1976 [23:06:54<6:57:04, 54.52s/it]
 77%|███████▋  | 1518/1976 [23:07:49<6:56:48, 54.60s/it]
 77%|███████▋  | 1519/1976 [23:08:43<6:55:44, 54.58s/it]
 77%|███████▋  | 1520/1976 [23:09:38<6:54:53, 54.59s/it]
                                                        
{'loss': 0.0892, 'learning_rate': 3.074063563915852e-05, 'epoch': 0.77}

 77%|███████▋  | 1520/1976 [23:09:38<6:54:53, 54.59s/it]
 77%|███████▋  | 1521/1976 [23:10:32<6:53:08, 54.48s/it]
 77%|███████▋  | 1522/1976 [23:11:26<6:51:25, 54.37s/it]
 77%|███████▋  | 1523/1976 [23:12:20<6:50:06, 54.32s/it]
 77%|███████▋  | 1524/1976 [23:13:15<6:48:46, 54.26s/it]
 77%|███████▋  | 1525/1976 [23:14:09<6:48:22, 54.33s/it]
                                                        
{'loss': 0.0902, 'learning_rate': 3.010608056362316e-05, 'epoch': 0.77}

 77%|███████▋  | 1525/1976 [23:14:09<6:48:22, 54.33s/it]
 77%|███████▋  | 1526/1976 [23:15:04<6:47:55, 54.39s/it]
 77%|███████▋  | 1527/1976 [23:15:58<6:47:10, 54.41s/it]
 77%|███████▋  | 1528/1976 [23:16:53<6:46:35, 54.45s/it]
 77%|███████▋  | 1529/1976 [23:17:47<6:45:59, 54.50s/it]
 77%|███████▋  | 1530/1976 [23:18:42<6:45:13, 54.51s/it]
                                                        
{'loss': 0.0913, 'learning_rate': 2.9476980714889012e-05, 'epoch': 0.77}

 77%|███████▋  | 1530/1976 [23:18:42<6:45:13, 54.51s/it]
 77%|███████▋  | 1531/1976 [23:19:36<6:43:26, 54.40s/it]
 78%|███████▊  | 1532/1976 [23:20:30<6:42:16, 54.36s/it]
 78%|███████▊  | 1533/1976 [23:21:24<6:41:09, 54.33s/it]
 78%|███████▊  | 1534/1976 [23:22:19<6:39:55, 54.29s/it]
 78%|███████▊  | 1535/1976 [23:23:13<6:38:41, 54.24s/it]
                                                        
{'loss': 0.0886, 'learning_rate': 2.8853385194256676e-05, 'epoch': 0.78}

 78%|███████▊  | 1535/1976 [23:23:13<6:38:41, 54.24s/it]
 78%|███████▊  | 1536/1976 [23:24:07<6:38:17, 54.31s/it]
 78%|███████▊  | 1537/1976 [23:25:02<6:38:00, 54.40s/it]
 78%|███████▊  | 1538/1976 [23:25:56<6:37:22, 54.43s/it]
 78%|███████▊  | 1539/1976 [23:26:51<6:37:23, 54.56s/it]
 78%|███████▊  | 1540/1976 [23:27:46<6:36:38, 54.58s/it]
                                                        
{'loss': 0.0886, 'learning_rate': 2.8235342673413635e-05, 'epoch': 0.78}

 78%|███████▊  | 1540/1976 [23:27:46<6:36:38, 54.58s/it]
 78%|███████▊  | 1541/1976 [23:28:40<6:35:14, 54.52s/it]
 78%|███████▊  | 1542/1976 [23:29:34<6:33:46, 54.44s/it]
 78%|███████▊  | 1543/1976 [23:30:29<6:32:25, 54.38s/it]
 78%|███████▊  | 1544/1976 [23:31:23<6:31:14, 54.34s/it]
 78%|███████▊  | 1545/1976 [23:32:17<6:30:06, 54.31s/it]
                                                        
{'loss': 0.0881, 'learning_rate': 2.7622901390635114e-05, 'epoch': 0.78}

 78%|███████▊  | 1545/1976 [23:32:17<6:30:06, 54.31s/it]
 78%|███████▊  | 1546/1976 [23:33:12<6:30:11, 54.45s/it]
 78%|███████▊  | 1547/1976 [23:34:07<6:29:41, 54.50s/it]
 78%|███████▊  | 1548/1976 [23:35:01<6:28:53, 54.52s/it]
 78%|███████▊  | 1549/1976 [23:35:56<6:28:17, 54.56s/it]
 78%|███████▊  | 1550/1976 [23:36:50<6:27:37, 54.59s/it]
                                                        
{'loss': 0.0906, 'learning_rate': 2.701610914701921e-05, 'epoch': 0.78}

 78%|███████▊  | 1550/1976 [23:36:50<6:27:37, 54.59s/it]
 78%|███████▊  | 1551/1976 [23:37:45<6:26:05, 54.51s/it]
 79%|███████▊  | 1552/1976 [23:38:39<6:24:21, 54.39s/it]
 79%|███████▊  | 1553/1976 [23:39:33<6:23:27, 54.39s/it]
 79%|███████▊  | 1554/1976 [23:40:27<6:22:08, 54.33s/it]
 79%|███████▊  | 1555/1976 [23:41:22<6:20:48, 54.27s/it]
                                                        
{'loss': 0.0895, 'learning_rate': 2.6415013302756e-05, 'epoch': 0.79}

 79%|███████▊  | 1555/1976 [23:41:22<6:20:48, 54.27s/it]
 79%|███████▊  | 1556/1976 [23:42:16<6:20:33, 54.36s/it]
 79%|███████▉  | 1557/1976 [23:43:11<6:20:01, 54.42s/it]
 79%|███████▉  | 1558/1976 [23:44:05<6:19:25, 54.46s/it]
 79%|███████▉  | 1559/1976 [23:45:00<6:18:30, 54.46s/it]
 79%|███████▉  | 1560/1976 [23:45:54<6:18:01, 54.52s/it]
                                                        
{'loss': 0.0897, 'learning_rate': 2.5819660773431087e-05, 'epoch': 0.79}

 79%|███████▉  | 1560/1976 [23:45:54<6:18:01, 54.52s/it]
 79%|███████▉  | 1561/1976 [23:46:49<6:16:38, 54.45s/it]
 79%|███████▉  | 1562/1976 [23:47:43<6:15:10, 54.37s/it]
 79%|███████▉  | 1563/1976 [23:48:37<6:13:39, 54.28s/it]
 79%|███████▉  | 1564/1976 [23:49:31<6:12:20, 54.23s/it]
 79%|███████▉  | 1565/1976 [23:50:25<6:11:18, 54.21s/it]
                                                        
{'loss': 0.0889, 'learning_rate': 2.52300980263638e-05, 'epoch': 0.79}

 79%|███████▉  | 1565/1976 [23:50:25<6:11:18, 54.21s/it]
 79%|███████▉  | 1566/1976 [23:51:20<6:10:55, 54.28s/it]
 79%|███████▉  | 1567/1976 [23:52:14<6:10:46, 54.39s/it]
 79%|███████▉  | 1568/1976 [23:53:09<6:10:09, 54.43s/it]
 79%|███████▉  | 1569/1976 [23:54:03<6:09:26, 54.46s/it]
 79%|███████▉  | 1570/1976 [23:54:58<6:08:27, 54.45s/it]
                                                        
{'loss': 0.0885, 'learning_rate': 2.4646371076980457e-05, 'epoch': 0.79}

 79%|███████▉  | 1570/1976 [23:54:58<6:08:27, 54.45s/it]
 80%|███████▉  | 1571/1976 [23:55:52<6:06:46, 54.34s/it]
 80%|███████▉  | 1572/1976 [23:56:46<6:05:23, 54.27s/it]
 80%|███████▉  | 1573/1976 [23:57:40<6:04:05, 54.21s/it]
 80%|███████▉  | 1574/1976 [23:58:34<6:03:19, 54.23s/it]
 80%|███████▉  | 1575/1976 [23:59:28<6:02:19, 54.21s/it]
                                                        
{'loss': 0.0889, 'learning_rate': 2.4068525485222793e-05, 'epoch': 0.8}

 80%|███████▉  | 1575/1976 [23:59:28<6:02:19, 54.21s/it]
 80%|███████▉  | 1576/1976 [24:00:23<6:01:51, 54.28s/it]
 80%|███████▉  | 1577/1976 [24:01:17<6:01:25, 54.35s/it]
 80%|███████▉  | 1578/1976 [24:02:12<6:00:56, 54.41s/it]
 80%|███████▉  | 1579/1976 [24:03:06<6:00:12, 54.44s/it]
 80%|███████▉  | 1580/1976 [24:04:01<5:59:20, 54.45s/it]
                                                        
{'loss': 0.0888, 'learning_rate': 2.349660635199209e-05, 'epoch': 0.8}

 80%|███████▉  | 1580/1976 [24:04:01<5:59:20, 54.45s/it]
 80%|████████  | 1581/1976 [24:04:55<5:58:35, 54.47s/it]
 80%|████████  | 1582/1976 [24:05:50<5:57:04, 54.38s/it]
 80%|████████  | 1583/1976 [24:06:44<5:55:44, 54.31s/it]
 80%|████████  | 1584/1976 [24:07:38<5:54:37, 54.28s/it]
 80%|████████  | 1585/1976 [24:08:32<5:53:30, 54.25s/it]
                                                        
{'loss': 0.088, 'learning_rate': 2.293065831562896e-05, 'epoch': 0.8}

 80%|████████  | 1585/1976 [24:08:32<5:53:30, 54.25s/it]
 80%|████████  | 1586/1976 [24:09:27<5:53:05, 54.32s/it]
 80%|████████  | 1587/1976 [24:10:21<5:52:24, 54.36s/it]
 80%|████████  | 1588/1976 [24:11:16<5:52:16, 54.48s/it]
 80%|████████  | 1589/1976 [24:12:10<5:51:32, 54.50s/it]
 80%|████████  | 1590/1976 [24:13:05<5:50:38, 54.51s/it]
                                                        
{'loss': 0.0884, 'learning_rate': 2.2370725548429382e-05, 'epoch': 0.8}

 80%|████████  | 1590/1976 [24:13:05<5:50:38, 54.51s/it]
 81%|████████  | 1591/1976 [24:13:59<5:49:14, 54.43s/it]
 81%|████████  | 1592/1976 [24:14:53<5:47:40, 54.32s/it]
 81%|████████  | 1593/1976 [24:15:47<5:46:34, 54.29s/it]
 81%|████████  | 1594/1976 [24:16:42<5:45:18, 54.24s/it]
 81%|████████  | 1595/1976 [24:17:36<5:44:39, 54.28s/it]
                                                        
{'loss': 0.0896, 'learning_rate': 2.181685175319702e-05, 'epoch': 0.81}

 81%|████████  | 1595/1976 [24:17:36<5:44:39, 54.28s/it]
 81%|████████  | 1596/1976 [24:18:30<5:44:11, 54.35s/it]
 81%|████████  | 1597/1976 [24:19:25<5:43:35, 54.39s/it]
 81%|████████  | 1598/1976 [24:20:20<5:42:56, 54.44s/it]
 81%|████████  | 1599/1976 [24:21:14<5:42:13, 54.47s/it]
 81%|████████  | 1600/1976 [24:22:09<5:41:28, 54.49s/it]
                                                        
{'loss': 0.0894, 'learning_rate': 2.1269080159832232e-05, 'epoch': 0.81}

 81%|████████  | 1600/1976 [24:22:09<5:41:28, 54.49s/it][INFO|trainer.py:2889] 2024-02-12 04:59:43,979 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1600
[INFO|tokenization_utils_base.py:2432] 2024-02-12 04:59:44,287 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-12 04:59:44,287 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1600/special_tokens_map.json
[2024-02-12 04:59:44,383] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1600 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-12 04:59:49,867] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1600/global_step1600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-12 04:59:49,867] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1600/global_step1600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-12 05:00:03,562] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1600/global_step1600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-12 05:00:04,076] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-12 05:00:04,657] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-12 05:00:04,658] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-12 05:00:04,746] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1600 is ready now!
[INFO|trainer.py:2979] 2024-02-12 05:00:04,753 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-1500] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 81%|████████  | 1601/1976 [24:23:51<7:11:11, 68.99s/it]
 81%|████████  | 1602/1976 [24:24:46<6:43:05, 64.67s/it]
 81%|████████  | 1603/1976 [24:25:40<6:22:34, 61.54s/it]
 81%|████████  | 1604/1976 [24:26:35<6:08:07, 59.38s/it]
 81%|████████  | 1605/1976 [24:27:29<5:57:52, 57.88s/it]
                                                        
{'loss': 0.0892, 'learning_rate': 2.072745352195794e-05, 'epoch': 0.81}

 81%|████████  | 1605/1976 [24:27:29<5:57:52, 57.88s/it]
 81%|████████▏ | 1606/1976 [24:28:24<5:51:01, 56.92s/it]
 81%|████████▏ | 1607/1976 [24:29:19<5:46:26, 56.33s/it]
 81%|████████▏ | 1608/1976 [24:30:13<5:42:37, 55.86s/it]
 81%|████████▏ | 1609/1976 [24:31:08<5:39:37, 55.52s/it]
 81%|████████▏ | 1610/1976 [24:32:03<5:37:09, 55.27s/it]
                                                        
{'loss': 0.0892, 'learning_rate': 2.019201411358275e-05, 'epoch': 0.81}

 81%|████████▏ | 1610/1976 [24:32:03<5:37:09, 55.27s/it]
 82%|████████▏ | 1611/1976 [24:32:57<5:34:32, 54.99s/it]
 82%|████████▏ | 1612/1976 [24:33:51<5:32:29, 54.81s/it]
 82%|████████▏ | 1613/1976 [24:34:46<5:30:42, 54.66s/it]
 82%|████████▏ | 1614/1976 [24:35:41<5:29:51, 54.67s/it]
 82%|████████▏ | 1615/1976 [24:36:35<5:28:00, 54.52s/it]
                                                        
{'loss': 0.0899, 'learning_rate': 1.9662803725801417e-05, 'epoch': 0.82}

 82%|████████▏ | 1615/1976 [24:36:35<5:28:00, 54.52s/it]
 82%|████████▏ | 1616/1976 [24:37:29<5:27:13, 54.54s/it]
 82%|████████▏ | 1617/1976 [24:38:24<5:26:17, 54.53s/it]
 82%|████████▏ | 1618/1976 [24:39:18<5:25:20, 54.53s/it]
 82%|████████▏ | 1619/1976 [24:40:13<5:24:29, 54.54s/it]
 82%|████████▏ | 1620/1976 [24:41:07<5:23:29, 54.52s/it]
                                                        
{'loss': 0.09, 'learning_rate': 1.9139863663533097e-05, 'epoch': 0.82}

 82%|████████▏ | 1620/1976 [24:41:07<5:23:29, 54.52s/it]
 82%|████████▏ | 1621/1976 [24:42:02<5:22:14, 54.46s/it]
 82%|████████▏ | 1622/1976 [24:42:56<5:20:44, 54.36s/it]
 82%|████████▏ | 1623/1976 [24:43:50<5:19:26, 54.30s/it]
 82%|████████▏ | 1624/1976 [24:44:44<5:18:17, 54.25s/it]
 82%|████████▏ | 1625/1976 [24:45:38<5:17:12, 54.22s/it]
                                                        
{'loss': 0.0887, 'learning_rate': 1.8623234742297457e-05, 'epoch': 0.82}

 82%|████████▏ | 1625/1976 [24:45:38<5:17:12, 54.22s/it]
 82%|████████▏ | 1626/1976 [24:46:33<5:16:52, 54.32s/it]
 82%|████████▏ | 1627/1976 [24:47:27<5:16:05, 54.34s/it]
 82%|████████▏ | 1628/1976 [24:48:22<5:15:46, 54.44s/it]
 82%|████████▏ | 1629/1976 [24:49:16<5:14:59, 54.47s/it]
 82%|████████▏ | 1630/1976 [24:50:11<5:14:12, 54.49s/it]
                                                        
{'loss': 0.09, 'learning_rate': 1.811295728502902e-05, 'epoch': 0.82}

 82%|████████▏ | 1630/1976 [24:50:11<5:14:12, 54.49s/it]
 83%|████████▎ | 1631/1976 [24:51:05<5:12:49, 54.41s/it]
 83%|████████▎ | 1632/1976 [24:51:59<5:11:39, 54.36s/it]
 83%|████████▎ | 1633/1976 [24:52:54<5:10:34, 54.33s/it]
 83%|████████▎ | 1634/1976 [24:53:48<5:09:36, 54.32s/it]
 83%|████████▎ | 1635/1976 [24:54:42<5:08:59, 54.37s/it]
                                                        
{'loss': 0.0877, 'learning_rate': 1.7609071118929966e-05, 'epoch': 0.83}

 83%|████████▎ | 1635/1976 [24:54:42<5:08:59, 54.37s/it]
 83%|████████▎ | 1636/1976 [24:55:37<5:08:36, 54.46s/it]
 83%|████████▎ | 1637/1976 [24:56:32<5:07:58, 54.51s/it]
 83%|████████▎ | 1638/1976 [24:57:26<5:07:10, 54.53s/it]
 83%|████████▎ | 1639/1976 [24:58:21<5:06:31, 54.58s/it]
 83%|████████▎ | 1640/1976 [24:59:16<5:05:39, 54.58s/it]
                                                        
{'loss': 0.0894, 'learning_rate': 1.7111615572361628e-05, 'epoch': 0.83}

 83%|████████▎ | 1640/1976 [24:59:16<5:05:39, 54.58s/it]
 83%|████████▎ | 1641/1976 [25:00:10<5:04:15, 54.49s/it]
 83%|████████▎ | 1642/1976 [25:01:04<5:03:35, 54.54s/it]
 83%|████████▎ | 1643/1976 [25:01:59<5:02:06, 54.43s/it]
 83%|████████▎ | 1644/1976 [25:02:53<5:00:57, 54.39s/it]
 83%|████████▎ | 1645/1976 [25:03:47<4:59:36, 54.31s/it]
                                                        
{'loss': 0.0891, 'learning_rate': 1.6620629471774886e-05, 'epoch': 0.83}

 83%|████████▎ | 1645/1976 [25:03:47<4:59:36, 54.31s/it]
 83%|████████▎ | 1646/1976 [25:04:42<4:59:08, 54.39s/it]
 83%|████████▎ | 1647/1976 [25:05:36<4:58:29, 54.44s/it]
 83%|████████▎ | 1648/1976 [25:06:31<4:57:44, 54.46s/it]
 83%|████████▎ | 1649/1976 [25:07:25<4:57:13, 54.54s/it]
 84%|████████▎ | 1650/1976 [25:08:20<4:56:15, 54.53s/it]
                                                        
{'loss': 0.0896, 'learning_rate': 1.6136151138679767e-05, 'epoch': 0.83}

 84%|████████▎ | 1650/1976 [25:08:20<4:56:15, 54.53s/it]
 84%|████████▎ | 1651/1976 [25:09:14<4:54:44, 54.41s/it]
 84%|████████▎ | 1652/1976 [25:10:08<4:53:19, 54.32s/it]
 84%|████████▎ | 1653/1976 [25:11:02<4:52:07, 54.27s/it]
 84%|████████▎ | 1654/1976 [25:11:57<4:51:04, 54.24s/it]
 84%|████████▍ | 1655/1976 [25:12:51<4:49:51, 54.18s/it]
                                                        
{'loss': 0.0889, 'learning_rate': 1.5658218386654465e-05, 'epoch': 0.84}

 84%|████████▍ | 1655/1976 [25:12:51<4:49:51, 54.18s/it]
 84%|████████▍ | 1656/1976 [25:13:45<4:49:43, 54.32s/it]
 84%|████████▍ | 1657/1976 [25:14:40<4:49:04, 54.37s/it]
 84%|████████▍ | 1658/1976 [25:15:34<4:48:16, 54.39s/it]
 84%|████████▍ | 1659/1976 [25:16:29<4:47:33, 54.43s/it]
 84%|████████▍ | 1660/1976 [25:17:23<4:46:48, 54.46s/it]
                                                        
{'loss': 0.0886, 'learning_rate': 1.518686851839397e-05, 'epoch': 0.84}

 84%|████████▍ | 1660/1976 [25:17:23<4:46:48, 54.46s/it]
 84%|████████▍ | 1661/1976 [25:18:17<4:45:24, 54.36s/it]
 84%|████████▍ | 1662/1976 [25:19:11<4:43:59, 54.27s/it]
 84%|████████▍ | 1663/1976 [25:20:06<4:43:05, 54.27s/it]
 84%|████████▍ | 1664/1976 [25:21:00<4:42:03, 54.24s/it]
 84%|████████▍ | 1665/1976 [25:21:54<4:40:58, 54.21s/it]
                                                        
{'loss': 0.0886, 'learning_rate': 1.472213832279855e-05, 'epoch': 0.84}

 84%|████████▍ | 1665/1976 [25:21:54<4:40:58, 54.21s/it]
 84%|████████▍ | 1666/1976 [25:22:48<4:40:24, 54.27s/it]
 84%|████████▍ | 1667/1976 [25:23:43<4:39:51, 54.34s/it]
 84%|████████▍ | 1668/1976 [25:24:37<4:39:06, 54.37s/it]
 84%|████████▍ | 1669/1976 [25:25:32<4:38:33, 54.44s/it]
 85%|████████▍ | 1670/1976 [25:26:27<4:38:25, 54.59s/it]
                                                        
{'loss': 0.0889, 'learning_rate': 1.4264064072102502e-05, 'epoch': 0.84}

 85%|████████▍ | 1670/1976 [25:26:27<4:38:25, 54.59s/it]
 85%|████████▍ | 1671/1976 [25:27:21<4:36:52, 54.47s/it]
 85%|████████▍ | 1672/1976 [25:28:15<4:35:34, 54.39s/it]
 85%|████████▍ | 1673/1976 [25:29:09<4:34:23, 54.34s/it]
 85%|████████▍ | 1674/1976 [25:30:04<4:33:17, 54.30s/it]
 85%|████████▍ | 1675/1976 [25:30:58<4:32:12, 54.26s/it]
                                                        
{'loss': 0.0898, 'learning_rate': 1.381268151904298e-05, 'epoch': 0.85}

 85%|████████▍ | 1675/1976 [25:30:58<4:32:12, 54.26s/it]
 85%|████████▍ | 1676/1976 [25:31:52<4:31:44, 54.35s/it]
 85%|████████▍ | 1677/1976 [25:32:47<4:31:28, 54.48s/it]
 85%|████████▍ | 1678/1976 [25:33:42<4:30:37, 54.49s/it]
 85%|████████▍ | 1679/1976 [25:34:36<4:29:46, 54.50s/it]
 85%|████████▌ | 1680/1976 [25:35:31<4:28:53, 54.51s/it]
                                                        
{'loss': 0.0893, 'learning_rate': 1.3368025894069447e-05, 'epoch': 0.85}

 85%|████████▌ | 1680/1976 [25:35:31<4:28:53, 54.51s/it]
 85%|████████▌ | 1681/1976 [25:36:25<4:27:28, 54.40s/it]
 85%|████████▌ | 1682/1976 [25:37:19<4:26:26, 54.38s/it]
 85%|████████▌ | 1683/1976 [25:38:13<4:25:19, 54.33s/it]
 85%|████████▌ | 1684/1976 [25:39:08<4:24:34, 54.36s/it]
 85%|████████▌ | 1685/1976 [25:40:02<4:23:27, 54.32s/it]
                                                        
{'loss': 0.0893, 'learning_rate': 1.2930131902594089e-05, 'epoch': 0.85}

 85%|████████▌ | 1685/1976 [25:40:02<4:23:27, 54.32s/it]
 85%|████████▌ | 1686/1976 [25:40:57<4:23:00, 54.41s/it]
 85%|████████▌ | 1687/1976 [25:41:51<4:22:18, 54.46s/it]
 85%|████████▌ | 1688/1976 [25:42:46<4:21:39, 54.51s/it]
 85%|████████▌ | 1689/1976 [25:43:41<4:21:03, 54.58s/it]
 86%|████████▌ | 1690/1976 [25:44:35<4:20:18, 54.61s/it]
                                                        
{'loss': 0.0886, 'learning_rate': 1.2499033722283026e-05, 'epoch': 0.86}

 86%|████████▌ | 1690/1976 [25:44:35<4:20:18, 54.61s/it]
 86%|████████▌ | 1691/1976 [25:45:30<4:19:13, 54.58s/it]
 86%|████████▌ | 1692/1976 [25:46:24<4:17:54, 54.49s/it]
 86%|████████▌ | 1693/1976 [25:47:18<4:16:40, 54.42s/it]
 86%|████████▌ | 1694/1976 [25:48:13<4:15:27, 54.35s/it]
 86%|████████▌ | 1695/1976 [25:49:07<4:14:30, 54.34s/it]
                                                        
{'loss': 0.0877, 'learning_rate': 1.2074765000388611e-05, 'epoch': 0.86}

 86%|████████▌ | 1695/1976 [25:49:07<4:14:30, 54.34s/it]
 86%|████████▌ | 1696/1976 [25:50:02<4:14:01, 54.43s/it]
 86%|████████▌ | 1697/1976 [25:50:56<4:13:27, 54.51s/it]
 86%|████████▌ | 1698/1976 [25:51:51<4:13:08, 54.64s/it]
 86%|████████▌ | 1699/1976 [25:52:46<4:12:13, 54.63s/it]
 86%|████████▌ | 1700/1976 [25:53:40<4:11:11, 54.61s/it]
                                                        
{'loss': 0.0877, 'learning_rate': 1.1657358851123357e-05, 'epoch': 0.86}

 86%|████████▌ | 1700/1976 [25:53:40<4:11:11, 54.61s/it][INFO|trainer.py:2889] 2024-02-12 06:31:16,488 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1700
[INFO|tokenization_utils_base.py:2432] 2024-02-12 06:31:16,780 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-12 06:31:16,780 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1700/special_tokens_map.json
[2024-02-12 06:31:18,008] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1700 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-12 06:31:21,967] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1700/global_step1700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-12 06:31:21,967] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1700/global_step1700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-12 06:31:33,618] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1700/global_step1700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-12 06:31:33,970] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1700/global_step1700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-12 06:31:34,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1700/global_step1700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-12 06:31:34,574] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1700/global_step1700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-12 06:31:34,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1700 is ready now!
[INFO|trainer.py:2979] 2024-02-12 06:31:34,647 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-1600] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 86%|████████▌ | 1701/1976 [25:55:22<5:14:29, 68.62s/it]
 86%|████████▌ | 1702/1976 [25:56:16<4:53:38, 64.30s/it]
 86%|████████▌ | 1703/1976 [25:57:10<4:39:02, 61.33s/it]
 86%|████████▌ | 1704/1976 [25:58:04<4:28:04, 59.14s/it]
 86%|████████▋ | 1705/1976 [25:58:59<4:20:47, 57.74s/it]
                                                        
{'loss': 0.0889, 'learning_rate': 1.1246847853075349e-05, 'epoch': 0.86}

 86%|████████▋ | 1705/1976 [25:58:59<4:20:47, 57.74s/it]
 86%|████████▋ | 1706/1976 [25:59:53<4:15:44, 56.83s/it]
 86%|████████▋ | 1707/1976 [26:00:48<4:11:55, 56.19s/it]
 86%|████████▋ | 1708/1976 [26:01:43<4:08:59, 55.74s/it]
 86%|████████▋ | 1709/1976 [26:02:38<4:06:46, 55.45s/it]
 87%|████████▋ | 1710/1976 [26:03:33<4:05:19, 55.34s/it]
                                                        
{'loss': 0.0887, 'learning_rate': 1.0843264046665557e-05, 'epoch': 0.87}

 87%|████████▋ | 1710/1976 [26:03:33<4:05:19, 55.34s/it]
 87%|████████▋ | 1711/1976 [26:04:27<4:03:07, 55.05s/it]
 87%|████████▋ | 1712/1976 [26:05:21<4:01:13, 54.82s/it]
 87%|████████▋ | 1713/1976 [26:06:16<3:59:45, 54.70s/it]
 87%|████████▋ | 1714/1976 [26:07:10<3:58:24, 54.60s/it]
 87%|████████▋ | 1715/1976 [26:08:05<3:57:14, 54.54s/it]
                                                        
{'loss': 0.0886, 'learning_rate': 1.044663893164698e-05, 'epoch': 0.87}

 87%|████████▋ | 1715/1976 [26:08:05<3:57:14, 54.54s/it]
 87%|████████▋ | 1716/1976 [26:08:59<3:56:35, 54.60s/it]
 87%|████████▋ | 1717/1976 [26:09:54<3:56:10, 54.71s/it]
 87%|████████▋ | 1718/1976 [26:10:49<3:55:20, 54.73s/it]
 87%|████████▋ | 1719/1976 [26:11:44<3:54:24, 54.73s/it]
 87%|████████▋ | 1720/1976 [26:12:38<3:53:17, 54.68s/it]
                                                        
{'loss': 0.0885, 'learning_rate': 1.005700346464603e-05, 'epoch': 0.87}

 87%|████████▋ | 1720/1976 [26:12:38<3:53:17, 54.68s/it]
 87%|████████▋ | 1721/1976 [26:13:33<3:51:57, 54.58s/it]
 87%|████████▋ | 1722/1976 [26:14:27<3:50:28, 54.44s/it]
 87%|████████▋ | 1723/1976 [26:15:21<3:49:20, 54.39s/it]
 87%|████████▋ | 1724/1976 [26:16:16<3:48:33, 54.42s/it]
 87%|████████▋ | 1725/1976 [26:17:10<3:47:18, 54.34s/it]
                                                        
{'loss': 0.088, 'learning_rate': 9.674388056746553e-06, 'epoch': 0.87}

 87%|████████▋ | 1725/1976 [26:17:10<3:47:18, 54.34s/it]
 87%|████████▋ | 1726/1976 [26:18:04<3:46:42, 54.41s/it]
 87%|████████▋ | 1727/1976 [26:18:59<3:45:53, 54.43s/it]
 87%|████████▋ | 1728/1976 [26:19:53<3:45:05, 54.46s/it]
 88%|████████▊ | 1729/1976 [26:20:48<3:44:15, 54.47s/it]
 88%|████████▊ | 1730/1976 [26:21:42<3:43:19, 54.47s/it]
                                                        
{'loss': 0.0882, 'learning_rate': 9.298822571116139e-06, 'epoch': 0.88}

 88%|████████▊ | 1730/1976 [26:21:42<3:43:19, 54.47s/it]
 88%|████████▊ | 1731/1976 [26:22:37<3:42:22, 54.46s/it]
 88%|████████▊ | 1732/1976 [26:23:31<3:41:05, 54.37s/it]
 88%|████████▊ | 1733/1976 [26:24:25<3:39:53, 54.29s/it]
 88%|████████▊ | 1734/1976 [26:25:19<3:38:56, 54.28s/it]
 88%|████████▊ | 1735/1976 [26:26:13<3:37:58, 54.27s/it]
                                                        
{'loss': 0.0887, 'learning_rate': 8.930336320675281e-06, 'epoch': 0.88}

 88%|████████▊ | 1735/1976 [26:26:13<3:37:58, 54.27s/it]
 88%|████████▊ | 1736/1976 [26:27:08<3:37:20, 54.33s/it]
 88%|████████▊ | 1737/1976 [26:28:02<3:36:36, 54.38s/it]
 88%|████████▊ | 1738/1976 [26:28:57<3:36:12, 54.50s/it]
 88%|████████▊ | 1739/1976 [26:29:52<3:35:27, 54.55s/it]
 88%|████████▊ | 1740/1976 [26:30:46<3:34:36, 54.56s/it]
                                                        
{'loss': 0.0876, 'learning_rate': 8.56895806580943e-06, 'epoch': 0.88}

 88%|████████▊ | 1740/1976 [26:30:46<3:34:36, 54.56s/it]
 88%|████████▊ | 1741/1976 [26:31:41<3:33:25, 54.49s/it]
 88%|████████▊ | 1742/1976 [26:32:35<3:32:11, 54.41s/it]
 88%|████████▊ | 1743/1976 [26:33:29<3:31:16, 54.41s/it]
 88%|████████▊ | 1744/1976 [26:34:24<3:30:15, 54.38s/it]
 88%|████████▊ | 1745/1976 [26:35:18<3:29:45, 54.48s/it]
                                                        
{'loss': 0.0887, 'learning_rate': 8.21471601212449e-06, 'epoch': 0.88}

 88%|████████▊ | 1745/1976 [26:35:18<3:29:45, 54.48s/it]
 88%|████████▊ | 1746/1976 [26:36:13<3:29:05, 54.55s/it]
 88%|████████▊ | 1747/1976 [26:37:08<3:28:21, 54.59s/it]
 88%|████████▊ | 1748/1976 [26:38:02<3:27:25, 54.59s/it]
 89%|████████▊ | 1749/1976 [26:38:57<3:26:38, 54.62s/it]
 89%|████████▊ | 1750/1976 [26:39:52<3:25:46, 54.63s/it]
                                                        
{'loss': 0.0881, 'learning_rate': 7.86763780824512e-06, 'epoch': 0.89}

 89%|████████▊ | 1750/1976 [26:39:52<3:25:46, 54.63s/it]
 89%|████████▊ | 1751/1976 [26:40:46<3:24:27, 54.52s/it]
 89%|████████▊ | 1752/1976 [26:41:40<3:23:25, 54.49s/it]
 89%|████████▊ | 1753/1976 [26:42:35<3:22:15, 54.42s/it]
 89%|████████▉ | 1754/1976 [26:43:29<3:21:07, 54.36s/it]
 89%|████████▉ | 1755/1976 [26:44:23<3:20:01, 54.31s/it]
                                                        
{'loss': 0.0884, 'learning_rate': 7.527750543656853e-06, 'epoch': 0.89}

 89%|████████▉ | 1755/1976 [26:44:23<3:20:01, 54.31s/it]
 89%|████████▉ | 1756/1976 [26:45:18<3:19:21, 54.37s/it]
 89%|████████▉ | 1757/1976 [26:46:12<3:18:37, 54.42s/it]
 89%|████████▉ | 1758/1976 [26:47:07<3:17:45, 54.43s/it]
 89%|████████▉ | 1759/1976 [26:48:01<3:17:13, 54.53s/it]
 89%|████████▉ | 1760/1976 [26:48:56<3:16:18, 54.53s/it]
                                                        
{'loss': 0.0879, 'learning_rate': 7.1950807465917315e-06, 'epoch': 0.89}

 89%|████████▉ | 1760/1976 [26:48:56<3:16:18, 54.53s/it]
 89%|████████▉ | 1761/1976 [26:49:50<3:15:00, 54.42s/it]
 89%|████████▉ | 1762/1976 [26:50:44<3:13:53, 54.36s/it]
 89%|████████▉ | 1763/1976 [26:51:39<3:12:53, 54.33s/it]
 89%|████████▉ | 1764/1976 [26:52:33<3:11:44, 54.27s/it]
 89%|████████▉ | 1765/1976 [26:53:27<3:10:41, 54.23s/it]
                                                        
{'loss': 0.0881, 'learning_rate': 6.869654381957857e-06, 'epoch': 0.89}

 89%|████████▉ | 1765/1976 [26:53:27<3:10:41, 54.23s/it]
 89%|████████▉ | 1766/1976 [26:54:22<3:10:20, 54.38s/it]
 89%|████████▉ | 1767/1976 [26:55:16<3:09:39, 54.45s/it]
 89%|████████▉ | 1768/1976 [26:56:11<3:08:47, 54.46s/it]
 90%|████████▉ | 1769/1976 [26:57:05<3:08:09, 54.54s/it]
 90%|████████▉ | 1770/1976 [26:58:00<3:07:15, 54.54s/it]
                                                        
{'loss': 0.0895, 'learning_rate': 6.551496849312744e-06, 'epoch': 0.9}

 90%|████████▉ | 1770/1976 [26:58:00<3:07:15, 54.54s/it]
 90%|████████▉ | 1771/1976 [26:58:54<3:06:06, 54.47s/it]
 90%|████████▉ | 1772/1976 [26:59:48<3:04:58, 54.41s/it]
 90%|████████▉ | 1773/1976 [27:00:43<3:04:08, 54.43s/it]
 90%|████████▉ | 1774/1976 [27:01:37<3:03:00, 54.36s/it]
 90%|████████▉ | 1775/1976 [27:02:31<3:02:00, 54.33s/it]
                                                        
{'loss': 0.0873, 'learning_rate': 6.2406329808808895e-06, 'epoch': 0.9}

 90%|████████▉ | 1775/1976 [27:02:31<3:02:00, 54.33s/it]
 90%|████████▉ | 1776/1976 [27:03:26<3:01:18, 54.39s/it]
 90%|████████▉ | 1777/1976 [27:04:21<3:00:42, 54.49s/it]
 90%|████████▉ | 1778/1976 [27:05:15<2:59:59, 54.54s/it]
 90%|█████████ | 1779/1976 [27:06:10<2:59:12, 54.58s/it]
 90%|█████████ | 1780/1976 [27:07:05<2:58:35, 54.67s/it]
                                                        
{'loss': 0.0869, 'learning_rate': 5.937087039615619e-06, 'epoch': 0.9}

 90%|█████████ | 1780/1976 [27:07:05<2:58:35, 54.67s/it]
 90%|█████████ | 1781/1976 [27:07:59<2:57:11, 54.52s/it]
 90%|█████████ | 1782/1976 [27:08:53<2:56:00, 54.44s/it]
 90%|█████████ | 1783/1976 [27:09:47<2:54:51, 54.36s/it]
 90%|█████████ | 1784/1976 [27:10:42<2:53:49, 54.32s/it]
 90%|█████████ | 1785/1976 [27:11:36<2:52:47, 54.28s/it]
                                                        
{'loss': 0.0881, 'learning_rate': 5.640882717305418e-06, 'epoch': 0.9}

 90%|█████████ | 1785/1976 [27:11:36<2:52:47, 54.28s/it]
 90%|█████████ | 1786/1976 [27:12:30<2:52:09, 54.36s/it]
 90%|█████████ | 1787/1976 [27:13:25<2:51:33, 54.47s/it]
 90%|█████████ | 1788/1976 [27:14:20<2:50:47, 54.51s/it]
 91%|█████████ | 1789/1976 [27:15:14<2:49:54, 54.52s/it]
 91%|█████████ | 1790/1976 [27:16:09<2:49:05, 54.55s/it]
                                                        
{'loss': 0.0887, 'learning_rate': 5.352043132724726e-06, 'epoch': 0.91}

 91%|█████████ | 1790/1976 [27:16:09<2:49:05, 54.55s/it]
 91%|█████████ | 1791/1976 [27:17:03<2:47:55, 54.46s/it]
 91%|█████████ | 1792/1976 [27:17:57<2:46:44, 54.37s/it]
 91%|█████████ | 1793/1976 [27:18:52<2:45:38, 54.31s/it]
 91%|█████████ | 1794/1976 [27:19:46<2:44:50, 54.34s/it]
 91%|█████████ | 1795/1976 [27:20:40<2:43:49, 54.31s/it]
                                                        
{'loss': 0.0873, 'learning_rate': 5.070590829829514e-06, 'epoch': 0.91}

 91%|█████████ | 1795/1976 [27:20:40<2:43:49, 54.31s/it]
 91%|█████████ | 1796/1976 [27:21:35<2:43:09, 54.38s/it]
 91%|█████████ | 1797/1976 [27:22:29<2:42:23, 54.43s/it]
 91%|█████████ | 1798/1976 [27:23:24<2:41:38, 54.49s/it]
 91%|█████████ | 1799/1976 [27:24:19<2:40:59, 54.57s/it]
 91%|█████████ | 1800/1976 [27:25:13<2:40:07, 54.59s/it]
                                                        
{'loss': 0.0877, 'learning_rate': 4.7965477759977395e-06, 'epoch': 0.91}

 91%|█████████ | 1800/1976 [27:25:13<2:40:07, 54.59s/it][INFO|trainer.py:2889] 2024-02-12 08:02:47,723 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1800
[INFO|tokenization_utils_base.py:2432] 2024-02-12 08:02:47,994 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-12 08:02:47,995 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1800/special_tokens_map.json
[2024-02-12 08:02:48,087] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1800 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-12 08:02:53,025] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-12 08:02:53,026] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-12 08:03:05,554] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-12 08:03:06,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-12 08:03:06,508] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-12 08:03:06,508] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-12 08:03:06,624] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1800 is ready now!
[INFO|trainer.py:2979] 2024-02-12 08:03:06,632 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-1700] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 91%|█████████ | 1801/1976 [27:26:55<3:20:19, 68.68s/it]
 91%|█████████ | 1802/1976 [27:27:49<3:06:39, 64.36s/it]
 91%|█████████ | 1803/1976 [27:28:44<2:57:00, 61.39s/it]
 91%|█████████▏| 1804/1976 [27:29:38<2:49:57, 59.29s/it]
 91%|█████████▏| 1805/1976 [27:30:32<2:44:41, 57.79s/it]
                                                        
{'loss': 0.0879, 'learning_rate': 4.529935360314829e-06, 'epoch': 0.91}

 91%|█████████▏| 1805/1976 [27:30:32<2:44:41, 57.79s/it]
 91%|█████████▏| 1806/1976 [27:31:27<2:41:14, 56.91s/it]
 91%|█████████▏| 1807/1976 [27:32:22<2:38:16, 56.19s/it]
 91%|█████████▏| 1808/1976 [27:33:16<2:36:00, 55.72s/it]
 92%|█████████▏| 1809/1976 [27:34:11<2:34:06, 55.37s/it]
 92%|█████████▏| 1810/1976 [27:35:05<2:32:28, 55.11s/it]
                                                        
{'loss': 0.0863, 'learning_rate': 4.270774391904186e-06, 'epoch': 0.92}

 92%|█████████▏| 1810/1976 [27:35:05<2:32:28, 55.11s/it]
 92%|█████████▏| 1811/1976 [27:36:00<2:30:50, 54.85s/it]
 92%|█████████▏| 1812/1976 [27:36:54<2:29:23, 54.66s/it]
 92%|█████████▏| 1813/1976 [27:37:48<2:28:13, 54.56s/it]
 92%|█████████▏| 1814/1976 [27:38:42<2:26:58, 54.43s/it]
 92%|█████████▏| 1815/1976 [27:39:36<2:25:48, 54.34s/it]
                                                        
{'loss': 0.088, 'learning_rate': 4.019085098303077e-06, 'epoch': 0.92}

 92%|█████████▏| 1815/1976 [27:39:36<2:25:48, 54.34s/it]
 92%|█████████▏| 1816/1976 [27:40:31<2:25:04, 54.40s/it]
 92%|█████████▏| 1817/1976 [27:41:25<2:24:12, 54.42s/it]
 92%|█████████▏| 1818/1976 [27:42:20<2:23:22, 54.45s/it]
 92%|█████████▏| 1819/1976 [27:43:15<2:22:37, 54.50s/it]
 92%|█████████▏| 1820/1976 [27:44:09<2:22:04, 54.64s/it]
                                                        
{'loss': 0.0874, 'learning_rate': 3.7748871238838857e-06, 'epoch': 0.92}

 92%|█████████▏| 1820/1976 [27:44:09<2:22:04, 54.64s/it]
 92%|█████████▏| 1821/1976 [27:45:04<2:20:54, 54.55s/it]
 92%|█████████▏| 1822/1976 [27:45:58<2:19:50, 54.48s/it]
 92%|█████████▏| 1823/1976 [27:46:52<2:18:42, 54.40s/it]
 92%|█████████▏| 1824/1976 [27:47:47<2:17:46, 54.39s/it]
 92%|█████████▏| 1825/1976 [27:48:41<2:16:43, 54.33s/it]
                                                        
{'loss': 0.0867, 'learning_rate': 3.5381995283208402e-06, 'epoch': 0.92}

 92%|█████████▏| 1825/1976 [27:48:41<2:16:43, 54.33s/it]
 92%|█████████▏| 1826/1976 [27:49:35<2:15:58, 54.39s/it]
 92%|█████████▏| 1827/1976 [27:50:30<2:15:28, 54.55s/it]
 93%|█████████▎| 1828/1976 [27:51:25<2:14:32, 54.54s/it]
 93%|█████████▎| 1829/1976 [27:52:19<2:13:36, 54.53s/it]
 93%|█████████▎| 1830/1976 [27:53:14<2:12:43, 54.54s/it]
                                                        
{'loss': 0.0879, 'learning_rate': 3.309040785102402e-06, 'epoch': 0.93}

 93%|█████████▎| 1830/1976 [27:53:14<2:12:43, 54.54s/it]
 93%|█████████▎| 1831/1976 [27:54:08<2:11:36, 54.46s/it]
 93%|█████████▎| 1832/1976 [27:55:02<2:10:29, 54.37s/it]
 93%|█████████▎| 1833/1976 [27:55:57<2:09:28, 54.33s/it]
 93%|█████████▎| 1834/1976 [27:56:51<2:08:41, 54.38s/it]
 93%|█████████▎| 1835/1976 [27:57:45<2:07:37, 54.31s/it]
                                                        
{'loss': 0.0877, 'learning_rate': 3.08742878008943e-06, 'epoch': 0.93}

 93%|█████████▎| 1835/1976 [27:57:45<2:07:37, 54.31s/it]
 93%|█████████▎| 1836/1976 [27:58:40<2:06:50, 54.36s/it]
 93%|█████████▎| 1837/1976 [27:59:34<2:06:04, 54.42s/it]
 93%|█████████▎| 1838/1976 [28:00:29<2:05:13, 54.44s/it]
 93%|█████████▎| 1839/1976 [28:01:23<2:04:24, 54.48s/it]
 93%|█████████▎| 1840/1976 [28:02:18<2:03:31, 54.50s/it]
                                                        
{'loss': 0.0887, 'learning_rate': 2.873380810119175e-06, 'epoch': 0.93}

 93%|█████████▎| 1840/1976 [28:02:18<2:03:31, 54.50s/it]
 93%|█████████▎| 1841/1976 [28:03:12<2:02:37, 54.50s/it]
 93%|█████████▎| 1842/1976 [28:04:07<2:01:30, 54.41s/it]
 93%|█████████▎| 1843/1976 [28:05:01<2:00:26, 54.34s/it]
 93%|█████████▎| 1844/1976 [28:05:55<1:59:28, 54.31s/it]
 93%|█████████▎| 1845/1976 [28:06:49<1:58:29, 54.27s/it]
                                                        
{'loss': 0.0878, 'learning_rate': 2.6669135816552616e-06, 'epoch': 0.93}

 93%|█████████▎| 1845/1976 [28:06:49<1:58:29, 54.27s/it]
 93%|█████████▎| 1846/1976 [28:07:44<1:57:49, 54.38s/it]
 93%|█████████▎| 1847/1976 [28:08:38<1:57:02, 54.44s/it]
 94%|█████████▎| 1848/1976 [28:09:33<1:56:22, 54.55s/it]
 94%|█████████▎| 1849/1976 [28:10:28<1:55:27, 54.55s/it]
 94%|█████████▎| 1850/1976 [28:11:22<1:54:31, 54.53s/it]
                                                        
{'loss': 0.0886, 'learning_rate': 2.468043209483739e-06, 'epoch': 0.94}

 94%|█████████▎| 1850/1976 [28:11:22<1:54:31, 54.53s/it]
 94%|█████████▎| 1851/1976 [28:12:16<1:53:24, 54.44s/it]
 94%|█████████▎| 1852/1976 [28:13:11<1:52:23, 54.38s/it]
 94%|█████████▍| 1853/1976 [28:14:05<1:51:23, 54.34s/it]
 94%|█████████▍| 1854/1976 [28:14:59<1:50:25, 54.31s/it]
 94%|█████████▍| 1855/1976 [28:15:54<1:49:38, 54.37s/it]
                                                        
{'loss': 0.0889, 'learning_rate': 2.276785215455335e-06, 'epoch': 0.94}

 94%|█████████▍| 1855/1976 [28:15:54<1:49:38, 54.37s/it]
 94%|█████████▍| 1856/1976 [28:16:48<1:48:54, 54.45s/it]
 94%|█████████▍| 1857/1976 [28:17:43<1:48:06, 54.51s/it]
 94%|█████████▍| 1858/1976 [28:18:38<1:47:17, 54.56s/it]
 94%|█████████▍| 1859/1976 [28:19:32<1:46:22, 54.55s/it]
 94%|█████████▍| 1860/1976 [28:20:27<1:45:30, 54.57s/it]
                                                        
{'loss': 0.0872, 'learning_rate': 2.0931545272739885e-06, 'epoch': 0.94}

 94%|█████████▍| 1860/1976 [28:20:27<1:45:30, 54.57s/it]
 94%|█████████▍| 1861/1976 [28:21:21<1:44:25, 54.48s/it]
 94%|█████████▍| 1862/1976 [28:22:16<1:43:34, 54.51s/it]
 94%|█████████▍| 1863/1976 [28:23:10<1:42:30, 54.43s/it]
 94%|█████████▍| 1864/1976 [28:24:04<1:41:26, 54.35s/it]
 94%|█████████▍| 1865/1976 [28:24:58<1:40:29, 54.32s/it]
                                                        
{'loss': 0.0862, 'learning_rate': 1.9171654773316617e-06, 'epoch': 0.94}

 94%|█████████▍| 1865/1976 [28:24:58<1:40:29, 54.32s/it]
 94%|█████████▍| 1866/1976 [28:25:53<1:39:39, 54.36s/it]
 94%|█████████▍| 1867/1976 [28:26:47<1:38:51, 54.42s/it]
 95%|█████████▍| 1868/1976 [28:27:42<1:38:02, 54.47s/it]
 95%|█████████▍| 1869/1976 [28:28:37<1:37:18, 54.57s/it]
 95%|█████████▍| 1870/1976 [28:29:31<1:36:25, 54.58s/it]
                                                        
{'loss': 0.0887, 'learning_rate': 1.748831801589812e-06, 'epoch': 0.95}

 95%|█████████▍| 1870/1976 [28:29:31<1:36:25, 54.58s/it]
 95%|█████████▍| 1871/1976 [28:30:25<1:35:19, 54.47s/it]
 95%|█████████▍| 1872/1976 [28:31:20<1:34:17, 54.40s/it]
 95%|█████████▍| 1873/1976 [28:32:14<1:33:18, 54.36s/it]
 95%|█████████▍| 1874/1976 [28:33:08<1:32:20, 54.32s/it]
 95%|█████████▍| 1875/1976 [28:34:02<1:31:19, 54.26s/it]
                                                        
{'loss': 0.0875, 'learning_rate': 1.5881666385072158e-06, 'epoch': 0.95}

 95%|█████████▍| 1875/1976 [28:34:02<1:31:19, 54.26s/it]
 95%|█████████▍| 1876/1976 [28:34:57<1:30:44, 54.44s/it]
 95%|█████████▍| 1877/1976 [28:35:52<1:29:54, 54.49s/it]
 95%|█████████▌| 1878/1976 [28:36:46<1:29:02, 54.51s/it]
 95%|█████████▌| 1879/1976 [28:37:41<1:28:09, 54.53s/it]
 95%|█████████▌| 1880/1976 [28:38:35<1:27:12, 54.50s/it]
                                                        
{'loss': 0.0875, 'learning_rate': 1.4351825280145448e-06, 'epoch': 0.95}

 95%|█████████▌| 1880/1976 [28:38:35<1:27:12, 54.50s/it]
 95%|█████████▌| 1881/1976 [28:39:30<1:26:09, 54.42s/it]
 95%|█████████▌| 1882/1976 [28:40:24<1:25:07, 54.33s/it]
 95%|█████████▌| 1883/1976 [28:41:18<1:24:16, 54.37s/it]
 95%|█████████▌| 1884/1976 [28:42:12<1:23:16, 54.31s/it]
 95%|█████████▌| 1885/1976 [28:43:07<1:22:18, 54.27s/it]
                                                        
{'loss': 0.0876, 'learning_rate': 1.289891410535593e-06, 'epoch': 0.95}

 95%|█████████▌| 1885/1976 [28:43:07<1:22:18, 54.27s/it]
 95%|█████████▌| 1886/1976 [28:44:01<1:21:31, 54.35s/it]
 95%|█████████▌| 1887/1976 [28:44:56<1:20:44, 54.44s/it]
 96%|█████████▌| 1888/1976 [28:45:50<1:19:54, 54.49s/it]
 96%|█████████▌| 1889/1976 [28:46:45<1:19:03, 54.52s/it]
 96%|█████████▌| 1890/1976 [28:47:40<1:18:12, 54.57s/it]
                                                        
{'loss': 0.0886, 'learning_rate': 1.1523046260553672e-06, 'epoch': 0.96}

 96%|█████████▌| 1890/1976 [28:47:40<1:18:12, 54.57s/it]
 96%|█████████▌| 1891/1976 [28:48:34<1:17:10, 54.48s/it]
 96%|█████████▌| 1892/1976 [28:49:28<1:16:09, 54.40s/it]
 96%|█████████▌| 1893/1976 [28:50:22<1:15:10, 54.35s/it]
 96%|█████████▌| 1894/1976 [28:51:16<1:14:11, 54.28s/it]
 96%|█████████▌| 1895/1976 [28:52:11<1:13:17, 54.29s/it]
                                                        
{'loss': 0.0871, 'learning_rate': 1.0224329132349718e-06, 'epoch': 0.96}

 96%|█████████▌| 1895/1976 [28:52:11<1:13:17, 54.29s/it]
 96%|█████████▌| 1896/1976 [28:53:05<1:12:30, 54.38s/it]
 96%|█████████▌| 1897/1976 [28:54:00<1:11:46, 54.52s/it]
 96%|█████████▌| 1898/1976 [28:54:55<1:10:56, 54.57s/it]
 96%|█████████▌| 1899/1976 [28:55:49<1:10:02, 54.58s/it]
 96%|█████████▌| 1900/1976 [28:56:44<1:09:08, 54.59s/it]
                                                        
{'loss': 0.0893, 'learning_rate': 9.002864085734808e-07, 'epoch': 0.96}

 96%|█████████▌| 1900/1976 [28:56:44<1:09:08, 54.59s/it][INFO|trainer.py:2889] 2024-02-12 09:34:18,845 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1900
[INFO|tokenization_utils_base.py:2432] 2024-02-12 09:34:19,117 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-12 09:34:19,117 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1900/special_tokens_map.json
[2024-02-12 09:34:20,003] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1900 is about to be saved!
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-12 09:34:23,934] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1900/global_step1900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-02-12 09:34:23,934] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1900/global_step1900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-02-12 09:34:34,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1900/global_step1900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-02-12 09:34:35,706] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1900/global_step1900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-12 09:34:36,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1900/global_step1900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-12 09:34:36,317] [INFO] [engine.py:3393:_save_zero_checkpoint] zero checkpoint saved model/LDCC-SOLAR-10.7B-sft-qlora-v3/tmp-checkpoint-1900/global_step1900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-12 09:34:36,383] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1900 is ready now!
[INFO|trainer.py:2979] 2024-02-12 09:34:36,391 >> Deleting older checkpoint [model/LDCC-SOLAR-10.7B-sft-qlora-v3/checkpoint-1800] due to args.save_total_limit
/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

 96%|█████████▌| 1901/1976 [28:58:24<1:25:07, 68.10s/it]
 96%|█████████▋| 1902/1976 [28:59:18<1:18:57, 64.02s/it]
 96%|█████████▋| 1903/1976 [29:00:12<1:14:15, 61.03s/it]
 96%|█████████▋| 1904/1976 [29:01:07<1:10:55, 59.11s/it]
 96%|█████████▋| 1905/1976 [29:02:01<1:08:14, 57.67s/it]
                                                        
{'loss': 0.087, 'learning_rate': 7.858746456167243e-07, 'epoch': 0.96}

 96%|█████████▋| 1905/1976 [29:02:01<1:08:14, 57.67s/it]
 96%|█████████▋| 1906/1976 [29:02:56<1:06:14, 56.78s/it]
 97%|█████████▋| 1907/1976 [29:03:51<1:04:35, 56.16s/it]
 97%|█████████▋| 1908/1976 [29:04:45<1:03:09, 55.73s/it]
 97%|█████████▋| 1909/1976 [29:05:40<1:01:56, 55.46s/it]
 97%|█████████▋| 1910/1976 [29:06:35<1:00:44, 55.22s/it]
                                                        
{'loss': 0.0862, 'learning_rate': 6.792065542132742e-07, 'epoch': 0.97}

 97%|█████████▋| 1910/1976 [29:06:35<1:00:44, 55.22s/it]
 97%|█████████▋| 1911/1976 [29:07:29<59:32, 54.96s/it]  
 97%|█████████▋| 1912/1976 [29:08:23<58:25, 54.77s/it]
 97%|█████████▋| 1913/1976 [29:09:18<57:21, 54.63s/it]
 97%|█████████▋| 1914/1976 [29:10:12<56:22, 54.56s/it]
 97%|█████████▋| 1915/1976 [29:11:07<55:25, 54.51s/it]
                                                      
{'loss': 0.0879, 'learning_rate': 5.802904598174119e-07, 'epoch': 0.97}

 97%|█████████▋| 1915/1976 [29:11:07<55:25, 54.51s/it]
 97%|█████████▋| 1916/1976 [29:12:02<54:39, 54.66s/it]
 97%|█████████▋| 1917/1976 [29:12:56<53:45, 54.68s/it]
 97%|█████████▋| 1918/1976 [29:13:51<52:51, 54.68s/it]
 97%|█████████▋| 1919/1976 [29:14:46<51:56, 54.68s/it]
 97%|█████████▋| 1920/1976 [29:15:40<51:02, 54.69s/it]
                                                      
{'loss': 0.0885, 'learning_rate': 4.891340828393487e-07, 'epoch': 0.97}

 97%|█████████▋| 1920/1976 [29:15:40<51:02, 54.69s/it]
 97%|█████████▋| 1921/1976 [29:16:35<50:01, 54.57s/it]
 97%|█████████▋| 1922/1976 [29:17:29<49:03, 54.50s/it]
 97%|█████████▋| 1923/1976 [29:18:24<48:10, 54.53s/it]
 97%|█████████▋| 1924/1976 [29:19:18<47:11, 54.45s/it]
 97%|█████████▋| 1925/1976 [29:20:12<46:15, 54.42s/it]
                                                      
{'loss': 0.0865, 'learning_rate': 4.0574453804262945e-07, 'epoch': 0.97}

 97%|█████████▋| 1925/1976 [29:20:12<46:15, 54.42s/it]
 97%|█████████▋| 1926/1976 [29:21:07<45:24, 54.48s/it]
 98%|█████████▊| 1927/1976 [29:22:02<44:31, 54.53s/it]
 98%|█████████▊| 1928/1976 [29:22:56<43:37, 54.53s/it]
 98%|█████████▊| 1929/1976 [29:23:51<42:48, 54.64s/it]
 98%|█████████▊| 1930/1976 [29:24:45<41:51, 54.60s/it]
                                                      
{'loss': 0.0865, 'learning_rate': 3.301283339888661e-07, 'epoch': 0.98}

 98%|█████████▊| 1930/1976 [29:24:45<41:51, 54.60s/it]
 98%|█████████▊| 1931/1976 [29:25:40<40:52, 54.51s/it]
 98%|█████████▊| 1932/1976 [29:26:34<39:54, 54.41s/it]
 98%|█████████▊| 1933/1976 [29:27:28<38:57, 54.36s/it]
 98%|█████████▊| 1934/1976 [29:28:22<38:01, 54.33s/it]
 98%|█████████▊| 1935/1976 [29:29:17<37:05, 54.28s/it]
                                                      
{'loss': 0.087, 'learning_rate': 2.622913725296772e-07, 'epoch': 0.98}

 98%|█████████▊| 1935/1976 [29:29:17<37:05, 54.28s/it]
 98%|█████████▊| 1936/1976 [29:30:11<36:16, 54.42s/it]
 98%|█████████▊| 1937/1976 [29:31:06<35:23, 54.44s/it]
 98%|█████████▊| 1938/1976 [29:32:00<34:29, 54.46s/it]
 98%|█████████▊| 1939/1976 [29:32:55<33:35, 54.49s/it]
 98%|█████████▊| 1940/1976 [29:33:49<32:41, 54.49s/it]
                                                      
{'loss': 0.0885, 'learning_rate': 2.02238948346134e-07, 'epoch': 0.98}

 98%|█████████▊| 1940/1976 [29:33:49<32:41, 54.49s/it]
 98%|█████████▊| 1941/1976 [29:34:44<31:43, 54.39s/it]
 98%|█████████▊| 1942/1976 [29:35:38<30:47, 54.35s/it]
 98%|█████████▊| 1943/1976 [29:36:32<29:54, 54.37s/it]
 98%|█████████▊| 1944/1976 [29:37:26<28:57, 54.30s/it]
 98%|█████████▊| 1945/1976 [29:38:21<28:02, 54.28s/it]
                                                      
{'loss': 0.0865, 'learning_rate': 1.4997574853541363e-07, 'epoch': 0.98}

 98%|█████████▊| 1945/1976 [29:38:21<28:02, 54.28s/it]
 98%|█████████▊| 1946/1976 [29:39:15<27:10, 54.34s/it]
 99%|█████████▊| 1947/1976 [29:40:10<26:17, 54.41s/it]
 99%|█████████▊| 1948/1976 [29:41:04<25:24, 54.43s/it]
 99%|█████████▊| 1949/1976 [29:41:59<24:31, 54.49s/it]
 99%|█████████▊| 1950/1976 [29:42:54<23:39, 54.59s/it]
                                                      
{'loss': 0.0865, 'learning_rate': 1.0550585224502474e-07, 'epoch': 0.99}

 99%|█████████▊| 1950/1976 [29:42:54<23:39, 54.59s/it]
 99%|█████████▊| 1951/1976 [29:43:48<22:41, 54.45s/it]
 99%|█████████▉| 1952/1976 [29:44:42<21:45, 54.40s/it]
 99%|█████████▉| 1953/1976 [29:45:36<20:49, 54.34s/it]
 99%|█████████▉| 1954/1976 [29:46:30<19:55, 54.32s/it]
 99%|█████████▉| 1955/1976 [29:47:25<19:00, 54.29s/it]
                                                      
{'loss': 0.0878, 'learning_rate': 6.883273035447335e-08, 'epoch': 0.99}

 99%|█████████▉| 1955/1976 [29:47:25<19:00, 54.29s/it]
 99%|█████████▉| 1956/1976 [29:48:19<18:08, 54.41s/it]
 99%|█████████▉| 1957/1976 [29:49:14<17:16, 54.54s/it]
 99%|█████████▉| 1958/1976 [29:50:09<16:22, 54.57s/it]
 99%|█████████▉| 1959/1976 [29:51:04<15:28, 54.60s/it]
 99%|█████████▉| 1960/1976 [29:51:58<14:33, 54.59s/it]
                                                      
{'loss': 0.0869, 'learning_rate': 3.995924520424632e-08, 'epoch': 0.99}

 99%|█████████▉| 1960/1976 [29:51:58<14:33, 54.59s/it]
 99%|█████████▉| 1961/1976 [29:52:52<13:37, 54.51s/it]
 99%|█████████▉| 1962/1976 [29:53:47<12:41, 54.41s/it]
 99%|█████████▉| 1963/1976 [29:54:41<11:46, 54.36s/it]
 99%|█████████▉| 1964/1976 [29:55:35<10:52, 54.36s/it]
 99%|█████████▉| 1965/1976 [29:56:29<09:57, 54.33s/it]
                                                      
{'loss': 0.0874, 'learning_rate': 1.8887650372523268e-08, 'epoch': 0.99}

 99%|█████████▉| 1965/1976 [29:56:29<09:57, 54.33s/it]
 99%|█████████▉| 1966/1976 [29:57:24<09:03, 54.40s/it]
100%|█████████▉| 1967/1976 [29:58:18<08:09, 54.42s/it]
100%|█████████▉| 1968/1976 [29:59:13<07:15, 54.45s/it]
100%|█████████▉| 1969/1976 [30:00:07<06:21, 54.46s/it]
100%|█████████▉| 1970/1976 [30:01:02<05:27, 54.50s/it]
                                                      
{'loss': 0.0883, 'learning_rate': 5.619590499184035e-09, 'epoch': 1.0}

100%|█████████▉| 1970/1976 [30:01:02<05:27, 54.50s/it]
100%|█████████▉| 1971/1976 [30:01:57<04:32, 54.50s/it]
100%|█████████▉| 1972/1976 [30:02:51<03:37, 54.39s/it]
100%|█████████▉| 1973/1976 [30:03:45<02:42, 54.33s/it]
100%|█████████▉| 1974/1976 [30:04:39<01:48, 54.29s/it]
100%|█████████▉| 1975/1976 [30:05:33<00:54, 54.28s/it]
                                                      
{'loss': 0.0864, 'learning_rate': 1.5610115752240361e-10, 'epoch': 1.0}

100%|█████████▉| 1975/1976 [30:05:33<00:54, 54.28s/it]
100%|██████████| 1976/1976 [30:06:28<00:00, 54.38s/it][INFO|trainer.py:3166] 2024-02-12 10:43:37,293 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-02-12 10:43:37,293 >>   Num examples = 13296
[INFO|trainer.py:3171] 2024-02-12 10:43:37,293 >>   Batch size = 16


  0%|          | 0/416 [00:00<?, ?it/s][A

  0%|          | 2/416 [00:09<34:05,  4.94s/it][A

  1%|          | 3/416 [00:18<44:11,  6.42s/it][A

  1%|          | 4/416 [00:26<49:04,  7.15s/it][A

  1%|          | 5/416 [00:35<52:06,  7.61s/it][A

  1%|▏         | 6/416 [00:43<53:43,  7.86s/it][A

  2%|▏         | 7/416 [00:52<54:54,  8.05s/it][A

  2%|▏         | 8/416 [01:00<55:25,  8.15s/it][A

  2%|▏         | 9/416 [01:08<55:59,  8.25s/it][A

  2%|▏         | 10/416 [01:17<56:05,  8.29s/it][A

  3%|▎         | 11/416 [01:25<56:18,  8.34s/it][A

  3%|▎         | 12/416 [01:34<56:11,  8.35s/it][A

  3%|▎         | 13/416 [01:42<56:17,  8.38s/it][A

  3%|▎         | 14/416 [01:50<56:09,  8.38s/it][A

  4%|▎         | 15/416 [01:59<56:12,  8.41s/it][A

  4%|▍         | 16/416 [02:07<56:00,  8.40s/it][A

  4%|▍         | 17/416 [02:16<56:00,  8.42s/it][A

  4%|▍         | 18/416 [02:24<55:44,  8.40s/it][A

  5%|▍         | 19/416 [02:33<55:43,  8.42s/it][A

  5%|▍         | 20/416 [02:41<55:29,  8.41s/it][A

  5%|▌         | 21/416 [02:49<55:29,  8.43s/it][A

  5%|▌         | 22/416 [02:58<55:13,  8.41s/it][A

  6%|▌         | 23/416 [03:06<55:12,  8.43s/it][A

  6%|▌         | 24/416 [03:15<54:55,  8.41s/it][A

  6%|▌         | 25/416 [03:23<54:53,  8.42s/it][A

  6%|▋         | 26/416 [03:31<54:39,  8.41s/it][A

  6%|▋         | 27/416 [03:40<54:36,  8.42s/it][A

  7%|▋         | 28/416 [03:48<54:20,  8.40s/it][A

  7%|▋         | 29/416 [03:57<54:22,  8.43s/it][A

  7%|▋         | 30/416 [04:05<54:07,  8.41s/it][A

  7%|▋         | 31/416 [04:14<54:05,  8.43s/it][A

  8%|▊         | 32/416 [04:22<53:50,  8.41s/it][A

  8%|▊         | 33/416 [04:30<53:48,  8.43s/it][A

  8%|▊         | 34/416 [04:39<53:35,  8.42s/it][A

  8%|▊         | 35/416 [04:47<53:33,  8.43s/it][A

  9%|▊         | 36/416 [04:56<53:18,  8.42s/it][A

  9%|▉         | 37/416 [05:04<53:17,  8.44s/it][A

  9%|▉         | 38/416 [05:13<53:01,  8.42s/it][A

  9%|▉         | 39/416 [05:21<53:00,  8.44s/it][A

 10%|▉         | 40/416 [05:29<52:44,  8.42s/it][A

 10%|▉         | 41/416 [05:38<52:42,  8.43s/it][A

 10%|█         | 42/416 [05:46<52:26,  8.41s/it][A

 10%|█         | 43/416 [05:55<52:24,  8.43s/it][A

 11%|█         | 44/416 [06:03<52:11,  8.42s/it][A

 11%|█         | 45/416 [06:12<52:13,  8.45s/it][A

 11%|█         | 46/416 [06:20<51:56,  8.42s/it][A

 11%|█▏        | 47/416 [06:28<51:54,  8.44s/it][A

 12%|█▏        | 48/416 [06:37<51:39,  8.42s/it][A

 12%|█▏        | 49/416 [06:45<51:36,  8.44s/it][A

 12%|█▏        | 50/416 [06:54<51:21,  8.42s/it][A

 12%|█▏        | 51/416 [07:02<51:19,  8.44s/it][A

 12%|█▎        | 52/416 [07:11<51:03,  8.42s/it][A

 13%|█▎        | 53/416 [07:19<50:58,  8.43s/it][A

 13%|█▎        | 54/416 [07:27<50:44,  8.41s/it][A

 13%|█▎        | 55/416 [07:36<50:45,  8.44s/it][A

 13%|█▎        | 56/416 [07:44<50:32,  8.42s/it][A

 14%|█▎        | 57/416 [07:53<50:32,  8.45s/it][A

 14%|█▍        | 58/416 [08:01<50:15,  8.42s/it][A

 14%|█▍        | 59/416 [08:10<50:16,  8.45s/it][A

 14%|█▍        | 60/416 [08:18<49:57,  8.42s/it][A

 15%|█▍        | 61/416 [08:26<49:54,  8.43s/it][A

 15%|█▍        | 62/416 [08:35<49:38,  8.41s/it][A

 15%|█▌        | 63/416 [08:43<49:37,  8.43s/it][A

 15%|█▌        | 64/416 [08:52<49:22,  8.42s/it][A

 16%|█▌        | 65/416 [09:00<49:18,  8.43s/it][A

 16%|█▌        | 66/416 [09:08<49:05,  8.42s/it][A

 16%|█▌        | 67/416 [09:17<49:03,  8.43s/it][A

 16%|█▋        | 68/416 [09:25<48:49,  8.42s/it][A

 17%|█▋        | 69/416 [09:34<48:48,  8.44s/it][A

 17%|█▋        | 70/416 [09:42<48:33,  8.42s/it][A

 17%|█▋        | 71/416 [09:51<48:30,  8.44s/it][A

 17%|█▋        | 72/416 [09:59<48:15,  8.42s/it][A

 18%|█▊        | 73/416 [10:08<48:12,  8.43s/it][A

 18%|█▊        | 74/416 [10:16<47:58,  8.42s/it][A

 18%|█▊        | 75/416 [10:24<47:54,  8.43s/it][A

 18%|█▊        | 76/416 [10:33<47:43,  8.42s/it][A

 19%|█▊        | 77/416 [10:41<47:41,  8.44s/it][A

 19%|█▉        | 78/416 [10:50<47:28,  8.43s/it][A

 19%|█▉        | 79/416 [10:58<47:25,  8.44s/it][A

 19%|█▉        | 80/416 [11:07<47:10,  8.43s/it][A

 19%|█▉        | 81/416 [11:15<47:06,  8.44s/it][A

 20%|█▉        | 82/416 [11:23<46:51,  8.42s/it][A

 20%|█▉        | 83/416 [11:32<46:48,  8.43s/it][A

 20%|██        | 84/416 [11:40<46:33,  8.41s/it][A

 20%|██        | 85/416 [11:49<46:29,  8.43s/it][A

 21%|██        | 86/416 [11:57<46:15,  8.41s/it][A

 21%|██        | 87/416 [12:06<46:15,  8.44s/it][A

 21%|██        | 88/416 [12:14<46:03,  8.43s/it][A

 21%|██▏       | 89/416 [12:22<45:59,  8.44s/it][A

 22%|██▏       | 90/416 [12:31<45:44,  8.42s/it][A

 22%|██▏       | 91/416 [12:39<45:42,  8.44s/it][A

 22%|██▏       | 92/416 [12:48<45:30,  8.43s/it][A

 22%|██▏       | 93/416 [12:56<45:28,  8.45s/it][A

 23%|██▎       | 94/416 [13:05<45:12,  8.42s/it][A

 23%|██▎       | 95/416 [13:13<45:08,  8.44s/it][A

 23%|██▎       | 96/416 [13:21<44:53,  8.42s/it][A

 23%|██▎       | 97/416 [13:30<44:50,  8.43s/it][A

 24%|██▎       | 98/416 [13:38<44:36,  8.42s/it][A

 24%|██▍       | 99/416 [13:47<44:32,  8.43s/it][A

 24%|██▍       | 100/416 [13:55<44:21,  8.42s/it][A

 24%|██▍       | 101/416 [14:04<44:20,  8.45s/it][A

 25%|██▍       | 102/416 [14:12<44:04,  8.42s/it][A

 25%|██▍       | 103/416 [14:20<44:02,  8.44s/it][A

 25%|██▌       | 104/416 [14:29<43:46,  8.42s/it][A

 25%|██▌       | 105/416 [14:37<43:43,  8.44s/it][A

 25%|██▌       | 106/416 [14:46<43:28,  8.41s/it][A

 26%|██▌       | 107/416 [14:54<43:24,  8.43s/it][A

 26%|██▌       | 108/416 [15:02<43:11,  8.42s/it][A

 26%|██▌       | 109/416 [15:11<43:09,  8.44s/it][A

 26%|██▋       | 110/416 [15:19<42:56,  8.42s/it][A

 27%|██▋       | 111/416 [15:28<42:55,  8.44s/it][A

 27%|██▋       | 112/416 [15:36<42:40,  8.42s/it][A

 27%|██▋       | 113/416 [15:45<42:36,  8.44s/it][A

 27%|██▋       | 114/416 [15:53<42:21,  8.41s/it][A

 28%|██▊       | 115/416 [16:02<42:18,  8.43s/it][A

 28%|██▊       | 116/416 [16:10<42:04,  8.41s/it][A

 28%|██▊       | 117/416 [16:18<42:00,  8.43s/it][A

 28%|██▊       | 118/416 [16:27<41:48,  8.42s/it][A

 29%|██▊       | 119/416 [16:35<41:45,  8.43s/it][A

 29%|██▉       | 120/416 [16:44<41:31,  8.42s/it][A

 29%|██▉       | 121/416 [16:52<41:28,  8.44s/it][A

 29%|██▉       | 122/416 [17:00<41:17,  8.43s/it][A

 30%|██▉       | 123/416 [17:09<41:14,  8.45s/it][A

 30%|██▉       | 124/416 [17:17<40:59,  8.42s/it][A

 30%|███       | 125/416 [17:26<40:56,  8.44s/it][A

 30%|███       | 126/416 [17:34<40:43,  8.43s/it][A

 31%|███       | 127/416 [17:43<40:38,  8.44s/it][A

 31%|███       | 128/416 [17:51<40:23,  8.41s/it][A

 31%|███       | 129/416 [18:00<40:21,  8.44s/it][A

 31%|███▏      | 130/416 [18:08<40:06,  8.41s/it][A

 31%|███▏      | 131/416 [18:16<40:01,  8.43s/it][A

 32%|███▏      | 132/416 [18:25<39:49,  8.41s/it][A

 32%|███▏      | 133/416 [18:33<39:46,  8.43s/it][A

 32%|███▏      | 134/416 [18:42<39:33,  8.41s/it][A

 32%|███▏      | 135/416 [18:50<39:30,  8.44s/it][A

 33%|███▎      | 136/416 [18:58<39:17,  8.42s/it][A

 33%|███▎      | 137/416 [19:07<39:13,  8.43s/it][A

 33%|███▎      | 138/416 [19:15<38:59,  8.42s/it][A

 33%|███▎      | 139/416 [19:24<38:58,  8.44s/it][A

 34%|███▎      | 140/416 [19:32<38:44,  8.42s/it][A

 34%|███▍      | 141/416 [19:41<38:42,  8.44s/it][A

 34%|███▍      | 142/416 [19:49<38:30,  8.43s/it][A

 34%|███▍      | 143/416 [19:58<38:28,  8.46s/it][A

 35%|███▍      | 144/416 [20:06<38:13,  8.43s/it][A

 35%|███▍      | 145/416 [20:14<38:08,  8.44s/it][A

 35%|███▌      | 146/416 [20:23<37:53,  8.42s/it][A

 35%|███▌      | 147/416 [20:31<37:48,  8.43s/it][A

 36%|███▌      | 148/416 [20:40<37:34,  8.41s/it][A

 36%|███▌      | 149/416 [20:48<37:31,  8.43s/it][A

 36%|███▌      | 150/416 [20:56<37:19,  8.42s/it][A

 36%|███▋      | 151/416 [21:05<37:13,  8.43s/it][A

 37%|███▋      | 152/416 [21:13<37:01,  8.41s/it][A

 37%|███▋      | 153/416 [21:22<36:57,  8.43s/it][A

 37%|███▋      | 154/416 [21:30<36:44,  8.42s/it][A

 37%|███▋      | 155/416 [21:39<36:40,  8.43s/it][A

 38%|███▊      | 156/416 [21:47<36:28,  8.42s/it][A

 38%|███▊      | 157/416 [21:56<36:24,  8.43s/it][A

 38%|███▊      | 158/416 [22:04<36:10,  8.41s/it][A

 38%|███▊      | 159/416 [22:12<36:06,  8.43s/it][A

 38%|███▊      | 160/416 [22:21<35:52,  8.41s/it][A

 39%|███▊      | 161/416 [22:29<35:49,  8.43s/it][A

 39%|███▉      | 162/416 [22:38<35:36,  8.41s/it][A

 39%|███▉      | 163/416 [22:46<35:34,  8.44s/it][A

 39%|███▉      | 164/416 [22:54<35:20,  8.42s/it][A

 40%|███▉      | 165/416 [23:03<35:17,  8.44s/it][A

 40%|███▉      | 166/416 [23:11<35:04,  8.42s/it][A

 40%|████      | 167/416 [23:20<35:00,  8.43s/it][A

 40%|████      | 168/416 [23:28<34:46,  8.41s/it][A

 41%|████      | 169/416 [23:37<34:41,  8.43s/it][A

 41%|████      | 170/416 [23:45<34:30,  8.42s/it][A

 41%|████      | 171/416 [23:53<34:25,  8.43s/it][A

 41%|████▏     | 172/416 [24:02<34:15,  8.42s/it][A

 42%|████▏     | 173/416 [24:10<34:11,  8.44s/it][A

 42%|████▏     | 174/416 [24:19<34:01,  8.43s/it][A

 42%|████▏     | 175/416 [24:27<33:55,  8.45s/it][A

 42%|████▏     | 176/416 [24:36<33:42,  8.43s/it][A

 43%|████▎     | 177/416 [24:44<33:36,  8.44s/it][A

 43%|████▎     | 178/416 [24:52<33:24,  8.42s/it][A

 43%|████▎     | 179/416 [25:01<33:18,  8.43s/it][A

 43%|████▎     | 180/416 [25:09<33:06,  8.42s/it][A

 44%|████▎     | 181/416 [25:18<33:01,  8.43s/it][A

 44%|████▍     | 182/416 [25:26<32:50,  8.42s/it][A

 44%|████▍     | 183/416 [25:35<32:43,  8.43s/it][A

 44%|████▍     | 184/416 [25:43<32:33,  8.42s/it][A

 44%|████▍     | 185/416 [25:51<32:30,  8.44s/it][A

 45%|████▍     | 186/416 [26:00<32:17,  8.42s/it][A

 45%|████▍     | 187/416 [26:08<32:12,  8.44s/it][A

 45%|████▌     | 188/416 [26:17<32:00,  8.43s/it][A

 45%|████▌     | 189/416 [26:25<31:55,  8.44s/it][A

 46%|████▌     | 190/416 [26:34<31:43,  8.42s/it][A

 46%|████▌     | 191/416 [26:42<31:38,  8.44s/it][A

 46%|████▌     | 192/416 [26:50<31:25,  8.42s/it][A

 46%|████▋     | 193/416 [26:59<31:20,  8.43s/it][A

 47%|████▋     | 194/416 [27:07<31:08,  8.42s/it][A

 47%|████▋     | 195/416 [27:16<31:04,  8.44s/it][A

 47%|████▋     | 196/416 [27:24<30:52,  8.42s/it][A

 47%|████▋     | 197/416 [27:33<30:47,  8.44s/it][A

 48%|████▊     | 198/416 [27:41<30:34,  8.42s/it][A

 48%|████▊     | 199/416 [27:49<30:29,  8.43s/it][A

 48%|████▊     | 200/416 [27:58<30:16,  8.41s/it][A

 48%|████▊     | 201/416 [28:06<30:10,  8.42s/it][A

 49%|████▊     | 202/416 [28:15<29:59,  8.41s/it][A

 49%|████▉     | 203/416 [28:23<29:55,  8.43s/it][A

 49%|████▉     | 204/416 [28:32<29:45,  8.42s/it][A

 49%|████▉     | 205/416 [28:40<29:43,  8.45s/it][A

 50%|████▉     | 206/416 [28:48<29:30,  8.43s/it][A

 50%|████▉     | 207/416 [28:57<29:24,  8.44s/it][A

 50%|█████     | 208/416 [29:05<29:11,  8.42s/it][A

 50%|█████     | 209/416 [29:14<29:05,  8.43s/it][A

 50%|█████     | 210/416 [29:22<28:53,  8.42s/it][A

 51%|█████     | 211/416 [29:31<28:49,  8.44s/it][A

 51%|█████     | 212/416 [29:39<28:37,  8.42s/it][A

 51%|█████     | 213/416 [29:47<28:34,  8.45s/it][A

 51%|█████▏    | 214/416 [29:56<28:22,  8.43s/it][A

 52%|█████▏    | 215/416 [30:04<28:15,  8.44s/it][A

 52%|█████▏    | 216/416 [30:13<28:03,  8.42s/it][A

 52%|█████▏    | 217/416 [30:21<27:59,  8.44s/it][A

 52%|█████▏    | 218/416 [30:30<27:47,  8.42s/it][A

 53%|█████▎    | 219/416 [30:38<27:42,  8.44s/it][A

 53%|█████▎    | 220/416 [30:46<27:31,  8.42s/it][A

 53%|█████▎    | 221/416 [30:55<27:26,  8.45s/it][A

 53%|█████▎    | 222/416 [31:03<27:14,  8.42s/it][A

 54%|█████▎    | 223/416 [31:12<27:09,  8.44s/it][A

 54%|█████▍    | 224/416 [31:20<26:57,  8.42s/it][A

 54%|█████▍    | 225/416 [31:29<26:51,  8.44s/it][A

 54%|█████▍    | 226/416 [31:37<26:39,  8.42s/it][A

 55%|█████▍    | 227/416 [31:45<26:33,  8.43s/it][A

 55%|█████▍    | 228/416 [31:54<26:22,  8.42s/it][A

 55%|█████▌    | 229/416 [32:02<26:17,  8.44s/it][A

 55%|█████▌    | 230/416 [32:11<26:05,  8.42s/it][A

 56%|█████▌    | 231/416 [32:19<26:00,  8.43s/it][A

 56%|█████▌    | 232/416 [32:28<25:48,  8.42s/it][A

 56%|█████▌    | 233/416 [32:36<25:43,  8.44s/it][A

 56%|█████▋    | 234/416 [32:44<25:32,  8.42s/it][A

 56%|█████▋    | 235/416 [32:53<25:26,  8.43s/it][A

 57%|█████▋    | 236/416 [33:01<25:16,  8.43s/it][A

 57%|█████▋    | 237/416 [33:10<25:11,  8.44s/it][A

 57%|█████▋    | 238/416 [33:18<24:59,  8.42s/it][A

 57%|█████▋    | 239/416 [33:27<24:53,  8.44s/it][A

 58%|█████▊    | 240/416 [33:35<24:41,  8.42s/it][A

 58%|█████▊    | 241/416 [33:43<24:35,  8.43s/it][A

 58%|█████▊    | 242/416 [33:52<24:24,  8.42s/it][A

 58%|█████▊    | 243/416 [34:00<24:19,  8.44s/it][A

 59%|█████▊    | 244/416 [34:09<24:10,  8.43s/it][A

 59%|█████▉    | 245/416 [34:17<24:03,  8.44s/it][A

 59%|█████▉    | 246/416 [34:26<23:51,  8.42s/it][A

 59%|█████▉    | 247/416 [34:34<23:47,  8.44s/it][A

 60%|█████▉    | 248/416 [34:42<23:34,  8.42s/it][A

 60%|█████▉    | 249/416 [34:51<23:28,  8.44s/it][A

 60%|██████    | 250/416 [34:59<23:17,  8.42s/it][A

 60%|██████    | 251/416 [35:08<23:13,  8.44s/it][A

 61%|██████    | 252/416 [35:16<23:01,  8.42s/it][A

 61%|██████    | 253/416 [35:25<22:57,  8.45s/it][A

 61%|██████    | 254/416 [35:33<22:46,  8.44s/it][A

 61%|██████▏   | 255/416 [35:42<22:40,  8.45s/it][A

 62%|██████▏   | 256/416 [35:50<22:30,  8.44s/it][A

 62%|██████▏   | 257/416 [35:58<22:23,  8.45s/it][A

 62%|██████▏   | 258/416 [36:07<22:11,  8.43s/it][A

 62%|██████▏   | 259/416 [36:15<22:05,  8.44s/it][A

 62%|██████▎   | 260/416 [36:24<21:53,  8.42s/it][A

 63%|██████▎   | 261/416 [36:32<21:47,  8.44s/it][A

 63%|██████▎   | 262/416 [36:41<21:36,  8.42s/it][A

 63%|██████▎   | 263/416 [36:49<21:30,  8.43s/it][A

 63%|██████▎   | 264/416 [36:57<21:19,  8.42s/it][A

 64%|██████▎   | 265/416 [37:06<21:14,  8.44s/it][A

 64%|██████▍   | 266/416 [37:14<21:04,  8.43s/it][A

 64%|██████▍   | 267/416 [37:23<20:59,  8.45s/it][A

 64%|██████▍   | 268/416 [37:31<20:48,  8.43s/it][A

 65%|██████▍   | 269/416 [37:40<20:41,  8.45s/it][A

 65%|██████▍   | 270/416 [37:48<20:30,  8.43s/it][A

 65%|██████▌   | 271/416 [37:57<20:24,  8.44s/it][A

 65%|██████▌   | 272/416 [38:05<20:12,  8.42s/it][A

 66%|██████▌   | 273/416 [38:13<20:05,  8.43s/it][A

 66%|██████▌   | 274/416 [38:22<19:54,  8.41s/it][A

 66%|██████▌   | 275/416 [38:30<19:49,  8.43s/it][A

 66%|██████▋   | 276/416 [38:39<19:39,  8.42s/it][A

 67%|██████▋   | 277/416 [38:47<19:33,  8.44s/it][A

 67%|██████▋   | 278/416 [38:55<19:22,  8.43s/it][A

 67%|██████▋   | 279/416 [39:04<19:16,  8.44s/it][A

 67%|██████▋   | 280/416 [39:12<19:04,  8.42s/it][A

 68%|██████▊   | 281/416 [39:21<18:58,  8.44s/it][A

 68%|██████▊   | 282/416 [39:29<18:47,  8.41s/it][A

 68%|██████▊   | 283/416 [39:38<18:40,  8.43s/it][A

 68%|██████▊   | 284/416 [39:46<18:31,  8.42s/it][A

 69%|██████▊   | 285/416 [39:54<18:25,  8.44s/it][A

 69%|██████▉   | 286/416 [40:03<18:15,  8.43s/it][A

 69%|██████▉   | 287/416 [40:11<18:08,  8.44s/it][A

 69%|██████▉   | 288/416 [40:20<17:57,  8.42s/it][A

 69%|██████▉   | 289/416 [40:28<17:51,  8.44s/it][A

 70%|██████▉   | 290/416 [40:37<17:40,  8.42s/it][A

 70%|██████▉   | 291/416 [40:45<17:34,  8.43s/it][A

 70%|███████   | 292/416 [40:53<17:23,  8.42s/it][A

 70%|███████   | 293/416 [41:02<17:17,  8.44s/it][A

 71%|███████   | 294/416 [41:10<17:07,  8.42s/it][A

 71%|███████   | 295/416 [41:19<17:00,  8.44s/it][A

 71%|███████   | 296/416 [41:27<16:49,  8.42s/it][A

 71%|███████▏  | 297/416 [41:36<16:44,  8.44s/it][A

 72%|███████▏  | 298/416 [41:44<16:34,  8.43s/it][A

 72%|███████▏  | 299/416 [41:53<16:29,  8.46s/it][A

 72%|███████▏  | 300/416 [42:01<16:17,  8.43s/it][A

 72%|███████▏  | 301/416 [42:09<16:10,  8.44s/it][A

 73%|███████▎  | 302/416 [42:18<16:00,  8.42s/it][A

 73%|███████▎  | 303/416 [42:26<15:53,  8.44s/it][A

 73%|███████▎  | 304/416 [42:35<15:43,  8.42s/it][A

 73%|███████▎  | 305/416 [42:43<15:36,  8.44s/it][A

 74%|███████▎  | 306/416 [42:51<15:26,  8.42s/it][A

 74%|███████▍  | 307/416 [43:00<15:19,  8.44s/it][A

 74%|███████▍  | 308/416 [43:08<15:09,  8.42s/it][A

 74%|███████▍  | 309/416 [43:17<15:03,  8.44s/it][A

 75%|███████▍  | 310/416 [43:25<14:53,  8.42s/it][A

 75%|███████▍  | 311/416 [43:34<14:45,  8.43s/it][A

 75%|███████▌  | 312/416 [43:42<14:35,  8.42s/it][A

 75%|███████▌  | 313/416 [43:51<14:30,  8.45s/it][A

 75%|███████▌  | 314/416 [43:59<14:20,  8.43s/it][A

 76%|███████▌  | 315/416 [44:07<14:13,  8.45s/it][A

 76%|███████▌  | 316/416 [44:16<14:03,  8.43s/it][A

 76%|███████▌  | 317/416 [44:24<13:56,  8.45s/it][A

 76%|███████▋  | 318/416 [44:33<13:46,  8.43s/it][A

 77%|███████▋  | 319/416 [44:41<13:39,  8.45s/it][A

 77%|███████▋  | 320/416 [44:50<13:29,  8.43s/it][A

 77%|███████▋  | 321/416 [44:58<13:22,  8.45s/it][A

 77%|███████▋  | 322/416 [45:06<13:11,  8.42s/it][A

 78%|███████▊  | 323/416 [45:15<13:04,  8.44s/it][A

 78%|███████▊  | 324/416 [45:23<12:54,  8.42s/it][A

 78%|███████▊  | 325/416 [45:32<12:47,  8.43s/it][A

 78%|███████▊  | 326/416 [45:40<12:37,  8.42s/it][A

 79%|███████▊  | 327/416 [45:49<12:30,  8.43s/it][A

 79%|███████▉  | 328/416 [45:57<12:20,  8.42s/it][A

 79%|███████▉  | 329/416 [46:06<12:14,  8.45s/it][A

 79%|███████▉  | 330/416 [46:14<12:05,  8.44s/it][A

 80%|███████▉  | 331/416 [46:22<11:58,  8.45s/it][A

 80%|███████▉  | 332/416 [46:31<11:47,  8.43s/it][A

 80%|████████  | 333/416 [46:39<11:40,  8.44s/it][A

 80%|████████  | 334/416 [46:48<11:30,  8.42s/it][A

 81%|████████  | 335/416 [46:56<11:22,  8.43s/it][A

 81%|████████  | 336/416 [47:04<11:12,  8.41s/it][A

 81%|████████  | 337/416 [47:13<11:05,  8.43s/it][A

 81%|████████▏ | 338/416 [47:21<10:56,  8.41s/it][A

 81%|████████▏ | 339/416 [47:30<10:49,  8.43s/it][A

 82%|████████▏ | 340/416 [47:38<10:39,  8.42s/it][A

 82%|████████▏ | 341/416 [47:47<10:32,  8.44s/it][A

 82%|████████▏ | 342/416 [47:55<10:22,  8.42s/it][A

 82%|████████▏ | 343/416 [48:03<10:15,  8.43s/it][A

 83%|████████▎ | 344/416 [48:12<10:05,  8.42s/it][A

 83%|████████▎ | 345/416 [48:20<09:58,  8.43s/it][A

 83%|████████▎ | 346/416 [48:29<09:48,  8.41s/it][A

 83%|████████▎ | 347/416 [48:37<09:41,  8.43s/it][A

 84%|████████▎ | 348/416 [48:46<09:32,  8.42s/it][A

 84%|████████▍ | 349/416 [48:54<09:25,  8.43s/it][A

 84%|████████▍ | 350/416 [49:02<09:15,  8.41s/it][A

 84%|████████▍ | 351/416 [49:11<09:08,  8.44s/it][A

 85%|████████▍ | 352/416 [49:19<08:58,  8.42s/it][A

 85%|████████▍ | 353/416 [49:28<08:51,  8.43s/it][A

 85%|████████▌ | 354/416 [49:36<08:41,  8.42s/it][A

 85%|████████▌ | 355/416 [49:45<08:34,  8.43s/it][A

 86%|████████▌ | 356/416 [49:53<08:24,  8.41s/it][A

 86%|████████▌ | 357/416 [50:01<08:17,  8.43s/it][A

 86%|████████▌ | 358/416 [50:10<08:07,  8.41s/it][A

 86%|████████▋ | 359/416 [50:18<08:00,  8.43s/it][A

 87%|████████▋ | 360/416 [50:27<07:51,  8.42s/it][A

 87%|████████▋ | 361/416 [50:35<07:44,  8.44s/it][A

 87%|████████▋ | 362/416 [50:44<07:35,  8.43s/it][A

 87%|████████▋ | 363/416 [50:52<07:27,  8.45s/it][A

 88%|████████▊ | 364/416 [51:00<07:18,  8.42s/it][A

 88%|████████▊ | 365/416 [51:09<07:10,  8.44s/it][A

 88%|████████▊ | 366/416 [51:17<07:01,  8.42s/it][A

 88%|████████▊ | 367/416 [51:26<06:53,  8.44s/it][A

 88%|████████▊ | 368/416 [51:34<06:44,  8.42s/it][A

 89%|████████▊ | 369/416 [51:43<06:36,  8.44s/it][A

 89%|████████▉ | 370/416 [51:51<06:27,  8.42s/it][A

 89%|████████▉ | 371/416 [51:59<06:19,  8.44s/it][A

 89%|████████▉ | 372/416 [52:08<06:10,  8.43s/it][A

 90%|████████▉ | 373/416 [52:16<06:03,  8.44s/it][A

 90%|████████▉ | 374/416 [52:25<05:53,  8.42s/it][A

 90%|█████████ | 375/416 [52:33<05:46,  8.44s/it][A

 90%|█████████ | 376/416 [52:42<05:36,  8.42s/it][A

 91%|█████████ | 377/416 [52:50<05:28,  8.43s/it][A

 91%|█████████ | 378/416 [52:58<05:20,  8.42s/it][A

 91%|█████████ | 379/416 [53:07<05:12,  8.43s/it][A

 91%|█████████▏| 380/416 [53:15<05:03,  8.42s/it][A

 92%|█████████▏| 381/416 [53:24<04:55,  8.44s/it][A

 92%|█████████▏| 382/416 [53:32<04:46,  8.42s/it][A

 92%|█████████▏| 383/416 [53:41<04:38,  8.44s/it][A

 92%|█████████▏| 384/416 [53:49<04:29,  8.42s/it][A

 93%|█████████▎| 385/416 [53:57<04:21,  8.44s/it][A

 93%|█████████▎| 386/416 [54:06<04:12,  8.41s/it][A

 93%|█████████▎| 387/416 [54:14<04:04,  8.43s/it][A

 93%|█████████▎| 388/416 [54:23<03:55,  8.41s/it][A

 94%|█████████▎| 389/416 [54:31<03:47,  8.43s/it][A

 94%|█████████▍| 390/416 [54:39<03:38,  8.42s/it][A

 94%|█████████▍| 391/416 [54:48<03:30,  8.44s/it][A

 94%|█████████▍| 392/416 [54:56<03:21,  8.41s/it][A

 94%|█████████▍| 393/416 [55:05<03:13,  8.43s/it][A

 95%|█████████▍| 394/416 [55:13<03:05,  8.42s/it][A

 95%|█████████▍| 395/416 [55:22<02:57,  8.43s/it][A

 95%|█████████▌| 396/416 [55:30<02:48,  8.42s/it][A

 95%|█████████▌| 397/416 [55:39<02:40,  8.44s/it][A

 96%|█████████▌| 398/416 [55:47<02:31,  8.42s/it][A

 96%|█████████▌| 399/416 [55:55<02:23,  8.43s/it][A

 96%|█████████▌| 400/416 [56:04<02:14,  8.42s/it][A

 96%|█████████▋| 401/416 [56:12<02:06,  8.43s/it][A

 97%|█████████▋| 402/416 [56:21<01:57,  8.42s/it][A

 97%|█████████▋| 403/416 [56:29<01:49,  8.44s/it][A

 97%|█████████▋| 404/416 [56:38<01:41,  8.43s/it][A

 97%|█████████▋| 405/416 [56:46<01:32,  8.44s/it][A

 98%|█████████▊| 406/416 [56:54<01:24,  8.43s/it][A

 98%|█████████▊| 407/416 [57:03<01:15,  8.44s/it][A

 98%|█████████▊| 408/416 [57:11<01:07,  8.42s/it][A

 98%|█████████▊| 409/416 [57:20<00:59,  8.43s/it][A

 99%|█████████▊| 410/416 [57:28<00:50,  8.41s/it][A

 99%|█████████▉| 411/416 [57:37<00:42,  8.43s/it][A

 99%|█████████▉| 412/416 [57:45<00:33,  8.42s/it][A

 99%|█████████▉| 413/416 [57:53<00:25,  8.43s/it][A

100%|█████████▉| 414/416 [58:02<00:16,  8.42s/it][A

100%|█████████▉| 415/416 [58:10<00:08,  8.43s/it][A

100%|██████████| 416/416 [58:19<00:00,  8.41s/it][A
                                                      

                                                 
[A{'eval_loss': 0.08755173534154892, 'eval_runtime': 3507.3636, 'eval_samples_per_second': 3.791, 'eval_steps_per_second': 0.119, 'epoch': 1.0}

100%|██████████| 1976/1976 [31:04:55<00:00, 54.38s/it]

100%|██████████| 416/416 [58:19<00:00,  8.41s/it][A

                                                 [A[INFO|trainer.py:1947] 2024-02-12 11:42:04,658 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)



                                                      
{'train_runtime': 111895.8423, 'train_samples_per_second': 2.261, 'train_steps_per_second': 0.018, 'train_loss': 0.17988704664534644, 'epoch': 1.0}

100%|██████████| 1976/1976 [31:04:55<00:00, 54.38s/it]
100%|██████████| 1976/1976 [31:04:55<00:00, 56.63s/it]
***** train metrics *****
  epoch                    =               1.0
  train_loss               =            0.1799
  train_runtime            = 1 day, 7:04:55.84
  train_samples            =            386117
  train_samples_per_second =             2.261
  train_steps_per_second   =             0.018
2024-02-12 11:42:04 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3166] 2024-02-12 11:42:04,669 >> ***** Running Evaluation *****
[INFO|trainer.py:3168] 2024-02-12 11:42:04,669 >>   Num examples = 13296
[INFO|trainer.py:3171] 2024-02-12 11:42:04,669 >>   Batch size = 16

  0%|          | 0/416 [00:00<?, ?it/s]
  0%|          | 2/416 [00:08<28:51,  4.18s/it]
  1%|          | 3/416 [00:16<41:04,  5.97s/it]
  1%|          | 4/416 [00:25<47:09,  6.87s/it]
  1%|          | 5/416 [00:33<50:53,  7.43s/it]
  1%|▏         | 6/416 [00:42<52:55,  7.75s/it]
  2%|▏         | 7/416 [00:50<54:26,  7.99s/it]
  2%|▏         | 8/416 [00:58<55:11,  8.12s/it]
  2%|▏         | 9/416 [01:07<55:48,  8.23s/it]
  2%|▏         | 10/416 [01:15<55:59,  8.27s/it]
  3%|▎         | 11/416 [01:24<56:14,  8.33s/it]
  3%|▎         | 12/416 [01:32<56:11,  8.35s/it]
  3%|▎         | 13/416 [01:41<56:18,  8.38s/it]
  3%|▎         | 14/416 [01:49<56:09,  8.38s/it]
  4%|▎         | 15/416 [01:57<56:11,  8.41s/it]
  4%|▍         | 16/416 [02:06<56:02,  8.41s/it]
  4%|▍         | 17/416 [02:14<56:04,  8.43s/it]
  4%|▍         | 18/416 [02:23<55:52,  8.42s/it]
  5%|▍         | 19/416 [02:31<55:50,  8.44s/it]
  5%|▍         | 20/416 [02:40<55:36,  8.43s/it]
  5%|▌         | 21/416 [02:48<55:36,  8.45s/it]
  5%|▌         | 22/416 [02:57<55:21,  8.43s/it]
  6%|▌         | 23/416 [03:05<55:18,  8.44s/it]
  6%|▌         | 24/416 [03:13<55:02,  8.42s/it]
  6%|▌         | 25/416 [03:22<55:03,  8.45s/it]
  6%|▋         | 26/416 [03:30<54:47,  8.43s/it]
  6%|▋         | 27/416 [03:39<54:43,  8.44s/it]
  7%|▋         | 28/416 [03:47<54:28,  8.42s/it]
  7%|▋         | 29/416 [03:56<54:25,  8.44s/it]
  7%|▋         | 30/416 [04:04<54:09,  8.42s/it]
  7%|▋         | 31/416 [04:12<54:06,  8.43s/it]
  8%|▊         | 32/416 [04:21<53:50,  8.41s/it]
  8%|▊         | 33/416 [04:29<53:46,  8.43s/it]
  8%|▊         | 34/416 [04:38<53:31,  8.41s/it]
  8%|▊         | 35/416 [04:46<53:31,  8.43s/it]
  9%|▊         | 36/416 [04:55<53:21,  8.42s/it]
  9%|▉         | 37/416 [05:03<53:23,  8.45s/it]
  9%|▉         | 38/416 [05:11<53:06,  8.43s/it]
  9%|▉         | 39/416 [05:20<53:02,  8.44s/it]
 10%|▉         | 40/416 [05:28<52:47,  8.42s/it]
 10%|▉         | 41/416 [05:37<52:44,  8.44s/it]
 10%|█         | 42/416 [05:45<52:28,  8.42s/it]
 10%|█         | 43/416 [05:54<52:26,  8.44s/it]
 11%|█         | 44/416 [06:02<52:11,  8.42s/it]
 11%|█         | 45/416 [06:10<52:07,  8.43s/it]
 11%|█         | 46/416 [06:19<51:54,  8.42s/it]
 11%|█▏        | 47/416 [06:27<51:52,  8.44s/it]
 12%|█▏        | 48/416 [06:36<51:36,  8.41s/it]
 12%|█▏        | 49/416 [06:44<51:35,  8.43s/it]
 12%|█▏        | 50/416 [06:53<51:19,  8.41s/it]
 12%|█▏        | 51/416 [07:01<51:19,  8.44s/it]
 12%|█▎        | 52/416 [07:09<51:05,  8.42s/it]
 13%|█▎        | 53/416 [07:18<51:04,  8.44s/it]
 13%|█▎        | 54/416 [07:26<50:53,  8.44s/it]
 13%|█▎        | 55/416 [07:35<50:51,  8.45s/it]
 13%|█▎        | 56/416 [07:43<50:37,  8.44s/it]
 14%|█▎        | 57/416 [07:52<50:34,  8.45s/it]
 14%|█▍        | 58/416 [08:00<50:15,  8.42s/it]
 14%|█▍        | 59/416 [08:08<50:12,  8.44s/it]
 14%|█▍        | 60/416 [08:17<49:56,  8.42s/it]
 15%|█▍        | 61/416 [08:25<49:52,  8.43s/it]
 15%|█▍        | 62/416 [08:34<49:38,  8.41s/it]
 15%|█▌        | 63/416 [08:42<49:36,  8.43s/it]
 15%|█▌        | 64/416 [08:51<49:20,  8.41s/it]
 16%|█▌        | 65/416 [08:59<49:21,  8.44s/it]
 16%|█▌        | 66/416 [09:07<49:07,  8.42s/it]
 16%|█▌        | 67/416 [09:16<49:06,  8.44s/it]
 16%|█▋        | 68/416 [09:24<48:52,  8.43s/it]
 17%|█▋        | 69/416 [09:33<48:49,  8.44s/it]
 17%|█▋        | 70/416 [09:41<48:34,  8.42s/it]
 17%|█▋        | 71/416 [09:50<48:31,  8.44s/it]
 17%|█▋        | 72/416 [09:58<48:16,  8.42s/it]
 18%|█▊        | 73/416 [10:06<48:14,  8.44s/it]
 18%|█▊        | 74/416 [10:15<47:59,  8.42s/it]
 18%|█▊        | 75/416 [10:23<48:01,  8.45s/it]
 18%|█▊        | 76/416 [10:32<47:47,  8.43s/it]
 19%|█▊        | 77/416 [10:40<47:41,  8.44s/it]
 19%|█▉        | 78/416 [10:49<47:26,  8.42s/it]
 19%|█▉        | 79/416 [10:57<47:22,  8.43s/it]
 19%|█▉        | 80/416 [11:05<47:07,  8.42s/it]
 19%|█▉        | 81/416 [11:14<47:05,  8.43s/it]
 20%|█▉        | 82/416 [11:22<46:51,  8.42s/it]
 20%|█▉        | 83/416 [11:31<46:54,  8.45s/it]
 20%|██        | 84/416 [11:39<46:38,  8.43s/it]
 20%|██        | 85/416 [11:48<46:37,  8.45s/it]
 21%|██        | 86/416 [11:56<46:22,  8.43s/it]
 21%|██        | 87/416 [12:05<46:17,  8.44s/it]
 21%|██        | 88/416 [12:13<46:01,  8.42s/it]
 21%|██▏       | 89/416 [12:21<45:59,  8.44s/it]
 22%|██▏       | 90/416 [12:30<45:44,  8.42s/it]
 22%|██▏       | 91/416 [12:38<45:39,  8.43s/it]
 22%|██▏       | 92/416 [12:47<45:27,  8.42s/it]
 22%|██▏       | 93/416 [12:55<45:26,  8.44s/it]
 23%|██▎       | 94/416 [13:04<45:13,  8.43s/it]
 23%|██▎       | 95/416 [13:12<45:12,  8.45s/it]
 23%|██▎       | 96/416 [13:20<44:57,  8.43s/it]
 23%|██▎       | 97/416 [13:29<44:53,  8.44s/it]
 24%|██▎       | 98/416 [13:37<44:39,  8.42s/it]
 24%|██▍       | 99/416 [13:46<44:36,  8.44s/it]
 24%|██▍       | 100/416 [13:54<44:22,  8.43s/it]
 24%|██▍       | 101/416 [14:03<44:21,  8.45s/it]
 25%|██▍       | 102/416 [14:11<44:05,  8.43s/it]
 25%|██▍       | 103/416 [14:19<44:02,  8.44s/it]
 25%|██▌       | 104/416 [14:28<43:47,  8.42s/it]
 25%|██▌       | 105/416 [14:36<43:44,  8.44s/it]
 25%|██▌       | 106/416 [14:45<43:29,  8.42s/it]
 26%|██▌       | 107/416 [14:53<43:26,  8.44s/it]
 26%|██▌       | 108/416 [15:02<43:13,  8.42s/it]
 26%|██▌       | 109/416 [15:10<43:11,  8.44s/it]
 26%|██▋       | 110/416 [15:18<42:59,  8.43s/it]
 27%|██▋       | 111/416 [15:27<42:54,  8.44s/it]
 27%|██▋       | 112/416 [15:35<42:40,  8.42s/it]
 27%|██▋       | 113/416 [15:44<42:35,  8.43s/it]
 27%|██▋       | 114/416 [15:52<42:23,  8.42s/it]
 28%|██▊       | 115/416 [16:01<42:20,  8.44s/it]
 28%|██▊       | 116/416 [16:09<42:06,  8.42s/it]
 28%|██▊       | 117/416 [16:18<42:02,  8.44s/it]
 28%|██▊       | 118/416 [16:26<41:48,  8.42s/it]
 29%|██▊       | 119/416 [16:34<41:46,  8.44s/it]
 29%|██▉       | 120/416 [16:43<41:33,  8.42s/it]
 29%|██▉       | 121/416 [16:51<41:30,  8.44s/it]
 29%|██▉       | 122/416 [17:00<41:17,  8.43s/it]
 30%|██▉       | 123/416 [17:08<41:13,  8.44s/it]
 30%|██▉       | 124/416 [17:16<40:59,  8.42s/it]
 30%|███       | 125/416 [17:25<40:54,  8.44s/it]
 30%|███       | 126/416 [17:33<40:40,  8.42s/it]
 31%|███       | 127/416 [17:42<40:35,  8.43s/it]
 31%|███       | 128/416 [17:50<40:22,  8.41s/it]
 31%|███       | 129/416 [17:59<40:19,  8.43s/it]
 31%|███▏      | 130/416 [18:07<40:06,  8.41s/it]
 31%|███▏      | 131/416 [18:15<40:03,  8.43s/it]
 32%|███▏      | 132/416 [18:24<39:52,  8.42s/it]
 32%|███▏      | 133/416 [18:32<39:49,  8.44s/it]
 32%|███▏      | 134/416 [18:41<39:34,  8.42s/it]
 32%|███▏      | 135/416 [18:49<39:30,  8.44s/it]
 33%|███▎      | 136/416 [18:58<39:16,  8.42s/it]
 33%|███▎      | 137/416 [19:06<39:15,  8.44s/it]
 33%|███▎      | 138/416 [19:14<39:00,  8.42s/it]
 33%|███▎      | 139/416 [19:23<38:57,  8.44s/it]
 34%|███▎      | 140/416 [19:31<38:44,  8.42s/it]
 34%|███▍      | 141/416 [19:40<38:43,  8.45s/it]
 34%|███▍      | 142/416 [19:48<38:29,  8.43s/it]
 34%|███▍      | 143/416 [19:57<38:24,  8.44s/it]
 35%|███▍      | 144/416 [20:05<38:10,  8.42s/it]
 35%|███▍      | 145/416 [20:14<38:05,  8.43s/it]
 35%|███▌      | 146/416 [20:22<37:53,  8.42s/it]
 35%|███▌      | 147/416 [20:30<37:49,  8.44s/it]
 36%|███▌      | 148/416 [20:39<37:35,  8.42s/it]
 36%|███▌      | 149/416 [20:47<37:31,  8.43s/it]
 36%|███▌      | 150/416 [20:56<37:19,  8.42s/it]
 36%|███▋      | 151/416 [21:04<37:15,  8.44s/it]
 37%|███▋      | 152/416 [21:12<37:01,  8.42s/it]
 37%|███▋      | 153/416 [21:21<36:57,  8.43s/it]
 37%|███▋      | 154/416 [21:29<36:44,  8.42s/it]
 37%|███▋      | 155/416 [21:38<36:41,  8.44s/it]
 38%|███▊      | 156/416 [21:46<36:28,  8.42s/it]
 38%|███▊      | 157/416 [21:55<36:24,  8.43s/it]
 38%|███▊      | 158/416 [22:03<36:12,  8.42s/it]
 38%|███▊      | 159/416 [22:12<36:08,  8.44s/it]
 38%|███▊      | 160/416 [22:20<35:56,  8.42s/it]
 39%|███▊      | 161/416 [22:28<35:52,  8.44s/it]
 39%|███▉      | 162/416 [22:37<35:38,  8.42s/it]
 39%|███▉      | 163/416 [22:45<35:32,  8.43s/it]
 39%|███▉      | 164/416 [22:54<35:20,  8.41s/it]
 40%|███▉      | 165/416 [23:02<35:17,  8.44s/it]
 40%|███▉      | 166/416 [23:10<35:03,  8.41s/it]
 40%|████      | 167/416 [23:19<34:59,  8.43s/it]
 40%|████      | 168/416 [23:27<34:46,  8.41s/it]
 41%|████      | 169/416 [23:36<34:42,  8.43s/it]
 41%|████      | 170/416 [23:44<34:31,  8.42s/it]
 41%|████      | 171/416 [23:53<34:27,  8.44s/it]
 41%|████▏     | 172/416 [24:01<34:14,  8.42s/it]
 42%|████▏     | 173/416 [24:09<34:08,  8.43s/it]
 42%|████▏     | 174/416 [24:18<33:56,  8.42s/it]
 42%|████▏     | 175/416 [24:26<33:51,  8.43s/it]
 42%|████▏     | 176/416 [24:35<33:40,  8.42s/it]
 43%|████▎     | 177/416 [24:43<33:35,  8.43s/it]
 43%|████▎     | 178/416 [24:52<33:22,  8.42s/it]
 43%|████▎     | 179/416 [25:00<33:19,  8.44s/it]
 43%|████▎     | 180/416 [25:08<33:07,  8.42s/it]
 44%|████▎     | 181/416 [25:17<33:03,  8.44s/it]
 44%|████▍     | 182/416 [25:25<32:50,  8.42s/it]
 44%|████▍     | 183/416 [25:34<32:46,  8.44s/it]
 44%|████▍     | 184/416 [25:42<32:33,  8.42s/it]
 44%|████▍     | 185/416 [25:51<32:28,  8.43s/it]
 45%|████▍     | 186/416 [25:59<32:17,  8.42s/it]
 45%|████▍     | 187/416 [26:07<32:13,  8.44s/it]
 45%|████▌     | 188/416 [26:16<32:00,  8.42s/it]
 45%|████▌     | 189/416 [26:24<31:57,  8.45s/it]
 46%|████▌     | 190/416 [26:33<31:44,  8.43s/it]
 46%|████▌     | 191/416 [26:41<31:39,  8.44s/it]
 46%|████▌     | 192/416 [26:50<31:26,  8.42s/it]
 46%|████▋     | 193/416 [26:58<31:20,  8.43s/it]
 47%|████▋     | 194/416 [27:06<31:07,  8.41s/it]
 47%|████▋     | 195/416 [27:15<31:05,  8.44s/it]
 47%|████▋     | 196/416 [27:23<30:54,  8.43s/it]
 47%|████▋     | 197/416 [27:32<30:49,  8.45s/it]
 48%|████▊     | 198/416 [27:40<30:37,  8.43s/it]
 48%|████▊     | 199/416 [27:49<30:30,  8.44s/it]
 48%|████▊     | 200/416 [27:57<30:17,  8.42s/it]
 48%|████▊     | 201/416 [28:05<30:12,  8.43s/it]
 49%|████▊     | 202/416 [28:14<30:01,  8.42s/it]
 49%|████▉     | 203/416 [28:22<29:56,  8.43s/it]
 49%|████▉     | 204/416 [28:31<29:44,  8.42s/it]
 49%|████▉     | 205/416 [28:39<29:40,  8.44s/it]
 50%|████▉     | 206/416 [28:48<29:28,  8.42s/it]
 50%|████▉     | 207/416 [28:56<29:24,  8.44s/it]
 50%|█████     | 208/416 [29:04<29:11,  8.42s/it]
 50%|█████     | 209/416 [29:13<29:06,  8.44s/it]
 50%|█████     | 210/416 [29:21<28:54,  8.42s/it]
 51%|█████     | 211/416 [29:30<28:49,  8.44s/it]
 51%|█████     | 212/416 [29:38<28:37,  8.42s/it]
 51%|█████     | 213/416 [29:47<28:32,  8.43s/it]
 51%|█████▏    | 214/416 [29:55<28:21,  8.42s/it]
 52%|█████▏    | 215/416 [30:04<28:18,  8.45s/it]
 52%|█████▏    | 216/416 [30:12<28:06,  8.43s/it]
 52%|█████▏    | 217/416 [30:20<28:00,  8.44s/it]
 52%|█████▏    | 218/416 [30:29<27:47,  8.42s/it]
 53%|█████▎    | 219/416 [30:37<27:42,  8.44s/it]
 53%|█████▎    | 220/416 [30:46<27:30,  8.42s/it]
 53%|█████▎    | 221/416 [30:54<27:24,  8.43s/it]
 53%|█████▎    | 222/416 [31:02<27:11,  8.41s/it]
 54%|█████▎    | 223/416 [31:11<27:07,  8.43s/it]
 54%|█████▍    | 224/416 [31:19<26:56,  8.42s/it]
 54%|█████▍    | 225/416 [31:28<26:50,  8.43s/it]
 54%|█████▍    | 226/416 [31:36<26:39,  8.42s/it]
 55%|█████▍    | 227/416 [31:45<26:33,  8.43s/it]
 55%|█████▍    | 228/416 [31:53<26:22,  8.42s/it]
 55%|█████▌    | 229/416 [32:01<26:16,  8.43s/it]
 55%|█████▌    | 230/416 [32:10<26:05,  8.41s/it]
 56%|█████▌    | 231/416 [32:18<26:00,  8.43s/it]
 56%|█████▌    | 232/416 [32:27<25:48,  8.42s/it]
 56%|█████▌    | 233/416 [32:35<25:44,  8.44s/it]
 56%|█████▋    | 234/416 [32:44<25:33,  8.42s/it]
 56%|█████▋    | 235/416 [32:52<25:28,  8.45s/it]
 57%|█████▋    | 236/416 [33:00<25:17,  8.43s/it]
 57%|█████▋    | 237/416 [33:09<25:11,  8.44s/it]
 57%|█████▋    | 238/416 [33:17<24:58,  8.42s/it]
 57%|█████▋    | 239/416 [33:26<24:53,  8.44s/it]
 58%|█████▊    | 240/416 [33:34<24:41,  8.42s/it]
 58%|█████▊    | 241/416 [33:43<24:37,  8.44s/it]
 58%|█████▊    | 242/416 [33:51<24:25,  8.42s/it]
 58%|█████▊    | 243/416 [34:00<24:20,  8.44s/it]
 59%|█████▊    | 244/416 [34:08<24:08,  8.42s/it]
 59%|█████▉    | 245/416 [34:16<24:05,  8.45s/it]
 59%|█████▉    | 246/416 [34:25<23:52,  8.43s/it]
 59%|█████▉    | 247/416 [34:33<23:46,  8.44s/it]
 60%|█████▉    | 248/416 [34:42<23:33,  8.42s/it]
 60%|█████▉    | 249/416 [34:50<23:28,  8.43s/it]
 60%|██████    | 250/416 [34:58<23:16,  8.41s/it]
 60%|██████    | 251/416 [35:07<23:12,  8.44s/it]
 61%|██████    | 252/416 [35:15<23:01,  8.42s/it]
 61%|██████    | 253/416 [35:24<22:55,  8.44s/it]
 61%|██████    | 254/416 [35:32<22:44,  8.42s/it]
 61%|██████▏   | 255/416 [35:41<22:38,  8.44s/it]
 62%|██████▏   | 256/416 [35:49<22:26,  8.42s/it]
 62%|██████▏   | 257/416 [35:57<22:20,  8.43s/it]
 62%|██████▏   | 258/416 [36:06<22:09,  8.41s/it]
 62%|██████▏   | 259/416 [36:14<22:03,  8.43s/it]
 62%|██████▎   | 260/416 [36:23<21:52,  8.41s/it]
 63%|██████▎   | 261/416 [36:31<21:48,  8.44s/it]
 63%|██████▎   | 262/416 [36:40<21:37,  8.42s/it]
 63%|██████▎   | 263/416 [36:48<21:31,  8.44s/it]
 63%|██████▎   | 264/416 [36:56<21:21,  8.43s/it]
 64%|██████▎   | 265/416 [37:05<21:14,  8.44s/it]
 64%|██████▍   | 266/416 [37:13<21:02,  8.42s/it]
 64%|██████▍   | 267/416 [37:22<20:57,  8.44s/it]
 64%|██████▍   | 268/416 [37:30<20:45,  8.42s/it]
 65%|██████▍   | 269/416 [37:39<20:39,  8.43s/it]
 65%|██████▍   | 270/416 [37:47<20:29,  8.42s/it]
 65%|██████▌   | 271/416 [37:56<20:23,  8.44s/it]
 65%|██████▌   | 272/416 [38:04<20:12,  8.42s/it]
 66%|██████▌   | 273/416 [38:12<20:06,  8.44s/it]
 66%|██████▌   | 274/416 [38:21<19:55,  8.42s/it]
 66%|██████▌   | 275/416 [38:29<19:49,  8.44s/it]
 66%|██████▋   | 276/416 [38:38<19:38,  8.42s/it]
 67%|██████▋   | 277/416 [38:46<19:33,  8.44s/it]
 67%|██████▋   | 278/416 [38:54<19:21,  8.42s/it]
 67%|██████▋   | 279/416 [39:03<19:15,  8.43s/it]
 67%|██████▋   | 280/416 [39:11<19:05,  8.42s/it]
 68%|██████▊   | 281/416 [39:20<19:00,  8.45s/it]
 68%|██████▊   | 282/416 [39:28<18:49,  8.43s/it]
 68%|██████▊   | 283/416 [39:37<18:43,  8.45s/it]
 68%|██████▊   | 284/416 [39:45<18:32,  8.43s/it]
 69%|██████▊   | 285/416 [39:54<18:25,  8.44s/it]
 69%|██████▉   | 286/416 [40:02<18:14,  8.42s/it]
 69%|██████▉   | 287/416 [40:10<18:08,  8.44s/it]
 69%|██████▉   | 288/416 [40:19<17:57,  8.41s/it]
 69%|██████▉   | 289/416 [40:27<17:51,  8.44s/it]
 70%|██████▉   | 290/416 [40:36<17:40,  8.42s/it]
 70%|██████▉   | 291/416 [40:44<17:34,  8.44s/it]
 70%|███████   | 292/416 [40:53<17:24,  8.42s/it]
 70%|███████   | 293/416 [41:01<17:18,  8.44s/it]
 71%|███████   | 294/416 [41:09<17:06,  8.42s/it]
 71%|███████   | 295/416 [41:18<17:00,  8.43s/it]
 71%|███████   | 296/416 [41:26<16:49,  8.41s/it]
 71%|███████▏  | 297/416 [41:35<16:42,  8.42s/it]
 72%|███████▏  | 298/416 [41:43<16:32,  8.41s/it]
 72%|███████▏  | 299/416 [41:51<16:26,  8.43s/it]
 72%|███████▏  | 300/416 [42:00<16:16,  8.42s/it]
 72%|███████▏  | 301/416 [42:08<16:09,  8.43s/it]
 73%|███████▎  | 302/416 [42:17<15:59,  8.41s/it]
 73%|███████▎  | 303/416 [42:25<15:52,  8.43s/it]
 73%|███████▎  | 304/416 [42:34<15:41,  8.41s/it]
 73%|███████▎  | 305/416 [42:42<15:35,  8.43s/it]
 74%|███████▎  | 306/416 [42:50<15:24,  8.41s/it]
 74%|███████▍  | 307/416 [42:59<15:19,  8.43s/it]
 74%|███████▍  | 308/416 [43:07<15:09,  8.42s/it]
 74%|███████▍  | 309/416 [43:16<15:03,  8.45s/it]
 75%|███████▍  | 310/416 [43:24<14:53,  8.43s/it]
 75%|███████▍  | 311/416 [43:33<14:46,  8.44s/it]
 75%|███████▌  | 312/416 [43:41<14:36,  8.43s/it]
 75%|███████▌  | 313/416 [43:49<14:29,  8.44s/it]
 75%|███████▌  | 314/416 [43:58<14:18,  8.42s/it]
 76%|███████▌  | 315/416 [44:06<14:12,  8.44s/it]
 76%|███████▌  | 316/416 [44:15<14:01,  8.41s/it]
 76%|███████▌  | 317/416 [44:23<13:55,  8.44s/it]
 76%|███████▋  | 318/416 [44:32<13:45,  8.42s/it]
 77%|███████▋  | 319/416 [44:40<13:38,  8.44s/it]
 77%|███████▋  | 320/416 [44:48<13:29,  8.43s/it]
 77%|███████▋  | 321/416 [44:57<13:22,  8.44s/it]
 77%|███████▋  | 322/416 [45:05<13:11,  8.42s/it]
 78%|███████▊  | 323/416 [45:14<13:04,  8.43s/it]
 78%|███████▊  | 324/416 [45:22<12:54,  8.42s/it]
 78%|███████▊  | 325/416 [45:31<12:47,  8.43s/it]
 78%|███████▊  | 326/416 [45:39<12:37,  8.42s/it]
 79%|███████▊  | 327/416 [45:47<12:30,  8.43s/it]
 79%|███████▉  | 328/416 [45:56<12:21,  8.42s/it]
 79%|███████▉  | 329/416 [46:04<12:14,  8.44s/it]
 79%|███████▉  | 330/416 [46:13<12:04,  8.43s/it]
 80%|███████▉  | 331/416 [46:21<11:57,  8.44s/it]
 80%|███████▉  | 332/416 [46:30<11:47,  8.43s/it]
 80%|████████  | 333/416 [46:38<11:40,  8.44s/it]
 80%|████████  | 334/416 [46:46<11:30,  8.42s/it]
 81%|████████  | 335/416 [46:55<11:23,  8.44s/it]
 81%|████████  | 336/416 [47:03<11:13,  8.42s/it]
 81%|████████  | 337/416 [47:12<11:06,  8.44s/it]
 81%|████████▏ | 338/416 [47:20<10:56,  8.42s/it]
 81%|████████▏ | 339/416 [47:29<10:49,  8.43s/it]
 82%|████████▏ | 340/416 [47:37<10:39,  8.42s/it]
 82%|████████▏ | 341/416 [47:45<10:32,  8.43s/it]
 82%|████████▏ | 342/416 [47:54<10:23,  8.42s/it]
 82%|████████▏ | 343/416 [48:02<10:15,  8.43s/it]
 83%|████████▎ | 344/416 [48:11<10:06,  8.42s/it]
 83%|████████▎ | 345/416 [48:19<09:58,  8.43s/it]
 83%|████████▎ | 346/416 [48:28<09:49,  8.41s/it]
 83%|████████▎ | 347/416 [48:36<09:41,  8.43s/it]
 84%|████████▎ | 348/416 [48:44<09:31,  8.41s/it]
 84%|████████▍ | 349/416 [48:53<09:24,  8.43s/it]
 84%|████████▍ | 350/416 [49:01<09:15,  8.41s/it]
 84%|████████▍ | 351/416 [49:10<09:08,  8.43s/it]
 85%|████████▍ | 352/416 [49:18<08:58,  8.42s/it]
 85%|████████▍ | 353/416 [49:27<08:51,  8.44s/it]
 85%|████████▌ | 354/416 [49:35<08:41,  8.42s/it]
 85%|████████▌ | 355/416 [49:43<08:34,  8.43s/it]
 86%|████████▌ | 356/416 [49:52<08:24,  8.42s/it]
 86%|████████▌ | 357/416 [50:00<08:17,  8.44s/it]
 86%|████████▌ | 358/416 [50:09<08:08,  8.43s/it]
 86%|████████▋ | 359/416 [50:17<08:01,  8.45s/it]
 87%|████████▋ | 360/416 [50:26<07:51,  8.43s/it]
 87%|████████▋ | 361/416 [50:34<07:44,  8.44s/it]
 87%|████████▋ | 362/416 [50:42<07:34,  8.42s/it]
 87%|████████▋ | 363/416 [50:51<07:26,  8.43s/it]
 88%|████████▊ | 364/416 [50:59<07:17,  8.42s/it]
 88%|████████▊ | 365/416 [51:08<07:09,  8.43s/it]
 88%|████████▊ | 366/416 [51:16<07:01,  8.42s/it]
 88%|████████▊ | 367/416 [51:25<06:53,  8.44s/it]
 88%|████████▊ | 368/416 [51:33<06:44,  8.42s/it]
 89%|████████▊ | 369/416 [51:41<06:36,  8.44s/it]
 89%|████████▉ | 370/416 [51:50<06:27,  8.42s/it]
 89%|████████▉ | 371/416 [51:58<06:19,  8.43s/it]
 89%|████████▉ | 372/416 [52:07<06:10,  8.41s/it]
 90%|████████▉ | 373/416 [52:15<06:02,  8.43s/it]
 90%|████████▉ | 374/416 [52:24<05:54,  8.43s/it]
 90%|█████████ | 375/416 [52:32<05:46,  8.45s/it]
 90%|█████████ | 376/416 [52:40<05:37,  8.43s/it]
 91%|█████████ | 377/416 [52:49<05:29,  8.44s/it]
 91%|█████████ | 378/416 [52:57<05:19,  8.42s/it]
 91%|█████████ | 379/416 [53:06<05:12,  8.43s/it]
 91%|█████████▏| 380/416 [53:14<05:02,  8.42s/it]
 92%|█████████▏| 381/416 [53:23<04:55,  8.43s/it]
 92%|█████████▏| 382/416 [53:31<04:45,  8.41s/it]
 92%|█████████▏| 383/416 [53:39<04:38,  8.44s/it]
 92%|█████████▏| 384/416 [53:48<04:29,  8.43s/it]
 93%|█████████▎| 385/416 [53:56<04:21,  8.44s/it]
 93%|█████████▎| 386/416 [54:05<04:12,  8.43s/it]
 93%|█████████▎| 387/416 [54:13<04:04,  8.45s/it]
 93%|█████████▎| 388/416 [54:22<03:55,  8.43s/it]
 94%|█████████▎| 389/416 [54:30<03:47,  8.44s/it]
 94%|█████████▍| 390/416 [54:38<03:38,  8.42s/it]
 94%|█████████▍| 391/416 [54:47<03:30,  8.43s/it]
 94%|█████████▍| 392/416 [54:55<03:22,  8.42s/it]
 94%|█████████▍| 393/416 [55:04<03:13,  8.43s/it]
 95%|█████████▍| 394/416 [55:12<03:05,  8.41s/it]
 95%|█████████▍| 395/416 [55:21<02:57,  8.44s/it]
 95%|█████████▌| 396/416 [55:29<02:48,  8.42s/it]
 95%|█████████▌| 397/416 [55:37<02:40,  8.44s/it]
 96%|█████████▌| 398/416 [55:46<02:31,  8.42s/it]
 96%|█████████▌| 399/416 [55:54<02:23,  8.43s/it]
 96%|█████████▌| 400/416 [56:03<02:14,  8.41s/it]
 96%|█████████▋| 401/416 [56:11<02:06,  8.43s/it]
 97%|█████████▋| 402/416 [56:20<01:57,  8.42s/it]
 97%|█████████▋| 403/416 [56:28<01:49,  8.43s/it]
 97%|█████████▋| 404/416 [56:36<01:40,  8.41s/it]
 97%|█████████▋| 405/416 [56:45<01:32,  8.43s/it]
 98%|█████████▊| 406/416 [56:53<01:24,  8.42s/it]
 98%|█████████▊| 407/416 [57:02<01:15,  8.44s/it]
 98%|█████████▊| 408/416 [57:10<01:07,  8.42s/it]
 98%|█████████▊| 409/416 [57:19<00:59,  8.43s/it]
 99%|█████████▊| 410/416 [57:27<00:50,  8.41s/it]
 99%|█████████▉| 411/416 [57:35<00:42,  8.43s/it]
 99%|█████████▉| 412/416 [57:44<00:33,  8.42s/it]
 99%|█████████▉| 413/416 [57:52<00:25,  8.43s/it]
100%|█████████▉| 414/416 [58:01<00:16,  8.43s/it]
100%|█████████▉| 415/416 [58:09<00:08,  8.44s/it]
100%|██████████| 416/416 [58:17<00:00,  8.42s/it]
100%|██████████| 416/416 [58:17<00:00,  8.41s/it]
***** eval metrics *****
  epoch                   =        1.0
  eval_loss               =     0.0876
  eval_runtime            = 0:58:26.46
  eval_samples            =      20323
  eval_samples_per_second =      3.792
  eval_steps_per_second   =      0.119
2024-02-12 12:40:31 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:2889] 2024-02-12 12:40:57,872 >> Saving model checkpoint to model/LDCC-SOLAR-10.7B-sft-qlora-v3
[INFO|tokenization_utils_base.py:2432] 2024-02-12 12:40:58,181 >> tokenizer config file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-02-12 12:40:58,182 >> Special tokens file saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/special_tokens_map.json
2024-02-12 12:40:59 - INFO - __main__ - Model saved to model/LDCC-SOLAR-10.7B-sft-qlora-v3
[INFO|modelcard.py:452] 2024-02-12 12:40:59,702 >> Dropping the following result as it does not have all the necessary fields:
{'dataset': {'name': 'generator', 'type': 'generator', 'config': 'default', 'split': 'train', 'args': 'default'}}
[INFO|configuration_utils.py:483] 2024-02-12 12:40:59,709 >> Configuration saved in model/LDCC-SOLAR-10.7B-sft-qlora-v3/config.json
2024-02-12 12:40:59 - INFO - __main__ - *** Training complete ***
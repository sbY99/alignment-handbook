{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "base_model_name = \"GAI-LLM/Yi-Ko-6B-mixed-v15\"\n",
    "adapter_model_name = \"../model/GAI-LLM-Yi-Ko-6B-mixed-v15-sft-qlora-v1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, adapter_model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/eval_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(input_string):\n",
    "    index_t = input_string.find('<|assistant|>')\n",
    "    if index_t != -1:  \n",
    "        result = input_string[index_t + len('<|assistant|>'):]\n",
    "    else: \n",
    "        raise Exception\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2000 x 10\n",
    "questions = []\n",
    "inference_results = []\n",
    "PROMPT_TEMPLATE = '<|user|>{question}{sep_token}<|assistant|>'\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    result = []\n",
    "    row = data.iloc[i]\n",
    "\n",
    "    question = row['질문']\n",
    "    answer = row['답변']\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(question=question,\n",
    "                                     sep_token=tokenizer.eos_token)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(input_ids=inputs, \n",
    "                         max_length=512, \n",
    "                         num_beams=10,\n",
    "                         repetition_penalty=1.5,\n",
    "                         diversity_penalty=0.5,\n",
    "                         num_beam_groups=5,\n",
    "                         num_return_sequences=10)\n",
    "    \n",
    "    for k in range(10):\n",
    "        response = tokenizer.decode(outputs[k], skip_special_tokens=True)\n",
    "        response = extract_text(response)\n",
    "        result.append(response)\n",
    "    \n",
    "    questions.append(question)\n",
    "    inference_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(questions), len(inference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 결과 저장\n",
    "sft_inf_results = {'data':[]}\n",
    "for q, result in zip(questions,inference_results):\n",
    "    item = {\n",
    "        'question':q,\n",
    "        'result':result\n",
    "    }\n",
    "    sft_inf_results['data'].append(item)\n",
    "\n",
    "import json\n",
    "with open(\"../data/dpo/GAI-LLM-Yi-Ko-6B-mixed-v15-qlora-v1-dpo-raw.json\", \"w\") as json_file:\n",
    "    json.dump(sft_inf_results, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer \n",
    "\n",
    "embed_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "embed_model =  embed_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b) if norm_a != 0 and norm_b != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = []\n",
    "chosen = []\n",
    "rejected = []\n",
    "\n",
    "for i in tqdm(range(len(inference_results))):\n",
    "    row = data.iloc[i]\n",
    "    preds = inference_results[i]\n",
    "\n",
    "    answer = row['답변']\n",
    "\n",
    "    gt_embed = embed_model.encode(answer)\n",
    "    sample_score_list = []\n",
    "\n",
    "    for pred in preds:\n",
    "        pred_embed = embed_model.encode(pred)\n",
    "        sample_score = cosine_similarity(gt_embed, pred_embed)\n",
    "        sample_score_list.append(sample_score)\n",
    "    \n",
    "    max_idx = sample_score_list.index(max(sample_score_list))\n",
    "    min_idx = sample_score_list.index(min(sample_score_list))\n",
    "    \n",
    "    prompt.append(row['질문'])\n",
    "    chosen.append(preds[max_idx])\n",
    "    rejected.append(preds[min_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_df = pd.DataFrame({\n",
    "        'prompt':prompt,\n",
    "        'chosen':chosen,\n",
    "        'rejected':rejected\n",
    "    }\n",
    ")\n",
    "dpo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_ratio = 0.9 \n",
    "train_df, dev_df = train_test_split(dpo_df, test_size=1-train_ratio, random_state=42)\n",
    "print(train_df.shape) # 13xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/dpo/GAI-LLM-Yi-Ko-6B-mixed-v15-qlora-v1-dpo-train.csv')\n",
    "dev_df.to_csv('../data/dpo/GAI-LLM-Yi-Ko-6B-mixed-v15-qlora-v1-dpo-eval.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

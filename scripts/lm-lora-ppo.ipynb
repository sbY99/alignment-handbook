{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-21 03:27:05,095] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22716/22716 [00:05<00:00, 3878.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "python examples/scripts/ppo.py \\\n",
    "    --log_with=wandb\n",
    "\"\"\"\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, HfArgumentParser, pipeline\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "# We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}\n",
    "\n",
    "trl_model_class = AutoModelForCausalLMWithValueHead\n",
    "\n",
    "def extract_text(input_string):\n",
    "    index_t = input_string.find('[/INST]')\n",
    "    if index_t != -1:  \n",
    "        result = input_string[index_t + len('[/INST]'):]\n",
    "    else: \n",
    "        raise Exception\n",
    "    return result\n",
    "\n",
    "# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
    "# from the `datasets` library. One should customize this function to train the model on\n",
    "# its own dataset.\n",
    "def build_dataset(data_path, model_name):\n",
    "    with open(data_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    questions = []\n",
    "    label_dict = {}\n",
    "\n",
    "    for item in data['data']:\n",
    "        questions.append(\n",
    "            f\"[INST]{item['질문']}[/INST]\"\n",
    "        )\n",
    "        label_dict[item['질문']] = item['답변']\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dataset = Dataset.from_dict({'text':questions})\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"text\"])\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = dataset.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds, label_dict\n",
    "\n",
    "data_path = \"../data/train_ppo_v2.json\"\n",
    "model_name = \"yanolja/KoSOLAR-10.7B-v0.2\"\n",
    "adapter_path = '../model/yanolja-KoSOLAR-10.7B-v0.2-sft-qlora-v5'\n",
    "\n",
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "dataset, label_dict = build_dataset(data_path, model_name)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return {key: [d[key] for d in data] for key in data[0]}\n",
    "\n",
    "\n",
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'query'],\n",
       "    num_rows: 22716\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(dataset[-1]['text'], return_tensors=\"pt\").reshape(-1).reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(dataset[-1]['text'], return_tensors=\"pt\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST]인테리어에서 사용되는 바닥재 중 목재 바닥은 어떤 장점들이 있는가요? 주방을 모던하게 꾸미는 데 인테리어 아이디어가 어떤 것들이 있을까요?[/INST]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[-1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> [INST]인테리어에서 사용되는 바닥재 중 목재 바닥은 어떤 장점들이 있는가요? 주방을 모던하게 꾸미는 데 인테리어 아이디어가 어떤 것들이 있을까요?[/INST]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[-1]['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['면진장치란 지반에서 오는 진동 에너지를 흡수하여 건물에 주는 진동을 줄여주는 진동 격리장치입니다.',\n",
       " '면진장치란 건물의 지반에서 발생하는 진동 에너지를 흡수하여 건물을 보호하고, 진동을 줄여주는 장치입니다. 주로 지진이나 기타 지반의 진동으로 인한 피해를 방지하기 위해 사용됩니다.',\n",
       " '면진장치란 지반으로부터 발생하는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여주는 장치를 말합니다. 이를 통해 건물의 안전성과 안정성을 향상시키고, 지진 등의 외부 충격으로부터 보호하는 역할을 합니다. 지진으로 인한 건물의 피해를 최소화하기 위해 주로 사용됩니다.',\n",
       " '면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 최소화해 주는 진동 격리장치입니다. 이를 통해 건물 내부의 진동을 줄이고 안정성을 유지하는 데 도움을 줍니다.',\n",
       " '면진장치는 건물에 오는 지반 진동의 영향을 최대한으로 흡수하여 건물에 전달되는 진동을 줄여주는 장치입니다. 지반으로부터 오는 진동 에너지의 영향을 완화시키기 위해 사용됩니다.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "ref_model = None\n",
    "# Copy the model to each device\n",
    "device_map = {\"\": Accelerator().local_process_index}\n",
    "\n",
    "from trl import PPOConfig\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1e-5,\n",
    "    seed=42,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trl_model_class.from_pretrained(\n",
    "    adapter_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then build the sentiment analysis pipeline, passing the model name and the\n",
    "# sentiment analysis pipeline arguments. Let's also make sure to set the device\n",
    "# to the same device as the PPOTrainer.\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from gpt2\n",
    "    response_tensors, ref_response_tensors = ppo_trainer.generate(\n",
    "        query_tensors, return_prompt=False, generate_ref_response=True, **generation_kwargs\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(response_tensors)\n",
    "    batch[\"ref_response\"] = tokenizer.batch_decode(ref_response_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer.\n",
    "if not args.use_peft:\n",
    "    ref_model = trl_model_class.from_pretrained(ppo_config.model_name, trust_remote_code=args.trust_remote_code)\n",
    "    device_map = None\n",
    "    peft_config = None\n",
    "else:\n",
    "    peft_config = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    ref_model = None\n",
    "    # Copy the model to each device\n",
    "    device_map = {\"\": Accelerator().local_process_index}\n",
    "\n",
    "model = trl_model_class.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=args.trust_remote_code,\n",
    "    device_map=device_map,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)\n",
    "\n",
    "# We then build the sentiment analysis pipeline, passing the model name and the\n",
    "# sentiment analysis pipeline arguments. Let's also make sure to set the device\n",
    "# to the same device as the PPOTrainer.\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    if is_xpu_available():\n",
    "        device = \"xpu:0\"\n",
    "    elif is_npu_available():\n",
    "        device = \"npu:0\"\n",
    "    else:\n",
    "        device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin\n",
    "task, model_name = ppo_config.reward_model.split(\":\")\n",
    "if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():\n",
    "    with ds_plugin.zero3_init_context_manager(enable=False):\n",
    "        sentiment_pipe = pipeline(task, model=model_name, device=device)\n",
    "else:\n",
    "    sentiment_pipe = pipeline(task, model=model_name, device=device)\n",
    "\n",
    "# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\n",
    "if sentiment_pipe.tokenizer.pad_token_id is None:\n",
    "    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if sentiment_pipe.model.config.pad_token_id is None:\n",
    "    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 512,\n",
    "}\n",
    "\n",
    "for _epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from gpt2\n",
    "    response_tensors, ref_response_tensors = ppo_trainer.generate(\n",
    "        query_tensors, return_prompt=False, generate_ref_response=True, **generation_kwargs\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(response_tensors)\n",
    "    batch[\"ref_response\"] = tokenizer.batch_decode(ref_response_tensors)\n",
    "\n",
    "    # Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "    ref_texts = [q + r for q, r in zip(batch[\"query\"], batch[\"ref_response\"])]\n",
    "    ref_pipe_outputs = sentiment_pipe(ref_texts, **sent_kwargs)\n",
    "    ref_rewards = [torch.tensor(output[1][\"score\"]) for output in ref_pipe_outputs]\n",
    "    batch[\"ref_rewards\"] = ref_rewards\n",
    "\n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"ref_response\", \"ref_rewards\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

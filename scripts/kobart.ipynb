{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (BartForConditionalGeneration,\n",
    "                          PreTrainedTokenizerFast)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v2\", device_map=\"auto\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained( \"gogamza/kobart-base-v2\", bos_token=\"<s>\", eos_token=\"</s>\",unk_token='<unk>',pad_token='<pad>',mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185796, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train_v3.csv')\n",
    "eval_df = pd.read_csv('../data/eval_v3.csv')\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, filepath, max_seq_len=128) -> None:\n",
    "        self.filepath = filepath\n",
    "        self.dataset = pd.read_csv(self.filepath)\n",
    "        self.bos_token = '<s>'\n",
    "        self.eos_token = '</s>'\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "            \"gogamza/kobart-base-v2\",\n",
    "            bos_token=\"<s>\",\n",
    "            eos_token=\"</s>\",\n",
    "            unk_token='<unk>',\n",
    "            pad_token='<pad>',\n",
    "            mask_token='<mask>'\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def make_input_id_mask(self, tokens, index):\n",
    "        input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        if len(input_id) < self.max_seq_len:\n",
    "            while len(input_id) < self.max_seq_len:\n",
    "                input_id += [self.tokenizer.pad_token_id]\n",
    "                attention_mask += [0]\n",
    "        else:\n",
    "            # logging.warning(f'exceed max_seq_len for given article : {index}')\n",
    "            input_id = input_id[:self.max_seq_len - 1] + [\n",
    "                self.tokenizer.eos_token_id]\n",
    "            attention_mask = attention_mask[:self.max_seq_len]\n",
    "        return input_id, attention_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        record = self.dataset.iloc[index]\n",
    "        q, a = record['질문'], record['답변']\n",
    "        q_tokens = [self.bos_token] + \\\n",
    "            self.tokenizer.tokenize(q) + [self.eos_token]\n",
    "        a_tokens = [self.bos_token] + \\\n",
    "            self.tokenizer.tokenize(a) + [self.eos_token]\n",
    "        encoder_input_id, encoder_attention_mask = self.make_input_id_mask(\n",
    "            q_tokens, index)\n",
    "        decoder_input_id, decoder_attention_mask = self.make_input_id_mask(\n",
    "            a_tokens, index)\n",
    "        labels = self.tokenizer.convert_tokens_to_ids(\n",
    "            a_tokens[1:(self.max_seq_len + 1)])\n",
    "        if len(labels) < self.max_seq_len:\n",
    "            while len(labels) < self.max_seq_len:\n",
    "                # for cross entropy loss masking\n",
    "                labels += [-100]\n",
    "        return {'input_ids': np.array(encoder_input_id, dtype=np.int_),\n",
    "                'attention_mask': np.array(encoder_attention_mask, dtype=np.float_),\n",
    "                'decoder_input_ids': np.array(decoder_input_id, dtype=np.int_),\n",
    "                'decoder_attention_mask': np.array(decoder_attention_mask, dtype=np.float_),\n",
    "                'labels': np.array(labels, dtype=np.int_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "train_data= ChatDataset('../data/train_v3.csv',512)\n",
    "val_data= ChatDataset('../data/eval_v3.csv',512)\n",
    "train = DataLoader(train_data,batch_size=64,num_workers=8, shuffle=True)\n",
    "val=DataLoader(val_data, batch_size=64,num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../model/kobart\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT-Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (BartForConditionalGeneration,\n",
    "                          PreTrainedTokenizerFast)\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "#model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v2\", device_map=\"auto\")\n",
    "model = BartForConditionalGeneration.from_pretrained('../model/kobart/checkpoint-2800')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained( \"gogamza/kobart-base-v2\", bos_token=\"<s>\", eos_token=\"</s>\",unk_token='<unk>',pad_token='<pad>',mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_response(text):\n",
    "    device = \"cuda\"\n",
    "\n",
    "    input_ids =  [tokenizer.bos_token_id] + tokenizer.encode(text) + [tokenizer.eos_token_id]\n",
    "    res_ids = model.generate(torch.tensor([input_ids]).to(device),\n",
    "                                                max_length=512,\n",
    "                                                num_beams=5,\n",
    "                                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                                bad_words_ids=[[tokenizer.unk_token_id]])\n",
    "    a = tokenizer.batch_decode(res_ids.tolist(), skip_special_tokens=True)[0]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "방청 페인트의 종류로는 광명단페인트, 방청산화철페인트, 알미늄페인트, 역청질페인트, 워시프라이머, 크롬산아연페인트, 규산염페인트 등이 있습니다. 이러한 페인트들은 각각의 특성과 용도에 맞게 선택하여 사용할 수 있습니다. 광명단페인트는 반사율이 높아서 더 밝은 공간으로 만들어주며, 방청산화철페인트는 방청제가 첨가되어 방청 효과를 내는 페인트입니다. 알미늄페인트는 금속 또는 철강 표면에 사용되어 내화, 방청 효과를 가지는 페인트입니다. 역청질페인트는 강철, 철판, 강관 등 강철재에 방청처리용으로 사용되는 페인트로, 방청제의 첨가와 염모처리에 의해 방청효과가 향상되었습니다. 워시프라이머는 철강, 알루미늄, 유리섬유, 금속, 플라스틱 등에 사용되는 중요한 물품의 부착용으로서 용도에 맞게 선정할 수 있습니다. 크롬산아연페인트와 규산염페인트도 각각의 특성과 용도에 따라 선택하여 사용할 수 있습니다.원목사이딩의 장점은 있으나, 단점으로는 가격대가 높고 관리가 어려우며 습기에 약해 뒤틀림, 부서짐, 수축/팽창이 생길 수 있는 점이 있습니다. 추가적으로 원목사이딩은 색상이 변색될 수 있고 해충 침입에 취약하다는 점도 고려해야 합니다.\n"
     ]
    }
   ],
   "source": [
    "text = \"방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요? 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요?\"\n",
    "print(get_response(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sent=[]\n",
    "test=pd.read_csv('../data/test_raw.csv')\n",
    "\n",
    "for i in range(len(test)):\n",
    "  row = test.iloc[i]\n",
    "  question = row['질문']\n",
    "  response = get_response(question)\n",
    "  generated_sent.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카페트의 기대수명은 일반적으로 6년입니다. 다만, 사용 빈도, 청소 방식, 햇빛 노출 정도 등 사용 환경과 관리에 따라 수명이 달라질 수 있으므로 주기적으로 청소 및 관리를 해야 합니다. 또한, 질 좋은 카페트를 구매하고 깔끔하게 관리하여 수명을 연장할 수 있습니다.오리지널징크의 장점은 다른 징크에 비해 수명이 길며 다양한 패턴과 디자인이 가능하다는 점입니다. 또한, 친환경적이고 금속 부식에 대한 내식성이 뛰어나 유지보수가 용이하다는 특징이 있습니다. 이러한 이점들로 인해 오리지널징크는 건물 외벽 및 지붕재로 널리 사용되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(generated_sent[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "m = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_list=[]\n",
    "for i in range(len(generated_sent)):\n",
    "  embed=m.encode(generated_sent[i]) #주어진 모델로 인코딩\n",
    "  encode_list.append(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(encode_list)):\n",
    "  sub.loc[i, 'vec_0':'vec_511']=encode_list[i]\n",
    "\n",
    "sub.set_index('id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec_0</th>\n",
       "      <th>vec_1</th>\n",
       "      <th>vec_2</th>\n",
       "      <th>vec_3</th>\n",
       "      <th>vec_4</th>\n",
       "      <th>vec_5</th>\n",
       "      <th>vec_6</th>\n",
       "      <th>vec_7</th>\n",
       "      <th>vec_8</th>\n",
       "      <th>vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>vec_502</th>\n",
       "      <th>vec_503</th>\n",
       "      <th>vec_504</th>\n",
       "      <th>vec_505</th>\n",
       "      <th>vec_506</th>\n",
       "      <th>vec_507</th>\n",
       "      <th>vec_508</th>\n",
       "      <th>vec_509</th>\n",
       "      <th>vec_510</th>\n",
       "      <th>vec_511</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TEST_000</th>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.036606</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.006188</td>\n",
       "      <td>0.062858</td>\n",
       "      <td>0.028712</td>\n",
       "      <td>-0.045880</td>\n",
       "      <td>0.01161</td>\n",
       "      <td>0.009886</td>\n",
       "      <td>0.033085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019034</td>\n",
       "      <td>-0.041623</td>\n",
       "      <td>-0.028136</td>\n",
       "      <td>-0.034332</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>-0.007698</td>\n",
       "      <td>0.029485</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>0.050413</td>\n",
       "      <td>0.038683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEST_001</th>\n",
       "      <td>-0.009441</td>\n",
       "      <td>0.023115</td>\n",
       "      <td>-0.015673</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.091690</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.014662</td>\n",
       "      <td>-0.00303</td>\n",
       "      <td>-0.020069</td>\n",
       "      <td>0.027384</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.014458</td>\n",
       "      <td>-0.008928</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>-0.068741</td>\n",
       "      <td>0.058391</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>-0.015291</td>\n",
       "      <td>-0.004628</td>\n",
       "      <td>0.040544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             vec_0     vec_1     vec_2     vec_3     vec_4     vec_5  \\\n",
       "id                                                                     \n",
       "TEST_000  0.005784  0.036606  0.007475  0.006188  0.062858  0.028712   \n",
       "TEST_001 -0.009441  0.023115 -0.015673  0.002355  0.091690  0.012363   \n",
       "\n",
       "             vec_6    vec_7     vec_8     vec_9  ...   vec_502   vec_503  \\\n",
       "id                                               ...                       \n",
       "TEST_000 -0.045880  0.01161  0.009886  0.033085  ... -0.019034 -0.041623   \n",
       "TEST_001  0.014662 -0.00303 -0.020069  0.027384  ... -0.000095 -0.014458   \n",
       "\n",
       "           vec_504   vec_505   vec_506   vec_507   vec_508   vec_509  \\\n",
       "id                                                                     \n",
       "TEST_000 -0.028136 -0.034332  0.001910 -0.007698  0.029485  0.005473   \n",
       "TEST_001 -0.008928  0.009849 -0.068741  0.058391  0.003188 -0.015291   \n",
       "\n",
       "           vec_510   vec_511  \n",
       "id                            \n",
       "TEST_000  0.050413  0.038683  \n",
       "TEST_001 -0.004628  0.040544  \n",
       "\n",
       "[2 rows x 512 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../result/kobart-v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../result/kobart-v3.txt ', 'w+') as file:\n",
    "    file.write('\\n'.join(generated_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at ../model/kobart/checkpoint-2800/ and are newly initialized: ['model.decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('../model/kobart/checkpoint-2800/')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained( \"gogamza/kobart-base-v2\", bos_token=\"<s>\", eos_token=\"</s>\",unk_token='<unk>',pad_token='<pad>',mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "embed_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_v3.csv')\n",
    "eval_df = pd.read_csv('../data/eval_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>질문</th>\n",
       "      <th>답변</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>177338</td>\n",
       "      <td>유성발수제의 장점이 뭐야?인테리어에서 현대적 감각을 나타내기 위해 어떤 디자인 요소...</td>\n",
       "      <td>유성발수제의 장점은 소재에 물의 침투를 차단하여 소재의 수명을 연장하고, 동결, 염...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245286</td>\n",
       "      <td>속건형 유성 발수제의 장점이 뭐야?어떤 종류의 인테리어 조명이 있죠?</td>\n",
       "      <td>속건형 유성 발수제의 장점은 다양한 건축자재의 표면에 발라 빗물과 기타 물질의 침투...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>213874</td>\n",
       "      <td>어떻게 다이닝 룸을 정통적으로 꾸밀 수 있을까요?비닐사이딩은 뭔가요?</td>\n",
       "      <td>다이닝 룸을 정통적으로 꾸미기 위해서는 대형 식탁과 고풍스러운 의자를 선택하는 것이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285334</td>\n",
       "      <td>규산염페인트가 뭐야?천연벽지의 장점과 단점에 대해 알고 계신가요?</td>\n",
       "      <td>규산염페인트는 교상의 규산염과 방청안료를 주원료로 장유성바니쉬를 혼합한 것을 의미합...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>206342</td>\n",
       "      <td>리얼징크가 뭐야?층간소음이 발생하는 기준은 어떤 것인가요?</td>\n",
       "      <td>리얼징크는 철판에 부식을 방지하기 위해 아연 도금을 한 후 페인트를 칠한 외장재로,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                 질문  \\\n",
       "0      177338  유성발수제의 장점이 뭐야?인테리어에서 현대적 감각을 나타내기 위해 어떤 디자인 요소...   \n",
       "1      245286             속건형 유성 발수제의 장점이 뭐야?어떤 종류의 인테리어 조명이 있죠?   \n",
       "2      213874             어떻게 다이닝 룸을 정통적으로 꾸밀 수 있을까요?비닐사이딩은 뭔가요?   \n",
       "3      285334               규산염페인트가 뭐야?천연벽지의 장점과 단점에 대해 알고 계신가요?   \n",
       "4      206342                   리얼징크가 뭐야?층간소음이 발생하는 기준은 어떤 것인가요?   \n",
       "\n",
       "                                                  답변  \n",
       "0  유성발수제의 장점은 소재에 물의 침투를 차단하여 소재의 수명을 연장하고, 동결, 염...  \n",
       "1  속건형 유성 발수제의 장점은 다양한 건축자재의 표면에 발라 빗물과 기타 물질의 침투...  \n",
       "2  다이닝 룸을 정통적으로 꾸미기 위해서는 대형 식탁과 고풍스러운 의자를 선택하는 것이...  \n",
       "3  규산염페인트는 교상의 규산염과 방청안료를 주원료로 장유성바니쉬를 혼합한 것을 의미합...  \n",
       "4  리얼징크는 철판에 부식을 방지하기 위해 아연 도금을 한 후 페인트를 칠한 외장재로,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PPODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path1, path2=None):\n",
    "        df1 = pd.read_csv(path1)\n",
    "        if path2:\n",
    "            df2 = pd.read_csv(path2)\n",
    "            df = pd.concat((df1, df2))\n",
    "        else:\n",
    "            df = df1\n",
    "            \n",
    "        self.dataset = df1\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "            \"gogamza/kobart-base-v2\",\n",
    "            bos_token=\"<s>\",\n",
    "            eos_token=\"</s>\",\n",
    "            unk_token='<unk>',\n",
    "            pad_token='<pad>',\n",
    "            mask_token='<mask>'\n",
    "        )\n",
    "        self.max_seq_len = 256\n",
    "        self.bos_token = '<s>'\n",
    "        self.eos_token = '</s>'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def make_input_id_mask(self, tokens, index):\n",
    "        input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        if len(input_id) < self.max_seq_len:\n",
    "            while len(input_id) < self.max_seq_len:\n",
    "                input_id += [self.tokenizer.pad_token_id]\n",
    "                attention_mask += [0]\n",
    "        else:\n",
    "            # logging.warning(f'exceed max_seq_len for given article : {index}')\n",
    "            input_id = input_id[:self.max_seq_len - 1] + [\n",
    "                self.tokenizer.eos_token_id]\n",
    "            attention_mask = attention_mask[:self.max_seq_len]\n",
    "        return input_id, attention_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        record = self.dataset.iloc[index]\n",
    "        q, a = record['질문'], record['답변']\n",
    "        q_tokens = [self.bos_token] + \\\n",
    "            self.tokenizer.tokenize(q) + [self.eos_token]\n",
    "        a_tokens = [self.bos_token] + \\\n",
    "            self.tokenizer.tokenize(a) + [self.eos_token]\n",
    "        encoder_input_id, encoder_attention_mask = self.make_input_id_mask(\n",
    "            q_tokens, index)\n",
    "        decoder_input_id, decoder_attention_mask = self.make_input_id_mask(\n",
    "            a_tokens, index)\n",
    "        labels = self.tokenizer.convert_tokens_to_ids(\n",
    "            a_tokens[1:(self.max_seq_len + 1)])\n",
    "        if len(labels) < self.max_seq_len:\n",
    "            while len(labels) < self.max_seq_len:\n",
    "                # for cross entropy loss masking\n",
    "                labels += [-100]\n",
    "        return {'input_ids': np.array(encoder_input_id, dtype=np.int_),\n",
    "                'attention_mask': np.array(encoder_attention_mask, dtype=np.float_),\n",
    "                'decoder_input_ids': np.array(decoder_input_id, dtype=np.int_),\n",
    "                'decoder_attention_mask': np.array(decoder_attention_mask, dtype=np.float_),\n",
    "                'labels': np.array(labels, dtype=np.int_)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "train_dataset=PPODataset('../data/train_v3.csv','../data/eval_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer\n",
    "from trl import PPOConfig\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"gogamza/kobart-base-v2\",\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    ppo_epochs=4,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_length\":512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [tokenizer.decode(l.squeeze(), skip_special_tokens=True) for l in [train_dataset[1]['decoder_input_ids'], train_dataset[0]['decoder_input_ids']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.5478), tensor(0.6613)]\n"
     ]
    }
   ],
   "source": [
    "response_texts = ['안녕?', '반가워']\n",
    "labels = ['저리가.', '반가웡']\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b) if norm_a != 0 and norm_b != 0 else 0\n",
    "\n",
    "def get_rewards(embed_model, response_texts, labels):\n",
    "    rewards = []\n",
    "    for pred, label in zip(response_texts, labels):\n",
    "        pred_embed = embed_model.encode(pred)\n",
    "        label_embed = embed_model.encode(label)\n",
    "    \n",
    "        sample_score = cosine_similarity(label_embed, pred_embed)\n",
    "        sample_score = torch.tensor(sample_score)\n",
    "        rewards.append(sample_score)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "print(get_rewards(embed_model, response_texts, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6033 [00:55<?, ?it/s]?it/s]\n",
      "epoch:   0%|          | 0/400 [00:55<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 1 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/seungbinyang/LLM-train/alignment-handbook/scripts/kobart.ipynb 셀 35\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bletsur1/home/seungbinyang/LLM-train/alignment-handbook/scripts/kobart.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m rewards \u001b[39m=\u001b[39m get_rewards(embed_model, response_texts, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bletsur1/home/seungbinyang/LLM-train/alignment-handbook/scripts/kobart.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#pipe_outputs = reward_model(texts)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bletsur1/home/seungbinyang/LLM-train/alignment-handbook/scripts/kobart.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bletsur1/home/seungbinyang/LLM-train/alignment-handbook/scripts/kobart.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bletsur1/home/seungbinyang/LLM-train/alignment-handbook/scripts/kobart.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#### Run PPO step\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bletsur1/home/seungbinyang/LLM-train/alignment-handbook/scripts/kobart.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m stats \u001b[39m=\u001b[39m ppo_trainer\u001b[39m.\u001b[39;49mstep(query_tensors_for_input, response_tensors, rewards)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bletsur1/home/seungbinyang/LLM-train/alignment-handbook/scripts/kobart.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m ppo_trainer\u001b[39m.\u001b[39mlog_stats(stats, batch, rewards)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:678\u001b[0m, in \u001b[0;36mPPOTrainer.step\u001b[0;34m(self, queries, responses, scores, response_masks)\u001b[0m\n\u001b[1;32m    674\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    676\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 678\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_model_inputs(queries, responses)\n\u001b[1;32m    680\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_distributed:\n\u001b[1;32m    681\u001b[0m     pad_first \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpadding_side \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:932\u001b[0m, in \u001b[0;36mPPOTrainer.prepare_model_inputs\u001b[0;34m(self, queries, responses)\u001b[0m\n\u001b[1;32m    930\u001b[0m     input_data[\u001b[39m\"\u001b[39m\u001b[39mdecoder_attention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m decoder_inputs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 932\u001b[0m     input_ids \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mcat([q, r]) \u001b[39mfor\u001b[39;00m q, r \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(queries, responses)]\n\u001b[1;32m    933\u001b[0m     input_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_collator(\n\u001b[1;32m    934\u001b[0m         [{\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: ids, \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mones_like(ids)} \u001b[39mfor\u001b[39;00m ids \u001b[39min\u001b[39;00m input_ids]\n\u001b[1;32m    935\u001b[0m     )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_device)\n\u001b[1;32m    937\u001b[0m input_data\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)  \u001b[39m# we don't want to compute LM losses\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:932\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    930\u001b[0m     input_data[\u001b[39m\"\u001b[39m\u001b[39mdecoder_attention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m decoder_inputs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 932\u001b[0m     input_ids \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49mcat([q, r]) \u001b[39mfor\u001b[39;00m q, r \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(queries, responses)]\n\u001b[1;32m    933\u001b[0m     input_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_collator(\n\u001b[1;32m    934\u001b[0m         [{\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: ids, \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mones_like(ids)} \u001b[39mfor\u001b[39;00m ids \u001b[39min\u001b[39;00m input_ids]\n\u001b[1;32m    935\u001b[0m     )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_device)\n\u001b[1;32m    937\u001b[0m input_data\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)  \u001b[39m# we don't want to compute LM losses\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 1 and 2"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in tqdm(range(ppo_trainer.config.ppo_epochs), \"epoch: \"):\n",
    "    for batch in tqdm(ppo_trainer.dataloader): \n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "        query_tensors_for_input = []\n",
    "        response_tensors = []\n",
    "        for query_tensor in query_tensors:\n",
    "            #### Get response from SFTModel\n",
    "            response_tensor = ppo_trainer.generate(query_tensor, **generation_kwargs)\n",
    "            response_tensors.append(response_tensor)\n",
    "            query_tensors_for_input.append(query_tensor)\n",
    "        \n",
    "        #batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "        response_texts = [tokenizer.decode(r.squeeze(), skip_special_tokens=True) for r in response_tensors]\n",
    "        labels =[tokenizer.decode(l.squeeze(), skip_special_tokens=True) for l in batch['decoder_input_ids']]\n",
    "        \n",
    "        #### Compute reward score\n",
    "        rewards = get_rewards(embed_model, response_texts, labels)\n",
    "\n",
    "        #pipe_outputs = reward_model(texts)\n",
    "        #rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(query_tensors_for_input, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save model\n",
    "ppo_trainer.save_model(\"../model/kobart-ppo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

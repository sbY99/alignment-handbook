{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/seungbinyang/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9b13946d30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (BartForConditionalGeneration,\n",
    "                          PreTrainedTokenizerFast)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v2\", device_map=\"auto\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained( \"gogamza/kobart-base-v2\", bos_token=\"<s>\", eos_token=\"</s>\",unk_token='<unk>',pad_token='<pad>',mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185796, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train_v3.csv')\n",
    "eval_df = pd.read_csv('../data/eval_v3.csv')\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, filepath, max_seq_len=128) -> None:\n",
    "        self.filepath = filepath\n",
    "        self.dataset = pd.read_csv(self.filepath)\n",
    "        self.bos_token = '<s>'\n",
    "        self.eos_token = '</s>'\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "            \"gogamza/kobart-base-v2\",\n",
    "            bos_token=\"<s>\",\n",
    "            eos_token=\"</s>\",\n",
    "            unk_token='<unk>',\n",
    "            pad_token='<pad>',\n",
    "            mask_token='<mask>'\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def make_input_id_mask(self, tokens, index):\n",
    "        input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        if len(input_id) < self.max_seq_len:\n",
    "            while len(input_id) < self.max_seq_len:\n",
    "                input_id += [self.tokenizer.pad_token_id]\n",
    "                attention_mask += [0]\n",
    "        else:\n",
    "            # logging.warning(f'exceed max_seq_len for given article : {index}')\n",
    "            input_id = input_id[:self.max_seq_len - 1] + [\n",
    "                self.tokenizer.eos_token_id]\n",
    "            attention_mask = attention_mask[:self.max_seq_len]\n",
    "        return input_id, attention_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        record = self.dataset.iloc[index]\n",
    "        q, a = record['질문'], record['답변']\n",
    "        q_tokens = [self.bos_token] + \\\n",
    "            self.tokenizer.tokenize(q) + [self.eos_token]\n",
    "        a_tokens = [self.bos_token] + \\\n",
    "            self.tokenizer.tokenize(a) + [self.eos_token]\n",
    "        encoder_input_id, encoder_attention_mask = self.make_input_id_mask(\n",
    "            q_tokens, index)\n",
    "        decoder_input_id, decoder_attention_mask = self.make_input_id_mask(\n",
    "            a_tokens, index)\n",
    "        labels = self.tokenizer.convert_tokens_to_ids(\n",
    "            a_tokens[1:(self.max_seq_len + 1)])\n",
    "        if len(labels) < self.max_seq_len:\n",
    "            while len(labels) < self.max_seq_len:\n",
    "                # for cross entropy loss masking\n",
    "                labels += [-100]\n",
    "        return {'input_ids': np.array(encoder_input_id, dtype=np.int_),\n",
    "                'attention_mask': np.array(encoder_attention_mask, dtype=np.float_),\n",
    "                'decoder_input_ids': np.array(decoder_input_id, dtype=np.int_),\n",
    "                'decoder_attention_mask': np.array(decoder_attention_mask, dtype=np.float_),\n",
    "                'labels': np.array(labels, dtype=np.int_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "train_data= ChatDataset('../data/train_v3.csv',512)\n",
    "val_data= ChatDataset('../data/eval_v3.csv',512)\n",
    "train = DataLoader(train_data,batch_size=64,num_workers=8, shuffle=True)\n",
    "val=DataLoader(val_data, batch_size=64,num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../model/kobart\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT-Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-16 07:49:05,289] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (BartForConditionalGeneration,\n",
    "                          PreTrainedTokenizerFast)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from trl import AutoModelForSeq2SeqLMWithValueHead\n",
    "\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "#model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v2\", device_map=\"auto\")\n",
    "#model = BartForConditionalGeneration.from_pretrained('../model/kobart/checkpoint-2800').to(device)\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained('../model/kobart/checkpoint-6400/').to(device)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained( \"gogamza/kobart-base-v2\", bos_token=\"<s>\", eos_token=\"</s>\",unk_token='<unk>',pad_token='<pad>',mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_response(text):\n",
    "    input_ids =  [tokenizer.bos_token_id] + tokenizer.encode(text) + [tokenizer.eos_token_id]\n",
    "    res_ids = model.generate(torch.tensor([input_ids]).to(device),\n",
    "                                                max_length=512,\n",
    "                                                num_beams=10,\n",
    "                                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                                bad_words_ids=[[tokenizer.unk_token_id]])\n",
    "    a = tokenizer.batch_decode(res_ids.tolist(), skip_special_tokens=True)[0]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "방청페인트의 종류는 광명단페인트, 방청산화철페인트, 알미늄페인트, 역청질페인트, 워시프라이머, 크롬산아연페인트, 규산염페인트가 있습니다.원목사이딩의 단점은 가격대가 높고 관리가 어려우며 습기에 약해 뒤틀림, 부서짐, 수축/팽장이 생길 수 있다는 점입니다.\n"
     ]
    }
   ],
   "source": [
    "text = \"방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요? 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요?\"\n",
    "print(get_response(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sent=[]\n",
    "test=pd.read_csv('../data/test_raw.csv')\n",
    "\n",
    "for i in range(len(test)):\n",
    "  row = test.iloc[i]\n",
    "  question = row['질문']\n",
    "  response = get_response(question)\n",
    "  generated_sent.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카페트의 기대수명은 6년입니다.오리지널징크는 다른 징크에 비해 수명이 길고 다양한 패턴과 디자인이 가능하며 친환경적이고 금속 부식에 대한 내식성이 뛰어나 유지보수가 용이하다는 장점이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(generated_sent[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "m = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_list=[]\n",
    "for i in range(len(generated_sent)):\n",
    "  embed=m.encode(generated_sent[i]) #주어진 모델로 인코딩\n",
    "  encode_list.append(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(encode_list)):\n",
    "  sub.loc[i, 'vec_0':'vec_511']=encode_list[i]\n",
    "\n",
    "sub.set_index('id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec_0</th>\n",
       "      <th>vec_1</th>\n",
       "      <th>vec_2</th>\n",
       "      <th>vec_3</th>\n",
       "      <th>vec_4</th>\n",
       "      <th>vec_5</th>\n",
       "      <th>vec_6</th>\n",
       "      <th>vec_7</th>\n",
       "      <th>vec_8</th>\n",
       "      <th>vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>vec_502</th>\n",
       "      <th>vec_503</th>\n",
       "      <th>vec_504</th>\n",
       "      <th>vec_505</th>\n",
       "      <th>vec_506</th>\n",
       "      <th>vec_507</th>\n",
       "      <th>vec_508</th>\n",
       "      <th>vec_509</th>\n",
       "      <th>vec_510</th>\n",
       "      <th>vec_511</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TEST_000</th>\n",
       "      <td>0.032679</td>\n",
       "      <td>-0.022694</td>\n",
       "      <td>0.015984</td>\n",
       "      <td>0.022094</td>\n",
       "      <td>0.061130</td>\n",
       "      <td>0.017720</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>-0.011087</td>\n",
       "      <td>-0.005358</td>\n",
       "      <td>0.053613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01573</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>-0.029576</td>\n",
       "      <td>-0.035200</td>\n",
       "      <td>-0.002526</td>\n",
       "      <td>0.023186</td>\n",
       "      <td>0.032703</td>\n",
       "      <td>0.036993</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>0.013723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEST_001</th>\n",
       "      <td>-0.001152</td>\n",
       "      <td>-0.008816</td>\n",
       "      <td>0.014305</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.038565</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>-0.011721</td>\n",
       "      <td>-0.002723</td>\n",
       "      <td>-0.000382</td>\n",
       "      <td>0.018276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01670</td>\n",
       "      <td>-0.010989</td>\n",
       "      <td>0.014790</td>\n",
       "      <td>-0.023875</td>\n",
       "      <td>-0.059724</td>\n",
       "      <td>0.054009</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.009057</td>\n",
       "      <td>0.010631</td>\n",
       "      <td>0.029853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             vec_0     vec_1     vec_2     vec_3     vec_4     vec_5  \\\n",
       "id                                                                     \n",
       "TEST_000  0.032679 -0.022694  0.015984  0.022094  0.061130  0.017720   \n",
       "TEST_001 -0.001152 -0.008816  0.014305  0.000614  0.038565  0.002583   \n",
       "\n",
       "             vec_6     vec_7     vec_8     vec_9  ...  vec_502   vec_503  \\\n",
       "id                                                ...                      \n",
       "TEST_000 -0.002545 -0.011087 -0.005358  0.053613  ... -0.01573  0.000518   \n",
       "TEST_001 -0.011721 -0.002723 -0.000382  0.018276  ...  0.01670 -0.010989   \n",
       "\n",
       "           vec_504   vec_505   vec_506   vec_507   vec_508   vec_509  \\\n",
       "id                                                                     \n",
       "TEST_000 -0.029576 -0.035200 -0.002526  0.023186  0.032703  0.036993   \n",
       "TEST_001  0.014790 -0.023875 -0.059724  0.054009  0.001227  0.009057   \n",
       "\n",
       "           vec_510   vec_511  \n",
       "id                            \n",
       "TEST_000  0.007826  0.013723  \n",
       "TEST_001  0.010631  0.029853  \n",
       "\n",
       "[2 rows x 512 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../result/kobart-v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'data':[]}\n",
    "for i in range(len(test)):\n",
    "  row = test.iloc[i]\n",
    "  question = row['질문']\n",
    "  response = generated_sent[i]\n",
    "  dataset['data'].append({\n",
    "    'question':question,\n",
    "    'response':response\n",
    "  })\n",
    "\n",
    "import json\n",
    "with open('../result/kobart-v4.json ', 'w') as file:\n",
    "    json.dump(dataset, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-16 08:55:54,551] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from trl import AutoModelForSeq2SeqLMWithValueHead\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\" \n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained('../model/kobart/checkpoint-6400/')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained( \"gogamza/kobart-base-v2\", bos_token=\"<s>\", eos_token=\"</s>\",unk_token='<unk>',pad_token='<pad>',mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "embed_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"gogamza/kobart-base-v2\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>',\n",
    "    mask_token='<mask>'\n",
    ")\n",
    "class PPODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, path):\n",
    "        df = pd.read_csv(path)\n",
    "        self.dataset = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = 512\n",
    "        self.bos_token = '<s>'\n",
    "        self.eos_token = '</s>'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def make_input_id_mask(self, tokens, index):\n",
    "        input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        if len(input_id) < self.max_seq_len:\n",
    "            while len(input_id) < self.max_seq_len:\n",
    "                input_id += [self.tokenizer.pad_token_id]\n",
    "                attention_mask += [0]\n",
    "        else:\n",
    "            # logging.warning(f'exceed max_seq_len for given article : {index}')\n",
    "            input_id = input_id[:self.max_seq_len - 1] + [\n",
    "                self.tokenizer.eos_token_id]\n",
    "            attention_mask = attention_mask[:self.max_seq_len]\n",
    "        return input_id, attention_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        record = self.dataset.iloc[index]\n",
    "        q, a = record['질문'], record['답변']\n",
    "        q_tokens = [self.bos_token] + \\\n",
    "            self.tokenizer.tokenize(q) + [self.eos_token]\n",
    "        a_tokens = [self.bos_token] + \\\n",
    "            self.tokenizer.tokenize(a) + [self.eos_token]\n",
    "        encoder_input_id, encoder_attention_mask = self.make_input_id_mask(\n",
    "            q_tokens, index)\n",
    "        decoder_input_id, decoder_attention_mask = self.make_input_id_mask(\n",
    "            a_tokens, index)\n",
    "        labels = self.tokenizer.convert_tokens_to_ids(\n",
    "            a_tokens[1:(self.max_seq_len + 1)])\n",
    "        if len(labels) < self.max_seq_len:\n",
    "            while len(labels) < self.max_seq_len:\n",
    "                # for cross entropy loss masking\n",
    "                labels += [-100]\n",
    "        return {'input_ids': np.array(encoder_input_id, dtype=np.int_),\n",
    "                'attention_mask': np.array(encoder_attention_mask, dtype=np.float_),\n",
    "                'decoder_input_ids': np.array(decoder_input_id, dtype=np.int_),\n",
    "                'decoder_attention_mask': np.array(decoder_attention_mask, dtype=np.float_),\n",
    "                'labels': np.array(labels, dtype=np.int_)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=PPODataset(tokenizer, path='../data/train_v2.csv')\n",
    "eval_dataset=PPODataset(tokenizer,path='../data/eval_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer\n",
    "from trl import PPOConfig\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"gogamza/kobart-base-v2\",\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    ppo_epochs=4,\n",
    "    seed=random_seed,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "    \"max_new_tokens\": 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.5478), tensor(0.6613)]\n"
     ]
    }
   ],
   "source": [
    "response_texts = ['안녕?', '반가워']\n",
    "labels = ['저리가.', '반가웡']\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b) if norm_a != 0 and norm_b != 0 else 0\n",
    "\n",
    "def get_rewards(embed_model, response_texts, labels):\n",
    "    rewards = []\n",
    "    for pred, label in zip(response_texts, labels):\n",
    "        pred_embed = embed_model.encode(pred)\n",
    "        label_embed = embed_model.encode(label)\n",
    "    \n",
    "        sample_score = cosine_similarity(label_embed, pred_embed)\n",
    "        sample_score = torch.tensor(sample_score)\n",
    "        rewards.append(sample_score)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "print(get_rewards(embed_model, response_texts, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "for epoch in tqdm(range(ppo_trainer.config.ppo_epochs), \"epoch: \"):\n",
    "    for batch in tqdm(ppo_trainer.dataloader): \n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "        query_tensors_for_step = []\n",
    "        response_tensors = []\n",
    "\n",
    "        for query_tensor in query_tensors:\n",
    "            #### Get response from SFTModel\n",
    "            response_tensor = ppo_trainer.generate(query_tensor, **generation_kwargs)\n",
    "            response_tensors.append(response_tensor)\n",
    "            query_tensors_for_step.append(query_tensor)\n",
    "        \n",
    "        response_tensors_for_input = pad_sequence(\n",
    "            [response_tensors[index][0] for index in range(len(response_tensors))],\n",
    "            batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        response_tensors_for_step = [i for i in response_tensors_for_input]\n",
    "        \n",
    "        #batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "        #response_texts = [tokenizer.decode(r.squeeze(), skip_special_tokens=True) for r in response_tensors]\n",
    "        response_texts =  tokenizer.batch_decode(response_tensors_for_input, skip_special_tokens=True)\n",
    "        labels = tokenizer.batch_decode(batch['decoder_input_ids'], skip_special_tokens=True)\n",
    "\n",
    "        #### Compute reward score\n",
    "        rewards = get_rewards(embed_model, response_texts, labels)\n",
    "\n",
    "        print(response_texts)\n",
    "        print(labels)\n",
    "        print(rewards)\n",
    "        print('-------')\n",
    "\n",
    "        #pipe_outputs = reward_model(texts)\n",
    "        #rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(query_tensors_for_step, response_tensors_for_step, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save model\n",
    "ppo_trainer.save_model(\"../model/kobart-ppo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO-Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import AutoModelForSeq2SeqLMWithValueHead\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\" \n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained('../model/kobart/checkpoint-5600/')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained( \"gogamza/kobart-base-v2\", bos_token=\"<s>\", eos_token=\"</s>\",unk_token='<unk>',pad_token='<pad>',mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_response(text):\n",
    "    input_ids =  [tokenizer.bos_token_id] + tokenizer.encode(text) + [tokenizer.eos_token_id]\n",
    "    res_ids = model.generate(torch.tensor([input_ids]).to(device),\n",
    "                                                max_length=512,\n",
    "                                                num_beams=10,\n",
    "                                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                                bad_words_ids=[[tokenizer.unk_token_id]])\n",
    "    a = tokenizer.batch_decode(res_ids.tolist(), skip_special_tokens=True)[0]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요? 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요?\"\n",
    "print(get_response(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sent=[]\n",
    "test=pd.read_csv('../data/test_raw.csv')\n",
    "\n",
    "for i in range(len(test)):\n",
    "  row = test.iloc[i]\n",
    "  question = row['질문']\n",
    "  response = get_response(question)\n",
    "  generated_sent.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_sent[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "m = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_list=[]\n",
    "for i in range(len(generated_sent)):\n",
    "  embed=m.encode(generated_sent[i]) #주어진 모델로 인코딩\n",
    "  encode_list.append(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(encode_list)):\n",
    "  sub.loc[i, 'vec_0':'vec_511']=encode_list[i]\n",
    "\n",
    "sub.set_index('id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../result/kobart-ppo-v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'data':[]}\n",
    "for i in range(len(test)):\n",
    "  row = test.iloc[i]\n",
    "  question = row['질문']\n",
    "  response = generated_sent[i]\n",
    "  dataset['data'].append({\n",
    "    'question':question,\n",
    "    'response':response\n",
    "  })\n",
    "\n",
    "import json\n",
    "with open('../result/kobart-ppo-v4.json ', 'w') as file:\n",
    "    json.dump(dataset, file, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
